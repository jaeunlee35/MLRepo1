{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6148ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (1762, 4)\n",
      "y_data shape: (1762, 1)\n",
      "Feature names: ['open', 'low', 'high', 'volume']\n",
      "1. Data Analysis\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Training Models\n",
      "Total configurations: 243\n",
      "Training model 1/243 with parameters:\n",
      "Config indices: (0, 0, 0, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 2/243 with parameters:\n",
      "Config indices: (0, 0, 0, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 80864733716021248.0000, Validation Loss: 80864939874451456.0000, Test Loss: 80864853975105536.0000\n",
      "Epoch 11, Training Loss: 3291.3865, Validation Loss: 3693.1748, Test Loss: 3223.3938\n",
      "Epoch 21, Training Loss: 808.9892, Validation Loss: 819.1157, Test Loss: 769.9291\n",
      "Epoch 31, Training Loss: 809.8434, Validation Loss: 829.7259, Test Loss: 770.0625\n",
      "Epoch 41, Training Loss: 808.9379, Validation Loss: 822.5789, Test Loss: 769.6181\n",
      "Epoch 51, Training Loss: 809.0536, Validation Loss: 818.4186, Test Loss: 770.0499\n",
      "Epoch 61, Training Loss: 808.9594, Validation Loss: 823.0302, Test Loss: 769.6078\n",
      "Epoch 71, Training Loss: 808.9235, Validation Loss: 822.1689, Test Loss: 769.6330\n",
      "Epoch 81, Training Loss: 809.1143, Validation Loss: 817.9109, Test Loss: 770.1520\n",
      "Epoch 91, Training Loss: 809.1846, Validation Loss: 817.4179, Test Loss: 770.2642\n",
      "Epoch 101, Training Loss: 809.0065, Validation Loss: 823.7672, Test Loss: 769.6040\n",
      "Epoch 111, Training Loss: 808.9197, Validation Loss: 820.4711, Test Loss: 769.7543\n",
      "Epoch 121, Training Loss: 809.0473, Validation Loss: 824.2761, Test Loss: 769.6103\n",
      "Epoch 131, Training Loss: 808.9102, Validation Loss: 821.3939, Test Loss: 769.6759\n",
      "Epoch 141, Training Loss: 808.9130, Validation Loss: 821.6874, Test Loss: 769.6572\n",
      "Epoch 151, Training Loss: 809.2737, Validation Loss: 826.3200, Test Loss: 769.7025\n",
      "Epoch 161, Training Loss: 809.2042, Validation Loss: 817.2931, Test Loss: 770.2946\n",
      "Epoch 171, Training Loss: 808.9183, Validation Loss: 821.9744, Test Loss: 769.6419\n",
      "Epoch 181, Training Loss: 809.0229, Validation Loss: 818.7239, Test Loss: 769.9941\n",
      "Epoch 191, Training Loss: 808.9325, Validation Loss: 822.4391, Test Loss: 769.6225\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 808.9325\n",
      "Validation loss: 822.4391\n",
      "Test loss: 769.6225\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 3/243 with parameters:\n",
      "Config indices: (0, 0, 0, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 8603219.0000, Validation Loss: 8626155.0000, Test Loss: 8601484.0000\n",
      "Epoch 11, Training Loss: 808.9637, Validation Loss: 823.1086, Test Loss: 769.6067\n",
      "Epoch 21, Training Loss: 808.9318, Validation Loss: 822.4232, Test Loss: 769.6232\n",
      "Epoch 31, Training Loss: 808.9226, Validation Loss: 820.3691, Test Loss: 769.7649\n",
      "Epoch 41, Training Loss: 808.9476, Validation Loss: 822.7920, Test Loss: 769.6124\n",
      "Epoch 51, Training Loss: 808.9305, Validation Loss: 822.3908, Test Loss: 769.6243\n",
      "Epoch 61, Training Loss: 809.3522, Validation Loss: 826.8815, Test Loss: 769.7452\n",
      "Epoch 71, Training Loss: 809.1302, Validation Loss: 817.7919, Test Loss: 770.1779\n",
      "Epoch 81, Training Loss: 808.9246, Validation Loss: 820.2993, Test Loss: 769.7723\n",
      "Epoch 91, Training Loss: 809.2310, Validation Loss: 817.1293, Test Loss: 770.3358\n",
      "Epoch 101, Training Loss: 808.9277, Validation Loss: 820.2139, Test Loss: 769.7817\n",
      "Epoch 111, Training Loss: 809.0452, Validation Loss: 824.2514, Test Loss: 769.6099\n",
      "Epoch 121, Training Loss: 808.9651, Validation Loss: 819.4575, Test Loss: 769.8779\n",
      "Epoch 131, Training Loss: 808.9854, Validation Loss: 819.1657, Test Loss: 769.9213\n",
      "Epoch 141, Training Loss: 809.1948, Validation Loss: 825.6962, Test Loss: 769.6635\n",
      "Epoch 151, Training Loss: 808.9578, Validation Loss: 819.5735, Test Loss: 769.8617\n",
      "Epoch 161, Training Loss: 808.9863, Validation Loss: 823.4773, Test Loss: 769.6036\n",
      "Epoch 171, Training Loss: 808.9468, Validation Loss: 822.7820, Test Loss: 769.6128\n",
      "Epoch 181, Training Loss: 809.6931, Validation Loss: 828.9410, Test Loss: 769.9591\n",
      "Epoch 191, Training Loss: 808.9534, Validation Loss: 819.6511, Test Loss: 769.8510\n",
      "Epoch 201, Training Loss: 809.0461, Validation Loss: 818.4919, Test Loss: 770.0361\n",
      "Epoch 211, Training Loss: 808.9277, Validation Loss: 820.2105, Test Loss: 769.7822\n",
      "Epoch 221, Training Loss: 808.9551, Validation Loss: 819.6234, Test Loss: 769.8547\n",
      "Epoch 231, Training Loss: 809.5564, Validation Loss: 828.1722, Test Loss: 769.8693\n",
      "Epoch 241, Training Loss: 809.0587, Validation Loss: 824.4065, Test Loss: 769.6130\n",
      "Epoch 251, Training Loss: 809.0573, Validation Loss: 824.3916, Test Loss: 769.6126\n",
      "Epoch 261, Training Loss: 808.9104, Validation Loss: 821.4533, Test Loss: 769.6718\n",
      "Epoch 271, Training Loss: 808.9731, Validation Loss: 819.3345, Test Loss: 769.8957\n",
      "Epoch 281, Training Loss: 808.9418, Validation Loss: 822.6733, Test Loss: 769.6155\n",
      "Epoch 291, Training Loss: 808.9214, Validation Loss: 822.0918, Test Loss: 769.6364\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 808.9214\n",
      "Validation loss: 822.0918\n",
      "Test loss: 769.6364\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 4/243 with parameters:\n",
      "Config indices: (0, 0, 0, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 4357.0557, Validation Loss: 3903.7800, Test Loss: 4352.2275\n",
      "Epoch 11, Training Loss: 808.9119, Validation Loss: 821.6052, Test Loss: 769.6622\n",
      "Epoch 21, Training Loss: 808.9301, Validation Loss: 822.3705, Test Loss: 769.6251\n",
      "Epoch 31, Training Loss: 808.9099, Validation Loss: 821.3365, Test Loss: 769.6800\n",
      "Epoch 41, Training Loss: 808.9139, Validation Loss: 821.7459, Test Loss: 769.6539\n",
      "Epoch 51, Training Loss: 808.9166, Validation Loss: 820.6046, Test Loss: 769.7411\n",
      "Epoch 61, Training Loss: 808.9194, Validation Loss: 820.4747, Test Loss: 769.7540\n",
      "Epoch 71, Training Loss: 808.9097, Validation Loss: 821.2695, Test Loss: 769.6847\n",
      "Epoch 81, Training Loss: 808.9099, Validation Loss: 821.1498, Test Loss: 769.6936\n",
      "Epoch 91, Training Loss: 808.9108, Validation Loss: 820.9763, Test Loss: 769.7075\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 808.9108\n",
      "Validation loss: 820.9763\n",
      "Test loss: 769.7075\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 5/243 with parameters:\n",
      "Config indices: (0, 0, 0, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 2241.1013, Validation Loss: 1957.6171, Test Loss: 2223.7297\n",
      "Epoch 11, Training Loss: 808.9108, Validation Loss: 821.5007, Test Loss: 769.6688\n",
      "Epoch 21, Training Loss: 808.9496, Validation Loss: 822.8442, Test Loss: 769.6113\n",
      "Epoch 31, Training Loss: 808.9244, Validation Loss: 822.2032, Test Loss: 769.6315\n",
      "Epoch 41, Training Loss: 808.9100, Validation Loss: 821.1895, Test Loss: 769.6907\n",
      "Epoch 51, Training Loss: 808.9395, Validation Loss: 822.6219, Test Loss: 769.6170\n",
      "Epoch 61, Training Loss: 808.9744, Validation Loss: 823.2945, Test Loss: 769.6046\n",
      "Epoch 71, Training Loss: 808.9097, Validation Loss: 821.2342, Test Loss: 769.6873\n",
      "Epoch 81, Training Loss: 809.0158, Validation Loss: 818.8026, Test Loss: 769.9805\n",
      "Epoch 91, Training Loss: 808.9109, Validation Loss: 821.5089, Test Loss: 769.6682\n",
      "Epoch 101, Training Loss: 808.9122, Validation Loss: 820.8441, Test Loss: 769.7188\n",
      "Epoch 111, Training Loss: 808.9132, Validation Loss: 821.7015, Test Loss: 769.6563\n",
      "Epoch 121, Training Loss: 808.9502, Validation Loss: 819.7087, Test Loss: 769.8433\n",
      "Epoch 131, Training Loss: 808.9100, Validation Loss: 821.1450, Test Loss: 769.6942\n",
      "Epoch 141, Training Loss: 808.9107, Validation Loss: 820.9969, Test Loss: 769.7059\n",
      "Epoch 151, Training Loss: 808.9464, Validation Loss: 822.7680, Test Loss: 769.6130\n",
      "Epoch 161, Training Loss: 808.9691, Validation Loss: 823.2020, Test Loss: 769.6054\n",
      "Epoch 171, Training Loss: 808.9211, Validation Loss: 820.4124, Test Loss: 769.7604\n",
      "Epoch 181, Training Loss: 808.9133, Validation Loss: 821.7206, Test Loss: 769.6553\n",
      "Epoch 191, Training Loss: 808.9434, Validation Loss: 819.8430, Test Loss: 769.8260\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 808.9434\n",
      "Validation loss: 819.8430\n",
      "Test loss: 769.8260\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 6/243 with parameters:\n",
      "Config indices: (0, 0, 0, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 2232.6936, Validation Loss: 1950.0787, Test Loss: 2215.2576\n",
      "Epoch 11, Training Loss: 808.9447, Validation Loss: 819.8164, Test Loss: 769.8294\n",
      "Epoch 21, Training Loss: 808.9202, Validation Loss: 820.4526, Test Loss: 769.7562\n",
      "Epoch 31, Training Loss: 808.9106, Validation Loss: 821.0304, Test Loss: 769.7032\n",
      "Epoch 41, Training Loss: 808.9120, Validation Loss: 821.6140, Test Loss: 769.6618\n",
      "Epoch 51, Training Loss: 808.9231, Validation Loss: 822.1546, Test Loss: 769.6335\n",
      "Epoch 61, Training Loss: 808.9775, Validation Loss: 823.3422, Test Loss: 769.6043\n",
      "Epoch 71, Training Loss: 808.9736, Validation Loss: 819.3295, Test Loss: 769.8965\n",
      "Epoch 81, Training Loss: 808.9147, Validation Loss: 820.6946, Test Loss: 769.7325\n",
      "Epoch 91, Training Loss: 808.9210, Validation Loss: 822.0774, Test Loss: 769.6371\n",
      "Epoch 101, Training Loss: 809.1252, Validation Loss: 825.0843, Test Loss: 769.6342\n",
      "Epoch 111, Training Loss: 808.9854, Validation Loss: 819.1662, Test Loss: 769.9212\n",
      "Epoch 121, Training Loss: 808.9103, Validation Loss: 821.4205, Test Loss: 769.6741\n",
      "Epoch 131, Training Loss: 808.9224, Validation Loss: 820.3754, Test Loss: 769.7643\n",
      "Epoch 141, Training Loss: 808.9172, Validation Loss: 821.9213, Test Loss: 769.6446\n",
      "Epoch 151, Training Loss: 808.9620, Validation Loss: 823.0763, Test Loss: 769.6071\n",
      "Epoch 161, Training Loss: 808.9107, Validation Loss: 820.9951, Test Loss: 769.7059\n",
      "Epoch 171, Training Loss: 808.9489, Validation Loss: 819.7348, Test Loss: 769.8400\n",
      "Epoch 181, Training Loss: 808.9222, Validation Loss: 822.1182, Test Loss: 769.6353\n",
      "Epoch 191, Training Loss: 808.9379, Validation Loss: 822.5759, Test Loss: 769.6182\n",
      "Epoch 201, Training Loss: 808.9108, Validation Loss: 821.4951, Test Loss: 769.6692\n",
      "Epoch 211, Training Loss: 808.9130, Validation Loss: 820.8054, Test Loss: 769.7224\n",
      "Epoch 221, Training Loss: 808.9256, Validation Loss: 822.2369, Test Loss: 769.6301\n",
      "Epoch 231, Training Loss: 808.9103, Validation Loss: 821.3872, Test Loss: 769.6763\n",
      "Epoch 241, Training Loss: 808.9333, Validation Loss: 820.0649, Test Loss: 769.7990\n",
      "Epoch 251, Training Loss: 808.9097, Validation Loss: 821.2386, Test Loss: 769.6869\n",
      "Epoch 261, Training Loss: 808.9483, Validation Loss: 819.7429, Test Loss: 769.8388\n",
      "Epoch 271, Training Loss: 808.9130, Validation Loss: 820.7904, Test Loss: 769.7236\n",
      "Epoch 281, Training Loss: 808.9384, Validation Loss: 822.5905, Test Loss: 769.6179\n",
      "Epoch 291, Training Loss: 808.9442, Validation Loss: 822.7224, Test Loss: 769.6143\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 808.9442\n",
      "Validation loss: 822.7224\n",
      "Test loss: 769.6143\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 7/243 with parameters:\n",
      "Config indices: (0, 0, 0, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 5542.2524, Validation Loss: 5016.8042, Test Loss: 5542.7549\n",
      "Epoch 11, Training Loss: 810.5273, Validation Loss: 812.9160, Test Loss: 772.0387\n",
      "Epoch 21, Training Loss: 808.9727, Validation Loss: 819.3418, Test Loss: 769.8947\n",
      "Epoch 31, Training Loss: 808.9109, Validation Loss: 821.5109, Test Loss: 769.6682\n",
      "Epoch 41, Training Loss: 808.9402, Validation Loss: 822.6357, Test Loss: 769.6166\n",
      "Epoch 51, Training Loss: 808.9224, Validation Loss: 822.1363, Test Loss: 769.6343\n",
      "Epoch 61, Training Loss: 808.9138, Validation Loss: 821.7388, Test Loss: 769.6542\n",
      "Epoch 71, Training Loss: 808.9218, Validation Loss: 820.3956, Test Loss: 769.7620\n",
      "Epoch 81, Training Loss: 808.9318, Validation Loss: 822.4164, Test Loss: 769.6233\n",
      "Epoch 91, Training Loss: 808.9285, Validation Loss: 822.3318, Test Loss: 769.6264\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 808.9285\n",
      "Validation loss: 822.3318\n",
      "Test loss: 769.6264\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 8/243 with parameters:\n",
      "Config indices: (0, 0, 0, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 3472.9902, Validation Loss: 3081.8684, Test Loss: 3463.5701\n",
      "Epoch 11, Training Loss: 809.7450, Validation Loss: 814.9318, Test Loss: 771.0496\n",
      "Epoch 21, Training Loss: 809.0110, Validation Loss: 818.8536, Test Loss: 769.9719\n",
      "Epoch 31, Training Loss: 808.9190, Validation Loss: 820.5016, Test Loss: 769.7512\n",
      "Epoch 41, Training Loss: 808.9496, Validation Loss: 819.7170, Test Loss: 769.8423\n",
      "Epoch 51, Training Loss: 808.9176, Validation Loss: 821.9351, Test Loss: 769.6439\n",
      "Epoch 61, Training Loss: 808.9345, Validation Loss: 820.0325, Test Loss: 769.8027\n",
      "Epoch 71, Training Loss: 808.9109, Validation Loss: 821.5237, Test Loss: 769.6672\n",
      "Epoch 81, Training Loss: 808.9421, Validation Loss: 819.8661, Test Loss: 769.8231\n",
      "Epoch 91, Training Loss: 808.9117, Validation Loss: 820.9053, Test Loss: 769.7135\n",
      "Epoch 101, Training Loss: 808.9101, Validation Loss: 821.1077, Test Loss: 769.6969\n",
      "Epoch 111, Training Loss: 808.9100, Validation Loss: 821.2288, Test Loss: 769.6877\n",
      "Epoch 121, Training Loss: 808.9222, Validation Loss: 820.3815, Test Loss: 769.7636\n",
      "Epoch 131, Training Loss: 809.0893, Validation Loss: 824.7314, Test Loss: 769.6217\n",
      "Epoch 141, Training Loss: 808.9232, Validation Loss: 820.3458, Test Loss: 769.7675\n",
      "Epoch 151, Training Loss: 808.9101, Validation Loss: 821.0684, Test Loss: 769.7001\n",
      "Epoch 161, Training Loss: 808.9105, Validation Loss: 821.0235, Test Loss: 769.7037\n",
      "Epoch 171, Training Loss: 808.9099, Validation Loss: 821.2538, Test Loss: 769.6858\n",
      "Epoch 181, Training Loss: 808.9285, Validation Loss: 822.3281, Test Loss: 769.6266\n",
      "Epoch 191, Training Loss: 808.9113, Validation Loss: 820.9420, Test Loss: 769.7104\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 808.9113\n",
      "Validation loss: 820.9420\n",
      "Test loss: 769.7104\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 9/243 with parameters:\n",
      "Config indices: (0, 0, 0, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 3886.2996, Validation Loss: 3465.0095, Test Loss: 3879.1086\n",
      "Epoch 11, Training Loss: 809.5766, Validation Loss: 815.5239, Test Loss: 770.8251\n",
      "Epoch 21, Training Loss: 808.9319, Validation Loss: 820.0975, Test Loss: 769.7950\n",
      "Epoch 31, Training Loss: 809.1232, Validation Loss: 825.0637, Test Loss: 769.6335\n",
      "Epoch 41, Training Loss: 808.9305, Validation Loss: 822.3807, Test Loss: 769.6246\n",
      "Epoch 51, Training Loss: 808.9792, Validation Loss: 823.3695, Test Loss: 769.6042\n",
      "Epoch 61, Training Loss: 808.9176, Validation Loss: 820.5529, Test Loss: 769.7461\n",
      "Epoch 71, Training Loss: 808.9357, Validation Loss: 822.5261, Test Loss: 769.6198\n",
      "Epoch 81, Training Loss: 808.9651, Validation Loss: 819.4575, Test Loss: 769.8779\n",
      "Epoch 91, Training Loss: 808.9229, Validation Loss: 822.1539, Test Loss: 769.6335\n",
      "Epoch 101, Training Loss: 808.9164, Validation Loss: 821.8834, Test Loss: 769.6464\n",
      "Epoch 111, Training Loss: 808.9111, Validation Loss: 821.5206, Test Loss: 769.6676\n",
      "Epoch 121, Training Loss: 808.9407, Validation Loss: 822.6450, Test Loss: 769.6163\n",
      "Epoch 131, Training Loss: 808.9183, Validation Loss: 821.9688, Test Loss: 769.6423\n",
      "Epoch 141, Training Loss: 808.9815, Validation Loss: 819.2167, Test Loss: 769.9134\n",
      "Epoch 151, Training Loss: 808.9100, Validation Loss: 821.3334, Test Loss: 769.6800\n",
      "Epoch 161, Training Loss: 808.9236, Validation Loss: 820.3413, Test Loss: 769.7679\n",
      "Epoch 171, Training Loss: 808.9109, Validation Loss: 821.5068, Test Loss: 769.6685\n",
      "Epoch 181, Training Loss: 808.9158, Validation Loss: 820.6453, Test Loss: 769.7371\n",
      "Epoch 191, Training Loss: 808.9515, Validation Loss: 819.6881, Test Loss: 769.8461\n",
      "Epoch 201, Training Loss: 808.9607, Validation Loss: 819.5289, Test Loss: 769.8677\n",
      "Epoch 211, Training Loss: 808.9258, Validation Loss: 820.2679, Test Loss: 769.7759\n",
      "Epoch 221, Training Loss: 808.9404, Validation Loss: 819.9019, Test Loss: 769.8186\n",
      "Epoch 231, Training Loss: 808.9166, Validation Loss: 821.8975, Test Loss: 769.6457\n",
      "Epoch 241, Training Loss: 808.9108, Validation Loss: 820.9741, Test Loss: 769.7079\n",
      "Epoch 251, Training Loss: 808.9095, Validation Loss: 821.2318, Test Loss: 769.6875\n",
      "Epoch 261, Training Loss: 808.9127, Validation Loss: 820.8159, Test Loss: 769.7215\n",
      "Epoch 271, Training Loss: 808.9529, Validation Loss: 819.6622, Test Loss: 769.8496\n",
      "Epoch 281, Training Loss: 808.9097, Validation Loss: 821.3265, Test Loss: 769.6807\n",
      "Epoch 291, Training Loss: 808.9218, Validation Loss: 820.3998, Test Loss: 769.7618\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 808.9218\n",
      "Validation loss: 820.3998\n",
      "Test loss: 769.7618\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 10/243 with parameters:\n",
      "Config indices: (0, 0, 1, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 5555.9707, Validation Loss: 5029.7466, Test Loss: 5556.5317\n",
      "Epoch 11, Training Loss: 1026.4060, Validation Loss: 923.4586, Test Loss: 995.6985\n",
      "Epoch 21, Training Loss: 818.9083, Validation Loss: 806.5218, Test Loss: 781.5110\n",
      "Epoch 31, Training Loss: 809.3773, Validation Loss: 816.3625, Test Loss: 770.5492\n",
      "Epoch 41, Training Loss: 808.9349, Validation Loss: 820.0265, Test Loss: 769.8034\n",
      "Epoch 51, Training Loss: 808.9126, Validation Loss: 820.8182, Test Loss: 769.7211\n",
      "Epoch 61, Training Loss: 808.9103, Validation Loss: 821.0536, Test Loss: 769.7012\n",
      "Epoch 71, Training Loss: 808.9099, Validation Loss: 821.1883, Test Loss: 769.6908\n",
      "Epoch 81, Training Loss: 808.9098, Validation Loss: 821.2297, Test Loss: 769.6876\n",
      "Epoch 91, Training Loss: 808.9098, Validation Loss: 821.2212, Test Loss: 769.6882\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 808.9098\n",
      "Validation loss: 821.2212\n",
      "Test loss: 769.6882\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 11/243 with parameters:\n",
      "Config indices: (0, 0, 1, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 5589.6763, Validation Loss: 5061.5415, Test Loss: 5590.3765\n",
      "Epoch 11, Training Loss: 1027.9280, Validation Loss: 924.5781, Test Loss: 997.2502\n",
      "Epoch 21, Training Loss: 818.9800, Validation Loss: 806.5050, Test Loss: 781.5893\n",
      "Epoch 31, Training Loss: 809.3742, Validation Loss: 816.3770, Test Loss: 770.5450\n",
      "Epoch 41, Training Loss: 808.9316, Validation Loss: 820.1086, Test Loss: 769.7938\n",
      "Epoch 51, Training Loss: 808.9102, Validation Loss: 821.0889, Test Loss: 769.6984\n",
      "Epoch 61, Training Loss: 808.9099, Validation Loss: 821.2039, Test Loss: 769.6895\n",
      "Epoch 71, Training Loss: 808.9099, Validation Loss: 821.1675, Test Loss: 769.6923\n",
      "Epoch 81, Training Loss: 808.9100, Validation Loss: 821.1368, Test Loss: 769.6946\n",
      "Epoch 91, Training Loss: 808.9099, Validation Loss: 821.2947, Test Loss: 769.6829\n",
      "Epoch 101, Training Loss: 808.9103, Validation Loss: 821.0858, Test Loss: 769.6987\n",
      "Epoch 111, Training Loss: 808.9097, Validation Loss: 821.2958, Test Loss: 769.6828\n",
      "Epoch 121, Training Loss: 808.9099, Validation Loss: 821.2194, Test Loss: 769.6883\n",
      "Epoch 131, Training Loss: 808.9099, Validation Loss: 821.3000, Test Loss: 769.6824\n",
      "Epoch 141, Training Loss: 808.9097, Validation Loss: 821.2438, Test Loss: 769.6866\n",
      "Epoch 151, Training Loss: 808.9097, Validation Loss: 821.2264, Test Loss: 769.6877\n",
      "Epoch 161, Training Loss: 808.9099, Validation Loss: 821.1845, Test Loss: 769.6910\n",
      "Epoch 171, Training Loss: 808.9100, Validation Loss: 821.2656, Test Loss: 769.6848\n",
      "Epoch 181, Training Loss: 808.9099, Validation Loss: 821.2530, Test Loss: 769.6858\n",
      "Epoch 191, Training Loss: 808.9100, Validation Loss: 821.3746, Test Loss: 769.6772\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 808.9100\n",
      "Validation loss: 821.3746\n",
      "Test loss: 769.6772\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 12/243 with parameters:\n",
      "Config indices: (0, 0, 1, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 171.9869, Validation Loss: 178.9948, Test Loss: 175.2861\n",
      "Epoch 11, Training Loss: 8.7998, Validation Loss: 8.0332, Test Loss: 8.2244\n",
      "Epoch 21, Training Loss: 5.9447, Validation Loss: 6.3499, Test Loss: 5.7810\n",
      "Epoch 31, Training Loss: 2.4981, Validation Loss: 1.9143, Test Loss: 2.3025\n",
      "Epoch 41, Training Loss: 8.2564, Validation Loss: 9.3962, Test Loss: 7.5461\n",
      "Epoch 51, Training Loss: 2.5412, Validation Loss: 2.3901, Test Loss: 2.4339\n",
      "Epoch 61, Training Loss: 15.1721, Validation Loss: 13.4777, Test Loss: 15.7261\n",
      "Epoch 71, Training Loss: 5.4624, Validation Loss: 5.7242, Test Loss: 5.0698\n",
      "Epoch 81, Training Loss: 1.5921, Validation Loss: 1.4502, Test Loss: 1.5809\n",
      "Epoch 91, Training Loss: 3.6912, Validation Loss: 3.1090, Test Loss: 3.6729\n",
      "Epoch 101, Training Loss: 7.6495, Validation Loss: 7.4775, Test Loss: 7.5781\n",
      "Epoch 111, Training Loss: 1.4166, Validation Loss: 1.1642, Test Loss: 1.3941\n",
      "Epoch 121, Training Loss: 4.5742, Validation Loss: 4.0973, Test Loss: 4.4604\n",
      "Epoch 131, Training Loss: 2.6375, Validation Loss: 1.7613, Test Loss: 2.9347\n",
      "Epoch 141, Training Loss: 4.0347, Validation Loss: 3.8554, Test Loss: 4.1696\n",
      "Epoch 151, Training Loss: 0.9328, Validation Loss: 0.8861, Test Loss: 0.9936\n",
      "Epoch 161, Training Loss: 2.3975, Validation Loss: 2.0705, Test Loss: 2.3178\n",
      "Epoch 171, Training Loss: 2.0351, Validation Loss: 1.8891, Test Loss: 2.2115\n",
      "Epoch 181, Training Loss: 3.2178, Validation Loss: 2.7056, Test Loss: 3.0462\n",
      "Epoch 191, Training Loss: 0.6410, Validation Loss: 0.4704, Test Loss: 0.6473\n",
      "Epoch 201, Training Loss: 12.4373, Validation Loss: 12.3290, Test Loss: 12.7568\n",
      "Epoch 211, Training Loss: 0.7279, Validation Loss: 0.6024, Test Loss: 0.7333\n",
      "Epoch 221, Training Loss: 1.2029, Validation Loss: 1.1712, Test Loss: 1.3492\n",
      "Epoch 231, Training Loss: 12.6723, Validation Loss: 11.0553, Test Loss: 13.2303\n",
      "Epoch 241, Training Loss: 0.5650, Validation Loss: 0.4709, Test Loss: 0.6748\n",
      "Epoch 251, Training Loss: 13.6292, Validation Loss: 13.0851, Test Loss: 14.1173\n",
      "Epoch 261, Training Loss: 0.5793, Validation Loss: 0.4829, Test Loss: 0.6891\n",
      "Epoch 271, Training Loss: 0.5513, Validation Loss: 0.4175, Test Loss: 0.6433\n",
      "Epoch 281, Training Loss: 3.4627, Validation Loss: 3.2825, Test Loss: 3.3442\n",
      "Epoch 291, Training Loss: 0.4287, Validation Loss: 0.3159, Test Loss: 0.5041\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.4287\n",
      "Validation loss: 0.3159\n",
      "Test loss: 0.5041\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 13/243 with parameters:\n",
      "Config indices: (0, 0, 1, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 368.9008, Validation Loss: 362.5319, Test Loss: 359.4442\n",
      "Epoch 11, Training Loss: 37.1375, Validation Loss: 36.8958, Test Loss: 38.0432\n",
      "Epoch 21, Training Loss: 13.3075, Validation Loss: 12.5902, Test Loss: 13.4260\n",
      "Epoch 31, Training Loss: 27.5370, Validation Loss: 25.1506, Test Loss: 27.8170\n",
      "Epoch 41, Training Loss: 1.0710, Validation Loss: 0.8347, Test Loss: 1.2686\n",
      "Epoch 51, Training Loss: 0.8425, Validation Loss: 0.6088, Test Loss: 0.9209\n",
      "Epoch 61, Training Loss: 2.0509, Validation Loss: 1.8581, Test Loss: 2.2399\n",
      "Epoch 71, Training Loss: 0.5960, Validation Loss: 0.5050, Test Loss: 0.7112\n",
      "Epoch 81, Training Loss: 2.5413, Validation Loss: 2.2694, Test Loss: 2.7780\n",
      "Epoch 91, Training Loss: 3.2055, Validation Loss: 2.9230, Test Loss: 3.4444\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 3.2055\n",
      "Validation loss: 2.9230\n",
      "Test loss: 3.4444\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 14/243 with parameters:\n",
      "Config indices: (0, 0, 1, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 24571.0352, Validation Loss: 21921.5859, Test Loss: 24140.8359\n",
      "Epoch 11, Training Loss: 1968.1980, Validation Loss: 1714.3856, Test Loss: 1948.6351\n",
      "Epoch 21, Training Loss: 1051.7583, Validation Loss: 942.2775, Test Loss: 1021.5334\n",
      "Epoch 31, Training Loss: 860.0452, Validation Loss: 816.4794, Test Loss: 824.9512\n",
      "Epoch 41, Training Loss: 819.6645, Validation Loss: 806.3604, Test Loss: 782.3351\n",
      "Epoch 51, Training Loss: 811.2320, Validation Loss: 811.6503, Test Loss: 772.8892\n",
      "Epoch 61, Training Loss: 809.4212, Validation Loss: 816.1622, Test Loss: 770.6110\n",
      "Epoch 71, Training Loss: 809.0219, Validation Loss: 818.7342, Test Loss: 769.9925\n",
      "Epoch 81, Training Loss: 808.9298, Validation Loss: 820.1534, Test Loss: 769.7887\n",
      "Epoch 91, Training Loss: 808.9130, Validation Loss: 820.7983, Test Loss: 769.7230\n",
      "Epoch 101, Training Loss: 808.9111, Validation Loss: 820.9468, Test Loss: 769.7101\n",
      "Epoch 111, Training Loss: 808.9099, Validation Loss: 821.1729, Test Loss: 769.6920\n",
      "Epoch 121, Training Loss: 808.9106, Validation Loss: 821.0089, Test Loss: 769.7050\n",
      "Epoch 131, Training Loss: 808.9099, Validation Loss: 821.1299, Test Loss: 769.6953\n",
      "Epoch 141, Training Loss: 808.9104, Validation Loss: 821.0516, Test Loss: 769.7015\n",
      "Epoch 151, Training Loss: 808.9114, Validation Loss: 820.9283, Test Loss: 769.7117\n",
      "Epoch 161, Training Loss: 808.9099, Validation Loss: 821.2876, Test Loss: 769.6833\n",
      "Epoch 171, Training Loss: 808.9097, Validation Loss: 821.2946, Test Loss: 769.6829\n",
      "Epoch 181, Training Loss: 808.9099, Validation Loss: 821.1516, Test Loss: 769.6934\n",
      "Epoch 191, Training Loss: 808.9100, Validation Loss: 821.3513, Test Loss: 769.6788\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 808.9100\n",
      "Validation loss: 821.3513\n",
      "Test loss: 769.6788\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 15/243 with parameters:\n",
      "Config indices: (0, 0, 1, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 1203.1777, Validation Loss: 1114.1781, Test Loss: 1179.5259\n",
      "Epoch 11, Training Loss: 33.4521, Validation Loss: 28.1931, Test Loss: 32.7831\n",
      "Epoch 21, Training Loss: 1.3416, Validation Loss: 0.7717, Test Loss: 1.5163\n",
      "Epoch 31, Training Loss: 12.7720, Validation Loss: 10.9669, Test Loss: 13.1722\n",
      "Epoch 41, Training Loss: 0.6682, Validation Loss: 0.4170, Test Loss: 0.7734\n",
      "Epoch 51, Training Loss: 0.8287, Validation Loss: 0.7029, Test Loss: 1.0048\n",
      "Epoch 61, Training Loss: 0.9042, Validation Loss: 0.6645, Test Loss: 1.1147\n",
      "Epoch 71, Training Loss: 4.1112, Validation Loss: 3.5608, Test Loss: 4.4141\n",
      "Epoch 81, Training Loss: 1.4417, Validation Loss: 1.2057, Test Loss: 1.4151\n",
      "Epoch 91, Training Loss: 1.3606, Validation Loss: 1.0759, Test Loss: 1.5866\n",
      "Epoch 101, Training Loss: 0.6858, Validation Loss: 0.5243, Test Loss: 0.8136\n",
      "Epoch 111, Training Loss: 2.2197, Validation Loss: 1.7982, Test Loss: 2.3371\n",
      "Epoch 121, Training Loss: 0.6256, Validation Loss: 0.4635, Test Loss: 0.8322\n",
      "Epoch 131, Training Loss: 2.7427, Validation Loss: 2.3367, Test Loss: 3.0455\n",
      "Epoch 141, Training Loss: 0.4933, Validation Loss: 0.3254, Test Loss: 0.7071\n",
      "Epoch 151, Training Loss: 0.8551, Validation Loss: 0.6526, Test Loss: 0.9165\n",
      "Epoch 161, Training Loss: 0.4097, Validation Loss: 0.3062, Test Loss: 0.5696\n",
      "Epoch 171, Training Loss: 0.6613, Validation Loss: 0.5213, Test Loss: 0.7271\n",
      "Epoch 181, Training Loss: 0.8176, Validation Loss: 0.6927, Test Loss: 1.0256\n",
      "Epoch 191, Training Loss: 0.6309, Validation Loss: 0.5308, Test Loss: 0.8273\n",
      "Epoch 201, Training Loss: 0.4968, Validation Loss: 0.3763, Test Loss: 0.6150\n",
      "Epoch 211, Training Loss: 0.3866, Validation Loss: 0.3138, Test Loss: 0.5191\n",
      "Epoch 221, Training Loss: 0.7569, Validation Loss: 0.6135, Test Loss: 0.8237\n",
      "Epoch 231, Training Loss: 0.4187, Validation Loss: 0.3243, Test Loss: 0.5441\n",
      "Epoch 241, Training Loss: 0.5245, Validation Loss: 0.4649, Test Loss: 0.6941\n",
      "Epoch 251, Training Loss: 0.7996, Validation Loss: 0.7974, Test Loss: 0.9033\n",
      "Epoch 261, Training Loss: 0.7007, Validation Loss: 0.5696, Test Loss: 0.7898\n",
      "Epoch 271, Training Loss: 3.5623, Validation Loss: 3.2053, Test Loss: 3.8220\n",
      "Epoch 281, Training Loss: 0.5980, Validation Loss: 0.5241, Test Loss: 0.7921\n",
      "Epoch 291, Training Loss: 0.4204, Validation Loss: 0.3351, Test Loss: 0.5883\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.4204\n",
      "Validation loss: 0.3351\n",
      "Test loss: 0.5883\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 16/243 with parameters:\n",
      "Config indices: (0, 0, 1, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 5672.3091, Validation Loss: 5148.8857, Test Loss: 5676.9033\n",
      "Epoch 11, Training Loss: 3518.2720, Validation Loss: 3123.7405, Test Loss: 3509.1045\n",
      "Epoch 21, Training Loss: 2023.9985, Validation Loss: 1763.8595, Test Loss: 2004.9030\n",
      "Epoch 31, Training Loss: 1354.3951, Validation Loss: 1184.1658, Test Loss: 1328.6580\n",
      "Epoch 41, Training Loss: 1054.3458, Validation Loss: 944.2195, Test Loss: 1024.1687\n",
      "Epoch 51, Training Loss: 919.0779, Validation Loss: 849.3654, Test Loss: 885.9157\n",
      "Epoch 61, Training Loss: 857.9860, Validation Loss: 815.5583, Test Loss: 822.8080\n",
      "Epoch 71, Training Loss: 831.1025, Validation Loss: 806.6100, Test Loss: 794.5996\n",
      "Epoch 81, Training Loss: 818.7777, Validation Loss: 806.5540, Test Loss: 781.3685\n",
      "Epoch 91, Training Loss: 813.2512, Validation Loss: 809.2952, Test Loss: 775.2314\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 813.2512\n",
      "Validation loss: 809.2952\n",
      "Test loss: 775.2314\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 17/243 with parameters:\n",
      "Config indices: (0, 0, 1, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6467.2900, Validation Loss: 5891.4565, Test Loss: 6471.5879\n",
      "Epoch 11, Training Loss: 3474.1899, Validation Loss: 3083.3115, Test Loss: 3464.9309\n",
      "Epoch 21, Training Loss: 2007.3602, Validation Loss: 1749.2731, Test Loss: 1988.2107\n",
      "Epoch 31, Training Loss: 1346.6697, Validation Loss: 1177.8292, Test Loss: 1320.8801\n",
      "Epoch 41, Training Loss: 1050.3228, Validation Loss: 941.2421, Test Loss: 1020.0900\n",
      "Epoch 51, Training Loss: 917.0395, Validation Loss: 848.0927, Test Loss: 883.8233\n",
      "Epoch 61, Training Loss: 856.8902, Validation Loss: 815.0571, Test Loss: 821.6586\n",
      "Epoch 71, Training Loss: 830.2773, Validation Loss: 806.4402, Test Loss: 793.7079\n",
      "Epoch 81, Training Loss: 818.4730, Validation Loss: 806.5853, Test Loss: 781.0154\n",
      "Epoch 91, Training Loss: 813.2150, Validation Loss: 809.2742, Test Loss: 775.1666\n",
      "Epoch 101, Training Loss: 810.9341, Validation Loss: 812.0861, Test Loss: 772.5071\n",
      "Epoch 111, Training Loss: 809.8015, Validation Loss: 814.6901, Test Loss: 771.0970\n",
      "Epoch 121, Training Loss: 809.3223, Validation Loss: 816.5688, Test Loss: 770.4426\n",
      "Epoch 131, Training Loss: 809.0873, Validation Loss: 818.0596, Test Loss: 770.0793\n",
      "Epoch 141, Training Loss: 808.9693, Validation Loss: 819.3300, Test Loss: 769.8577\n",
      "Epoch 151, Training Loss: 808.9372, Validation Loss: 819.9072, Test Loss: 769.7805\n",
      "Epoch 161, Training Loss: 808.9163, Validation Loss: 820.5435, Test Loss: 769.7109\n",
      "Epoch 171, Training Loss: 808.9118, Validation Loss: 820.8229, Test Loss: 769.6851\n",
      "Epoch 181, Training Loss: 808.9102, Validation Loss: 820.9863, Test Loss: 769.6714\n",
      "Epoch 191, Training Loss: 808.9104, Validation Loss: 820.9610, Test Loss: 769.6735\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 808.9104\n",
      "Validation loss: 820.9610\n",
      "Test loss: 769.6735\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 18/243 with parameters:\n",
      "Config indices: (0, 0, 1, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7498.2656, Validation Loss: 6665.5913, Test Loss: 7363.3784\n",
      "Epoch 11, Training Loss: 104.9009, Validation Loss: 103.1927, Test Loss: 107.7661\n",
      "Epoch 21, Training Loss: 21.1743, Validation Loss: 18.7864, Test Loss: 20.3461\n",
      "Epoch 31, Training Loss: 40.4517, Validation Loss: 39.4592, Test Loss: 41.9467\n",
      "Epoch 41, Training Loss: 54.6138, Validation Loss: 46.5081, Test Loss: 57.5614\n",
      "Epoch 51, Training Loss: 31.9486, Validation Loss: 30.2181, Test Loss: 30.3645\n",
      "Epoch 61, Training Loss: 99.3628, Validation Loss: 91.9952, Test Loss: 94.8544\n",
      "Epoch 71, Training Loss: 25.7318, Validation Loss: 26.4396, Test Loss: 27.0729\n",
      "Epoch 81, Training Loss: 63.4475, Validation Loss: 56.3817, Test Loss: 67.1093\n",
      "Epoch 91, Training Loss: 68.6928, Validation Loss: 60.1078, Test Loss: 67.5883\n",
      "Epoch 101, Training Loss: 59.7423, Validation Loss: 56.6310, Test Loss: 60.3765\n",
      "Epoch 111, Training Loss: 5.3377, Validation Loss: 4.4966, Test Loss: 4.8918\n",
      "Epoch 121, Training Loss: 46.0486, Validation Loss: 40.5776, Test Loss: 45.2787\n",
      "Epoch 131, Training Loss: 15.3890, Validation Loss: 14.7663, Test Loss: 15.5140\n",
      "Epoch 141, Training Loss: 20.9718, Validation Loss: 19.2196, Test Loss: 20.2739\n",
      "Epoch 151, Training Loss: 11.7079, Validation Loss: 10.7348, Test Loss: 11.3714\n",
      "Epoch 161, Training Loss: 62.9393, Validation Loss: 57.9685, Test Loss: 63.0636\n",
      "Epoch 171, Training Loss: 7.4303, Validation Loss: 7.2341, Test Loss: 7.1529\n",
      "Epoch 181, Training Loss: 4.9038, Validation Loss: 4.4103, Test Loss: 4.6668\n",
      "Epoch 191, Training Loss: 38.5460, Validation Loss: 34.7364, Test Loss: 38.1827\n",
      "Epoch 201, Training Loss: 10.0373, Validation Loss: 9.2769, Test Loss: 9.6206\n",
      "Epoch 211, Training Loss: 11.8818, Validation Loss: 10.3502, Test Loss: 11.4930\n",
      "Epoch 221, Training Loss: 1.2007, Validation Loss: 0.8767, Test Loss: 1.0946\n",
      "Epoch 231, Training Loss: 1.2575, Validation Loss: 0.9030, Test Loss: 1.1593\n",
      "Epoch 241, Training Loss: 9.1126, Validation Loss: 8.0742, Test Loss: 8.6314\n",
      "Epoch 251, Training Loss: 11.3738, Validation Loss: 10.2407, Test Loss: 11.0563\n",
      "Epoch 261, Training Loss: 4.6697, Validation Loss: 4.1286, Test Loss: 4.4753\n",
      "Epoch 271, Training Loss: 6.3770, Validation Loss: 6.3659, Test Loss: 6.7927\n",
      "Epoch 281, Training Loss: 8.4642, Validation Loss: 8.0583, Test Loss: 8.6233\n",
      "Epoch 291, Training Loss: 0.7778, Validation Loss: 0.5358, Test Loss: 0.7951\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.7778\n",
      "Validation loss: 0.5358\n",
      "Test loss: 0.7951\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 19/243 with parameters:\n",
      "Config indices: (0, 0, 2, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6970.3311, Validation Loss: 6369.0635, Test Loss: 6976.4297\n",
      "Epoch 11, Training Loss: 0.9365, Validation Loss: 0.6291, Test Loss: 1.0419\n",
      "Epoch 21, Training Loss: 0.6320, Validation Loss: 0.4427, Test Loss: 0.7596\n",
      "Epoch 31, Training Loss: 0.5914, Validation Loss: 0.4104, Test Loss: 0.7368\n",
      "Epoch 41, Training Loss: 0.5803, Validation Loss: 0.4135, Test Loss: 0.6570\n",
      "Epoch 51, Training Loss: 0.4986, Validation Loss: 0.3383, Test Loss: 0.6274\n",
      "Epoch 61, Training Loss: 0.4754, Validation Loss: 0.3205, Test Loss: 0.6013\n",
      "Epoch 71, Training Loss: 0.4732, Validation Loss: 0.3107, Test Loss: 0.6022\n",
      "Epoch 81, Training Loss: 0.4614, Validation Loss: 0.3049, Test Loss: 0.5546\n",
      "Epoch 91, Training Loss: 0.4384, Validation Loss: 0.2885, Test Loss: 0.5714\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.4384\n",
      "Validation loss: 0.2885\n",
      "Test loss: 0.5714\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 20/243 with parameters:\n",
      "Config indices: (0, 0, 2, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6949.3442, Validation Loss: 6349.1382, Test Loss: 6955.4502\n",
      "Epoch 11, Training Loss: 0.7125, Validation Loss: 0.4449, Test Loss: 0.9409\n",
      "Epoch 21, Training Loss: 0.6710, Validation Loss: 0.4816, Test Loss: 0.7836\n",
      "Epoch 31, Training Loss: 0.5326, Validation Loss: 0.3531, Test Loss: 0.7026\n",
      "Epoch 41, Training Loss: 0.5018, Validation Loss: 0.3442, Test Loss: 0.6445\n",
      "Epoch 51, Training Loss: 0.5784, Validation Loss: 0.3948, Test Loss: 0.7708\n",
      "Epoch 61, Training Loss: 0.4662, Validation Loss: 0.3164, Test Loss: 0.6030\n",
      "Epoch 71, Training Loss: 0.4422, Validation Loss: 0.2926, Test Loss: 0.5795\n",
      "Epoch 81, Training Loss: 0.4277, Validation Loss: 0.2821, Test Loss: 0.5649\n",
      "Epoch 91, Training Loss: 0.4204, Validation Loss: 0.2802, Test Loss: 0.5412\n",
      "Epoch 101, Training Loss: 0.4088, Validation Loss: 0.2618, Test Loss: 0.5358\n",
      "Epoch 111, Training Loss: 0.4498, Validation Loss: 0.3088, Test Loss: 0.5390\n",
      "Epoch 121, Training Loss: 0.3960, Validation Loss: 0.2776, Test Loss: 0.5412\n",
      "Epoch 131, Training Loss: 0.3732, Validation Loss: 0.2534, Test Loss: 0.4889\n",
      "Epoch 141, Training Loss: 0.3641, Validation Loss: 0.2485, Test Loss: 0.4933\n",
      "Epoch 151, Training Loss: 0.3620, Validation Loss: 0.2415, Test Loss: 0.4730\n",
      "Epoch 161, Training Loss: 0.3467, Validation Loss: 0.2358, Test Loss: 0.4549\n",
      "Epoch 171, Training Loss: 0.3475, Validation Loss: 0.2271, Test Loss: 0.4523\n",
      "Epoch 181, Training Loss: 0.3462, Validation Loss: 0.2386, Test Loss: 0.4431\n",
      "Epoch 191, Training Loss: 0.3378, Validation Loss: 0.2331, Test Loss: 0.4325\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.3378\n",
      "Validation loss: 0.2331\n",
      "Test loss: 0.4325\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 21/243 with parameters:\n",
      "Config indices: (0, 0, 2, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6954.1143, Validation Loss: 6353.6880, Test Loss: 6960.1650\n",
      "Epoch 11, Training Loss: 0.7761, Validation Loss: 0.4909, Test Loss: 0.8730\n",
      "Epoch 21, Training Loss: 0.5938, Validation Loss: 0.3836, Test Loss: 0.6875\n",
      "Epoch 31, Training Loss: 0.5461, Validation Loss: 0.3603, Test Loss: 0.6443\n",
      "Epoch 41, Training Loss: 0.5702, Validation Loss: 0.3759, Test Loss: 0.6363\n",
      "Epoch 51, Training Loss: 0.4956, Validation Loss: 0.3189, Test Loss: 0.5896\n",
      "Epoch 61, Training Loss: 0.4966, Validation Loss: 0.3355, Test Loss: 0.5788\n",
      "Epoch 71, Training Loss: 0.4591, Validation Loss: 0.3080, Test Loss: 0.5633\n",
      "Epoch 81, Training Loss: 0.4443, Validation Loss: 0.2966, Test Loss: 0.5519\n",
      "Epoch 91, Training Loss: 0.4543, Validation Loss: 0.3232, Test Loss: 0.5463\n",
      "Epoch 101, Training Loss: 0.4144, Validation Loss: 0.2773, Test Loss: 0.5230\n",
      "Epoch 111, Training Loss: 0.5211, Validation Loss: 0.3848, Test Loss: 0.6674\n",
      "Epoch 121, Training Loss: 0.4238, Validation Loss: 0.2913, Test Loss: 0.5557\n",
      "Epoch 131, Training Loss: 0.3799, Validation Loss: 0.2491, Test Loss: 0.4795\n",
      "Epoch 141, Training Loss: 0.3718, Validation Loss: 0.2414, Test Loss: 0.4718\n",
      "Epoch 151, Training Loss: 0.3654, Validation Loss: 0.2484, Test Loss: 0.4593\n",
      "Epoch 161, Training Loss: 0.3520, Validation Loss: 0.2348, Test Loss: 0.4490\n",
      "Epoch 171, Training Loss: 0.3470, Validation Loss: 0.2334, Test Loss: 0.4386\n",
      "Epoch 181, Training Loss: 0.3382, Validation Loss: 0.2247, Test Loss: 0.4454\n",
      "Epoch 191, Training Loss: 0.3262, Validation Loss: 0.2174, Test Loss: 0.4223\n",
      "Epoch 201, Training Loss: 0.3516, Validation Loss: 0.2393, Test Loss: 0.4271\n",
      "Epoch 211, Training Loss: 0.3206, Validation Loss: 0.2196, Test Loss: 0.4331\n",
      "Epoch 221, Training Loss: 0.3100, Validation Loss: 0.2052, Test Loss: 0.4047\n",
      "Epoch 231, Training Loss: 0.3345, Validation Loss: 0.2379, Test Loss: 0.4580\n",
      "Epoch 241, Training Loss: 0.2933, Validation Loss: 0.2037, Test Loss: 0.3945\n",
      "Epoch 251, Training Loss: 0.2951, Validation Loss: 0.2117, Test Loss: 0.3958\n",
      "Epoch 261, Training Loss: 0.2857, Validation Loss: 0.2021, Test Loss: 0.3918\n",
      "Epoch 271, Training Loss: 0.2817, Validation Loss: 0.1965, Test Loss: 0.3866\n",
      "Epoch 281, Training Loss: 0.2815, Validation Loss: 0.1971, Test Loss: 0.3666\n",
      "Epoch 291, Training Loss: 0.3027, Validation Loss: 0.2197, Test Loss: 0.3835\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.3027\n",
      "Validation loss: 0.2197\n",
      "Test loss: 0.3835\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 22/243 with parameters:\n",
      "Config indices: (0, 0, 2, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7062.0664, Validation Loss: 6456.1836, Test Loss: 7068.5186\n",
      "Epoch 11, Training Loss: 1.5975, Validation Loss: 1.1856, Test Loss: 1.7414\n",
      "Epoch 21, Training Loss: 0.7670, Validation Loss: 0.5672, Test Loss: 0.9300\n",
      "Epoch 31, Training Loss: 0.6351, Validation Loss: 0.4624, Test Loss: 0.7765\n",
      "Epoch 41, Training Loss: 0.6432, Validation Loss: 0.4811, Test Loss: 0.7556\n",
      "Epoch 51, Training Loss: 0.5855, Validation Loss: 0.4227, Test Loss: 0.7028\n",
      "Epoch 61, Training Loss: 0.5627, Validation Loss: 0.4021, Test Loss: 0.6837\n",
      "Epoch 71, Training Loss: 0.5335, Validation Loss: 0.3625, Test Loss: 0.6834\n",
      "Epoch 81, Training Loss: 0.5166, Validation Loss: 0.3501, Test Loss: 0.6416\n",
      "Epoch 91, Training Loss: 0.5055, Validation Loss: 0.3390, Test Loss: 0.6546\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.5055\n",
      "Validation loss: 0.3390\n",
      "Test loss: 0.6546\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 23/243 with parameters:\n",
      "Config indices: (0, 0, 2, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7066.5845, Validation Loss: 6460.5835, Test Loss: 7073.0396\n",
      "Epoch 11, Training Loss: 1.9289, Validation Loss: 1.3497, Test Loss: 2.0078\n",
      "Epoch 21, Training Loss: 0.7816, Validation Loss: 0.5521, Test Loss: 0.9016\n",
      "Epoch 31, Training Loss: 0.7049, Validation Loss: 0.5200, Test Loss: 0.7889\n",
      "Epoch 41, Training Loss: 0.6686, Validation Loss: 0.4906, Test Loss: 0.7376\n",
      "Epoch 51, Training Loss: 0.6028, Validation Loss: 0.4324, Test Loss: 0.6995\n",
      "Epoch 61, Training Loss: 0.5826, Validation Loss: 0.4137, Test Loss: 0.6773\n",
      "Epoch 71, Training Loss: 0.5702, Validation Loss: 0.4047, Test Loss: 0.6531\n",
      "Epoch 81, Training Loss: 0.5732, Validation Loss: 0.4098, Test Loss: 0.6508\n",
      "Epoch 91, Training Loss: 0.5472, Validation Loss: 0.3885, Test Loss: 0.6325\n",
      "Epoch 101, Training Loss: 0.5406, Validation Loss: 0.3803, Test Loss: 0.6537\n",
      "Epoch 111, Training Loss: 0.5191, Validation Loss: 0.3663, Test Loss: 0.6278\n",
      "Epoch 121, Training Loss: 0.5008, Validation Loss: 0.3524, Test Loss: 0.6025\n",
      "Epoch 131, Training Loss: 0.4892, Validation Loss: 0.3434, Test Loss: 0.5799\n",
      "Epoch 141, Training Loss: 0.4875, Validation Loss: 0.3358, Test Loss: 0.5631\n",
      "Epoch 151, Training Loss: 0.4732, Validation Loss: 0.3317, Test Loss: 0.5626\n",
      "Epoch 161, Training Loss: 0.4656, Validation Loss: 0.3145, Test Loss: 0.5699\n",
      "Epoch 171, Training Loss: 0.5339, Validation Loss: 0.3833, Test Loss: 0.5854\n",
      "Epoch 181, Training Loss: 0.4441, Validation Loss: 0.2994, Test Loss: 0.5381\n",
      "Epoch 191, Training Loss: 0.4463, Validation Loss: 0.3065, Test Loss: 0.5258\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.4463\n",
      "Validation loss: 0.3065\n",
      "Test loss: 0.5258\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 24/243 with parameters:\n",
      "Config indices: (0, 0, 2, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7065.2466, Validation Loss: 6459.3184, Test Loss: 7071.6875\n",
      "Epoch 11, Training Loss: 2.1558, Validation Loss: 1.7672, Test Loss: 2.0939\n",
      "Epoch 21, Training Loss: 0.8917, Validation Loss: 0.7327, Test Loss: 0.9947\n",
      "Epoch 31, Training Loss: 0.7748, Validation Loss: 0.6156, Test Loss: 0.9439\n",
      "Epoch 41, Training Loss: 0.6929, Validation Loss: 0.5660, Test Loss: 0.8252\n",
      "Epoch 51, Training Loss: 0.7120, Validation Loss: 0.6038, Test Loss: 0.8041\n",
      "Epoch 61, Training Loss: 0.6375, Validation Loss: 0.5206, Test Loss: 0.7630\n",
      "Epoch 71, Training Loss: 0.6565, Validation Loss: 0.5229, Test Loss: 0.8156\n",
      "Epoch 81, Training Loss: 0.5768, Validation Loss: 0.4570, Test Loss: 0.6975\n",
      "Epoch 91, Training Loss: 0.5485, Validation Loss: 0.4198, Test Loss: 0.6813\n",
      "Epoch 101, Training Loss: 0.5432, Validation Loss: 0.4281, Test Loss: 0.6577\n",
      "Epoch 111, Training Loss: 0.5144, Validation Loss: 0.3801, Test Loss: 0.6505\n",
      "Epoch 121, Training Loss: 0.5069, Validation Loss: 0.3670, Test Loss: 0.6577\n",
      "Epoch 131, Training Loss: 0.4827, Validation Loss: 0.3499, Test Loss: 0.6022\n",
      "Epoch 141, Training Loss: 0.4638, Validation Loss: 0.3282, Test Loss: 0.5968\n",
      "Epoch 151, Training Loss: 0.4533, Validation Loss: 0.3228, Test Loss: 0.5808\n",
      "Epoch 161, Training Loss: 0.4470, Validation Loss: 0.3051, Test Loss: 0.5919\n",
      "Epoch 171, Training Loss: 0.4287, Validation Loss: 0.3002, Test Loss: 0.5611\n",
      "Epoch 181, Training Loss: 0.4214, Validation Loss: 0.2925, Test Loss: 0.5477\n",
      "Epoch 191, Training Loss: 0.4160, Validation Loss: 0.2794, Test Loss: 0.5616\n",
      "Epoch 201, Training Loss: 0.4579, Validation Loss: 0.3133, Test Loss: 0.6304\n",
      "Epoch 211, Training Loss: 0.4070, Validation Loss: 0.2728, Test Loss: 0.5564\n",
      "Epoch 221, Training Loss: 0.3972, Validation Loss: 0.2768, Test Loss: 0.5120\n",
      "Epoch 231, Training Loss: 0.3802, Validation Loss: 0.2618, Test Loss: 0.5134\n",
      "Epoch 241, Training Loss: 0.3726, Validation Loss: 0.2538, Test Loss: 0.4972\n",
      "Epoch 251, Training Loss: 0.3747, Validation Loss: 0.2547, Test Loss: 0.4916\n",
      "Epoch 261, Training Loss: 0.3847, Validation Loss: 0.2562, Test Loss: 0.5296\n",
      "Epoch 271, Training Loss: 0.3583, Validation Loss: 0.2410, Test Loss: 0.4736\n",
      "Epoch 281, Training Loss: 0.3498, Validation Loss: 0.2379, Test Loss: 0.4778\n",
      "Epoch 291, Training Loss: 0.3680, Validation Loss: 0.2550, Test Loss: 0.4713\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.3680\n",
      "Validation loss: 0.2550\n",
      "Test loss: 0.4713\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 25/243 with parameters:\n",
      "Config indices: (0, 0, 2, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7115.8247, Validation Loss: 6507.3945, Test Loss: 7122.4497\n",
      "Epoch 11, Training Loss: 6051.7314, Validation Loss: 5502.2422, Test Loss: 6055.6211\n",
      "Epoch 21, Training Loss: 4.0424, Validation Loss: 3.2087, Test Loss: 3.7014\n",
      "Epoch 31, Training Loss: 1.4136, Validation Loss: 1.0407, Test Loss: 1.4170\n",
      "Epoch 41, Training Loss: 0.9398, Validation Loss: 0.6760, Test Loss: 1.0248\n",
      "Epoch 51, Training Loss: 0.7498, Validation Loss: 0.5374, Test Loss: 0.8089\n",
      "Epoch 61, Training Loss: 0.6919, Validation Loss: 0.5104, Test Loss: 0.7742\n",
      "Epoch 71, Training Loss: 0.6335, Validation Loss: 0.4629, Test Loss: 0.7116\n",
      "Epoch 81, Training Loss: 0.5975, Validation Loss: 0.4279, Test Loss: 0.6680\n",
      "Epoch 91, Training Loss: 0.6580, Validation Loss: 0.4840, Test Loss: 0.7724\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.6580\n",
      "Validation loss: 0.4840\n",
      "Test loss: 0.7724\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 26/243 with parameters:\n",
      "Config indices: (0, 0, 2, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7114.6899, Validation Loss: 6506.2749, Test Loss: 7121.3174\n",
      "Epoch 11, Training Loss: 138.7722, Validation Loss: 140.9900, Test Loss: 134.0489\n",
      "Epoch 21, Training Loss: 1.9155, Validation Loss: 1.5028, Test Loss: 1.9261\n",
      "Epoch 31, Training Loss: 1.1388, Validation Loss: 0.8418, Test Loss: 1.1997\n",
      "Epoch 41, Training Loss: 0.8896, Validation Loss: 0.6627, Test Loss: 0.9939\n",
      "Epoch 51, Training Loss: 0.8044, Validation Loss: 0.5944, Test Loss: 0.8877\n",
      "Epoch 61, Training Loss: 0.7035, Validation Loss: 0.5132, Test Loss: 0.7946\n",
      "Epoch 71, Training Loss: 0.6529, Validation Loss: 0.4740, Test Loss: 0.7456\n",
      "Epoch 81, Training Loss: 0.6395, Validation Loss: 0.4713, Test Loss: 0.7158\n",
      "Epoch 91, Training Loss: 0.5881, Validation Loss: 0.4286, Test Loss: 0.6845\n",
      "Epoch 101, Training Loss: 0.5697, Validation Loss: 0.4121, Test Loss: 0.6664\n",
      "Epoch 111, Training Loss: 0.6102, Validation Loss: 0.4630, Test Loss: 0.6736\n",
      "Epoch 121, Training Loss: 0.5400, Validation Loss: 0.3909, Test Loss: 0.6286\n",
      "Epoch 131, Training Loss: 0.5788, Validation Loss: 0.4393, Test Loss: 0.6438\n",
      "Epoch 141, Training Loss: 0.5325, Validation Loss: 0.3845, Test Loss: 0.6431\n",
      "Epoch 151, Training Loss: 0.5457, Validation Loss: 0.4085, Test Loss: 0.6161\n",
      "Epoch 161, Training Loss: 0.5633, Validation Loss: 0.4278, Test Loss: 0.6307\n",
      "Epoch 171, Training Loss: 0.5260, Validation Loss: 0.3772, Test Loss: 0.6499\n",
      "Epoch 181, Training Loss: 0.4951, Validation Loss: 0.3573, Test Loss: 0.6055\n",
      "Epoch 191, Training Loss: 0.5286, Validation Loss: 0.3957, Test Loss: 0.6008\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.5286\n",
      "Validation loss: 0.3957\n",
      "Test loss: 0.6008\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 27/243 with parameters:\n",
      "Config indices: (0, 0, 2, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7116.5527, Validation Loss: 6508.0747, Test Loss: 7123.1875\n",
      "Epoch 11, Training Loss: 6508.9141, Validation Loss: 5930.7373, Test Loss: 6513.4028\n",
      "Epoch 21, Training Loss: 6.2561, Validation Loss: 5.2048, Test Loss: 6.0706\n",
      "Epoch 31, Training Loss: 1.4563, Validation Loss: 1.0417, Test Loss: 1.7654\n",
      "Epoch 41, Training Loss: 0.9696, Validation Loss: 0.6769, Test Loss: 1.2423\n",
      "Epoch 51, Training Loss: 0.8080, Validation Loss: 0.5837, Test Loss: 1.0181\n",
      "Epoch 61, Training Loss: 0.7404, Validation Loss: 0.5443, Test Loss: 0.9260\n",
      "Epoch 71, Training Loss: 0.7676, Validation Loss: 0.5519, Test Loss: 0.9963\n",
      "Epoch 81, Training Loss: 0.6575, Validation Loss: 0.4787, Test Loss: 0.8181\n",
      "Epoch 91, Training Loss: 0.6371, Validation Loss: 0.4618, Test Loss: 0.7899\n",
      "Epoch 101, Training Loss: 0.6280, Validation Loss: 0.4520, Test Loss: 0.7683\n",
      "Epoch 111, Training Loss: 0.5882, Validation Loss: 0.4173, Test Loss: 0.7415\n",
      "Epoch 121, Training Loss: 0.5569, Validation Loss: 0.3863, Test Loss: 0.7301\n",
      "Epoch 131, Training Loss: 0.5674, Validation Loss: 0.3890, Test Loss: 0.7570\n",
      "Epoch 141, Training Loss: 0.5230, Validation Loss: 0.3624, Test Loss: 0.6898\n",
      "Epoch 151, Training Loss: 0.5123, Validation Loss: 0.3476, Test Loss: 0.6867\n",
      "Epoch 161, Training Loss: 0.5056, Validation Loss: 0.3435, Test Loss: 0.6773\n",
      "Epoch 171, Training Loss: 0.5128, Validation Loss: 0.3436, Test Loss: 0.6993\n",
      "Epoch 181, Training Loss: 0.4837, Validation Loss: 0.3240, Test Loss: 0.6515\n",
      "Epoch 191, Training Loss: 0.4791, Validation Loss: 0.3179, Test Loss: 0.6535\n",
      "Epoch 201, Training Loss: 0.4809, Validation Loss: 0.3269, Test Loss: 0.6352\n",
      "Epoch 211, Training Loss: 0.4686, Validation Loss: 0.3111, Test Loss: 0.6411\n",
      "Epoch 221, Training Loss: 0.4758, Validation Loss: 0.3113, Test Loss: 0.6566\n",
      "Epoch 231, Training Loss: 0.4586, Validation Loss: 0.3031, Test Loss: 0.6250\n",
      "Epoch 241, Training Loss: 0.4529, Validation Loss: 0.2967, Test Loss: 0.6227\n",
      "Epoch 251, Training Loss: 0.4482, Validation Loss: 0.2934, Test Loss: 0.6164\n",
      "Epoch 261, Training Loss: 0.4639, Validation Loss: 0.3009, Test Loss: 0.6448\n",
      "Epoch 271, Training Loss: 0.4433, Validation Loss: 0.2876, Test Loss: 0.6122\n",
      "Epoch 281, Training Loss: 0.4437, Validation Loss: 0.2947, Test Loss: 0.6195\n",
      "Epoch 291, Training Loss: 0.4336, Validation Loss: 0.2851, Test Loss: 0.5981\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.4336\n",
      "Validation loss: 0.2851\n",
      "Test loss: 0.5981\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 28/243 with parameters:\n",
      "Config indices: (0, 1, 0, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 336.6583, Validation Loss: 340.4443, Test Loss: 341.8528\n",
      "Epoch 11, Training Loss: 0.6711, Validation Loss: 0.4437, Test Loss: 0.9145\n",
      "Epoch 21, Training Loss: 0.4818, Validation Loss: 0.3147, Test Loss: 0.6591\n",
      "Epoch 31, Training Loss: 1.0993, Validation Loss: 0.8666, Test Loss: 1.3337\n",
      "Epoch 41, Training Loss: 0.6698, Validation Loss: 0.5322, Test Loss: 0.8102\n",
      "Epoch 51, Training Loss: 0.4756, Validation Loss: 0.3500, Test Loss: 0.5066\n",
      "Epoch 61, Training Loss: 0.3290, Validation Loss: 0.2906, Test Loss: 0.4389\n",
      "Epoch 71, Training Loss: 1.1612, Validation Loss: 1.0058, Test Loss: 1.3536\n",
      "Epoch 81, Training Loss: 0.2662, Validation Loss: 0.2136, Test Loss: 0.3663\n",
      "Epoch 91, Training Loss: 0.5026, Validation Loss: 0.3815, Test Loss: 0.5061\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.5026\n",
      "Validation loss: 0.3815\n",
      "Test loss: 0.5061\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 29/243 with parameters:\n",
      "Config indices: (0, 1, 0, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 219.0550, Validation Loss: 220.1135, Test Loss: 212.4782\n",
      "Epoch 11, Training Loss: 0.5077, Validation Loss: 0.3482, Test Loss: 0.6617\n",
      "Epoch 21, Training Loss: 0.4071, Validation Loss: 0.2950, Test Loss: 0.5292\n",
      "Epoch 31, Training Loss: 0.3662, Validation Loss: 0.2730, Test Loss: 0.4617\n",
      "Epoch 41, Training Loss: 0.2792, Validation Loss: 0.2106, Test Loss: 0.3912\n",
      "Epoch 51, Training Loss: 0.2486, Validation Loss: 0.1734, Test Loss: 0.3287\n",
      "Epoch 61, Training Loss: 0.3476, Validation Loss: 0.3072, Test Loss: 0.4570\n",
      "Epoch 71, Training Loss: 0.3418, Validation Loss: 0.2748, Test Loss: 0.3838\n",
      "Epoch 81, Training Loss: 2.2086, Validation Loss: 1.9786, Test Loss: 2.4131\n",
      "Epoch 91, Training Loss: 1.7252, Validation Loss: 1.6200, Test Loss: 1.8978\n",
      "Epoch 101, Training Loss: 2.8688, Validation Loss: 2.5774, Test Loss: 3.0965\n",
      "Epoch 111, Training Loss: 0.6908, Validation Loss: 0.6139, Test Loss: 0.8255\n",
      "Epoch 121, Training Loss: 0.8417, Validation Loss: 0.6568, Test Loss: 0.8149\n",
      "Epoch 131, Training Loss: 0.6133, Validation Loss: 0.5448, Test Loss: 0.7436\n",
      "Epoch 141, Training Loss: 0.2921, Validation Loss: 0.2121, Test Loss: 0.3291\n",
      "Epoch 151, Training Loss: 0.3742, Validation Loss: 0.3409, Test Loss: 0.4706\n",
      "Epoch 161, Training Loss: 0.4414, Validation Loss: 0.3788, Test Loss: 0.5480\n",
      "Epoch 171, Training Loss: 0.8073, Validation Loss: 0.7215, Test Loss: 0.9389\n",
      "Epoch 181, Training Loss: 0.8972, Validation Loss: 0.8260, Test Loss: 0.9192\n",
      "Epoch 191, Training Loss: 0.5298, Validation Loss: 0.5139, Test Loss: 0.6080\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.5298\n",
      "Validation loss: 0.5139\n",
      "Test loss: 0.6080\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 30/243 with parameters:\n",
      "Config indices: (0, 1, 0, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 162.7449, Validation Loss: 158.1247, Test Loss: 165.3002\n",
      "Epoch 11, Training Loss: 0.7897, Validation Loss: 0.5823, Test Loss: 0.8637\n",
      "Epoch 21, Training Loss: 0.5277, Validation Loss: 0.3503, Test Loss: 0.6914\n",
      "Epoch 31, Training Loss: 1.7329, Validation Loss: 1.4775, Test Loss: 1.9608\n",
      "Epoch 41, Training Loss: 0.7034, Validation Loss: 0.5870, Test Loss: 0.8616\n",
      "Epoch 51, Training Loss: 0.2665, Validation Loss: 0.1976, Test Loss: 0.3363\n",
      "Epoch 61, Training Loss: 1.7194, Validation Loss: 1.5197, Test Loss: 1.6689\n",
      "Epoch 71, Training Loss: 0.4965, Validation Loss: 0.4475, Test Loss: 0.6216\n",
      "Epoch 81, Training Loss: 0.5497, Validation Loss: 0.5164, Test Loss: 0.6422\n",
      "Epoch 91, Training Loss: 0.2680, Validation Loss: 0.2340, Test Loss: 0.3364\n",
      "Epoch 101, Training Loss: 0.4130, Validation Loss: 0.4056, Test Loss: 0.4748\n",
      "Epoch 111, Training Loss: 0.6861, Validation Loss: 0.6073, Test Loss: 0.7155\n",
      "Epoch 121, Training Loss: 0.2121, Validation Loss: 0.1600, Test Loss: 0.2839\n",
      "Epoch 131, Training Loss: 0.5273, Validation Loss: 0.5218, Test Loss: 0.5763\n",
      "Epoch 141, Training Loss: 0.4054, Validation Loss: 0.3557, Test Loss: 0.5193\n",
      "Epoch 151, Training Loss: 0.8304, Validation Loss: 0.7287, Test Loss: 0.9959\n",
      "Epoch 161, Training Loss: 0.6409, Validation Loss: 0.6257, Test Loss: 0.6380\n",
      "Epoch 171, Training Loss: 1.2092, Validation Loss: 1.0393, Test Loss: 1.1564\n",
      "Epoch 181, Training Loss: 0.3299, Validation Loss: 0.2891, Test Loss: 0.3675\n",
      "Epoch 191, Training Loss: 0.3845, Validation Loss: 0.3543, Test Loss: 0.4238\n",
      "Epoch 201, Training Loss: 0.4685, Validation Loss: 0.4025, Test Loss: 0.4900\n",
      "Epoch 211, Training Loss: 0.3248, Validation Loss: 0.3037, Test Loss: 0.3965\n",
      "Epoch 221, Training Loss: 1.4271, Validation Loss: 1.2482, Test Loss: 1.6168\n",
      "Epoch 231, Training Loss: 0.8611, Validation Loss: 0.6964, Test Loss: 0.8346\n",
      "Epoch 241, Training Loss: 0.2804, Validation Loss: 0.2363, Test Loss: 0.3491\n",
      "Epoch 251, Training Loss: 0.5438, Validation Loss: 0.4094, Test Loss: 0.5458\n",
      "Epoch 261, Training Loss: 0.5624, Validation Loss: 0.4926, Test Loss: 0.6929\n",
      "Epoch 271, Training Loss: 0.3541, Validation Loss: 0.2613, Test Loss: 0.3902\n",
      "Epoch 281, Training Loss: 0.9620, Validation Loss: 0.8653, Test Loss: 1.1023\n",
      "Epoch 291, Training Loss: 0.2379, Validation Loss: 0.2038, Test Loss: 0.3232\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2379\n",
      "Validation loss: 0.2038\n",
      "Test loss: 0.3232\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 31/243 with parameters:\n",
      "Config indices: (0, 1, 0, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 1088.9409, Validation Loss: 1063.6729, Test Loss: 1062.0364\n",
      "Epoch 11, Training Loss: 0.5876, Validation Loss: 0.3609, Test Loss: 0.8195\n",
      "Epoch 21, Training Loss: 0.5504, Validation Loss: 0.3691, Test Loss: 0.6634\n",
      "Epoch 31, Training Loss: 0.4558, Validation Loss: 0.2863, Test Loss: 0.6063\n",
      "Epoch 41, Training Loss: 0.4288, Validation Loss: 0.2928, Test Loss: 0.5904\n",
      "Epoch 51, Training Loss: 0.7286, Validation Loss: 0.5638, Test Loss: 0.7656\n",
      "Epoch 61, Training Loss: 0.3978, Validation Loss: 0.2770, Test Loss: 0.5431\n",
      "Epoch 71, Training Loss: 0.4700, Validation Loss: 0.3410, Test Loss: 0.5145\n",
      "Epoch 81, Training Loss: 0.5173, Validation Loss: 0.4387, Test Loss: 0.6762\n",
      "Epoch 91, Training Loss: 0.4859, Validation Loss: 0.4067, Test Loss: 0.6224\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.4859\n",
      "Validation loss: 0.4067\n",
      "Test loss: 0.6224\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 32/243 with parameters:\n",
      "Config indices: (0, 1, 0, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 1177.0256, Validation Loss: 1177.5314, Test Loss: 1116.2755\n",
      "Epoch 11, Training Loss: 0.8713, Validation Loss: 0.6088, Test Loss: 1.0250\n",
      "Epoch 21, Training Loss: 0.5810, Validation Loss: 0.3946, Test Loss: 0.7415\n",
      "Epoch 31, Training Loss: 0.4975, Validation Loss: 0.3229, Test Loss: 0.5847\n",
      "Epoch 41, Training Loss: 1.0492, Validation Loss: 0.8423, Test Loss: 1.0358\n",
      "Epoch 51, Training Loss: 0.4737, Validation Loss: 0.3341, Test Loss: 0.6167\n",
      "Epoch 61, Training Loss: 0.5944, Validation Loss: 0.4686, Test Loss: 0.7817\n",
      "Epoch 71, Training Loss: 0.3055, Validation Loss: 0.2300, Test Loss: 0.4165\n",
      "Epoch 81, Training Loss: 0.3710, Validation Loss: 0.3022, Test Loss: 0.4601\n",
      "Epoch 91, Training Loss: 1.5558, Validation Loss: 1.3362, Test Loss: 1.7789\n",
      "Epoch 101, Training Loss: 0.3403, Validation Loss: 0.2781, Test Loss: 0.4697\n",
      "Epoch 111, Training Loss: 0.2329, Validation Loss: 0.1672, Test Loss: 0.2955\n",
      "Epoch 121, Training Loss: 0.2788, Validation Loss: 0.2213, Test Loss: 0.3704\n",
      "Epoch 131, Training Loss: 0.7218, Validation Loss: 0.5943, Test Loss: 0.7110\n",
      "Epoch 141, Training Loss: 0.2608, Validation Loss: 0.2089, Test Loss: 0.3520\n",
      "Epoch 151, Training Loss: 0.3347, Validation Loss: 0.2802, Test Loss: 0.3867\n",
      "Epoch 161, Training Loss: 0.4759, Validation Loss: 0.3866, Test Loss: 0.4858\n",
      "Epoch 171, Training Loss: 0.2389, Validation Loss: 0.1902, Test Loss: 0.3230\n",
      "Epoch 181, Training Loss: 0.2853, Validation Loss: 0.2428, Test Loss: 0.3812\n",
      "Epoch 191, Training Loss: 0.2261, Validation Loss: 0.1729, Test Loss: 0.2882\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.2261\n",
      "Validation loss: 0.1729\n",
      "Test loss: 0.2882\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 33/243 with parameters:\n",
      "Config indices: (0, 1, 0, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 1154.9667, Validation Loss: 1161.6780, Test Loss: 1111.2374\n",
      "Epoch 11, Training Loss: 0.7012, Validation Loss: 0.4257, Test Loss: 0.8881\n",
      "Epoch 21, Training Loss: 0.5228, Validation Loss: 0.3499, Test Loss: 0.6645\n",
      "Epoch 31, Training Loss: 0.4876, Validation Loss: 0.3349, Test Loss: 0.5757\n",
      "Epoch 41, Training Loss: 0.5944, Validation Loss: 0.4462, Test Loss: 0.7617\n",
      "Epoch 51, Training Loss: 0.3840, Validation Loss: 0.2641, Test Loss: 0.4702\n",
      "Epoch 61, Training Loss: 0.3707, Validation Loss: 0.2588, Test Loss: 0.4935\n",
      "Epoch 71, Training Loss: 0.5810, Validation Loss: 0.4591, Test Loss: 0.7256\n",
      "Epoch 81, Training Loss: 0.3533, Validation Loss: 0.2463, Test Loss: 0.3989\n",
      "Epoch 91, Training Loss: 0.2444, Validation Loss: 0.1793, Test Loss: 0.3266\n",
      "Epoch 101, Training Loss: 0.5124, Validation Loss: 0.4339, Test Loss: 0.6591\n",
      "Epoch 111, Training Loss: 1.0613, Validation Loss: 0.9338, Test Loss: 1.2212\n",
      "Epoch 121, Training Loss: 0.5124, Validation Loss: 0.4932, Test Loss: 0.5474\n",
      "Epoch 131, Training Loss: 0.2568, Validation Loss: 0.1874, Test Loss: 0.3002\n",
      "Epoch 141, Training Loss: 0.5931, Validation Loss: 0.5214, Test Loss: 0.6186\n",
      "Epoch 151, Training Loss: 0.2136, Validation Loss: 0.1661, Test Loss: 0.2923\n",
      "Epoch 161, Training Loss: 0.3989, Validation Loss: 0.3326, Test Loss: 0.4520\n",
      "Epoch 171, Training Loss: 0.7552, Validation Loss: 0.6804, Test Loss: 0.8855\n",
      "Epoch 181, Training Loss: 0.2171, Validation Loss: 0.1673, Test Loss: 0.2919\n",
      "Epoch 191, Training Loss: 0.8682, Validation Loss: 0.7820, Test Loss: 1.0126\n",
      "Epoch 201, Training Loss: 1.4816, Validation Loss: 1.4493, Test Loss: 1.6440\n",
      "Epoch 211, Training Loss: 0.5410, Validation Loss: 0.4752, Test Loss: 0.6704\n",
      "Epoch 221, Training Loss: 0.7260, Validation Loss: 0.6630, Test Loss: 0.8379\n",
      "Epoch 231, Training Loss: 0.2727, Validation Loss: 0.2308, Test Loss: 0.3649\n",
      "Epoch 241, Training Loss: 0.2945, Validation Loss: 0.2553, Test Loss: 0.3510\n",
      "Epoch 251, Training Loss: 0.6015, Validation Loss: 0.5064, Test Loss: 0.5894\n",
      "Epoch 261, Training Loss: 0.2335, Validation Loss: 0.1720, Test Loss: 0.2867\n",
      "Epoch 271, Training Loss: 0.4474, Validation Loss: 0.4242, Test Loss: 0.5262\n",
      "Epoch 281, Training Loss: 0.7744, Validation Loss: 0.6516, Test Loss: 0.7731\n",
      "Epoch 291, Training Loss: 0.7431, Validation Loss: 0.6924, Test Loss: 0.8788\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.7431\n",
      "Validation loss: 0.6924\n",
      "Test loss: 0.8788\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 34/243 with parameters:\n",
      "Config indices: (0, 1, 0, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6179.0396, Validation Loss: 5589.3994, Test Loss: 6196.0264\n",
      "Epoch 11, Training Loss: 1.9858, Validation Loss: 1.4787, Test Loss: 2.2994\n",
      "Epoch 21, Training Loss: 0.7077, Validation Loss: 0.4503, Test Loss: 0.8680\n",
      "Epoch 31, Training Loss: 0.6065, Validation Loss: 0.3947, Test Loss: 0.7220\n",
      "Epoch 41, Training Loss: 0.5377, Validation Loss: 0.3360, Test Loss: 0.6872\n",
      "Epoch 51, Training Loss: 0.5426, Validation Loss: 0.3529, Test Loss: 0.6361\n",
      "Epoch 61, Training Loss: 0.4768, Validation Loss: 0.3102, Test Loss: 0.6064\n",
      "Epoch 71, Training Loss: 0.4500, Validation Loss: 0.2941, Test Loss: 0.5748\n",
      "Epoch 81, Training Loss: 0.4543, Validation Loss: 0.3003, Test Loss: 0.6040\n",
      "Epoch 91, Training Loss: 0.4322, Validation Loss: 0.3131, Test Loss: 0.5375\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.4322\n",
      "Validation loss: 0.3131\n",
      "Test loss: 0.5375\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 35/243 with parameters:\n",
      "Config indices: (0, 1, 0, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6377.6226, Validation Loss: 5795.9546, Test Loss: 6394.0063\n",
      "Epoch 11, Training Loss: 2.0718, Validation Loss: 1.5804, Test Loss: 2.1983\n",
      "Epoch 21, Training Loss: 0.6437, Validation Loss: 0.4059, Test Loss: 0.8535\n",
      "Epoch 31, Training Loss: 0.5276, Validation Loss: 0.3279, Test Loss: 0.7131\n",
      "Epoch 41, Training Loss: 0.5317, Validation Loss: 0.3387, Test Loss: 0.6575\n",
      "Epoch 51, Training Loss: 0.4824, Validation Loss: 0.3008, Test Loss: 0.6481\n",
      "Epoch 61, Training Loss: 0.4689, Validation Loss: 0.2981, Test Loss: 0.5935\n",
      "Epoch 71, Training Loss: 0.6893, Validation Loss: 0.4964, Test Loss: 0.8999\n",
      "Epoch 81, Training Loss: 0.4199, Validation Loss: 0.2809, Test Loss: 0.5742\n",
      "Epoch 91, Training Loss: 0.3856, Validation Loss: 0.2609, Test Loss: 0.5144\n",
      "Epoch 101, Training Loss: 0.3840, Validation Loss: 0.2578, Test Loss: 0.4858\n",
      "Epoch 111, Training Loss: 0.3468, Validation Loss: 0.2336, Test Loss: 0.4807\n",
      "Epoch 121, Training Loss: 0.3231, Validation Loss: 0.2198, Test Loss: 0.4417\n",
      "Epoch 131, Training Loss: 0.3041, Validation Loss: 0.2200, Test Loss: 0.4407\n",
      "Epoch 141, Training Loss: 0.3842, Validation Loss: 0.2697, Test Loss: 0.4467\n",
      "Epoch 151, Training Loss: 0.3116, Validation Loss: 0.2231, Test Loss: 0.4240\n",
      "Epoch 161, Training Loss: 0.8717, Validation Loss: 0.7162, Test Loss: 0.8776\n",
      "Epoch 171, Training Loss: 0.2457, Validation Loss: 0.1795, Test Loss: 0.3340\n",
      "Epoch 181, Training Loss: 0.2475, Validation Loss: 0.1733, Test Loss: 0.3243\n",
      "Epoch 191, Training Loss: 0.2829, Validation Loss: 0.2104, Test Loss: 0.3475\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.2829\n",
      "Validation loss: 0.2104\n",
      "Test loss: 0.3475\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 36/243 with parameters:\n",
      "Config indices: (0, 1, 0, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 5712.3008, Validation Loss: 5202.6533, Test Loss: 5739.0215\n",
      "Epoch 11, Training Loss: 1.2113, Validation Loss: 0.7878, Test Loss: 1.4772\n",
      "Epoch 21, Training Loss: 0.6168, Validation Loss: 0.3768, Test Loss: 0.8206\n",
      "Epoch 31, Training Loss: 0.5457, Validation Loss: 0.3443, Test Loss: 0.6798\n",
      "Epoch 41, Training Loss: 0.5225, Validation Loss: 0.3344, Test Loss: 0.6326\n",
      "Epoch 51, Training Loss: 0.5064, Validation Loss: 0.3196, Test Loss: 0.5941\n",
      "Epoch 61, Training Loss: 0.5448, Validation Loss: 0.3597, Test Loss: 0.6075\n",
      "Epoch 71, Training Loss: 0.4378, Validation Loss: 0.2904, Test Loss: 0.5535\n",
      "Epoch 81, Training Loss: 0.5959, Validation Loss: 0.4386, Test Loss: 0.6347\n",
      "Epoch 91, Training Loss: 0.4055, Validation Loss: 0.2967, Test Loss: 0.5024\n",
      "Epoch 101, Training Loss: 0.3785, Validation Loss: 0.2721, Test Loss: 0.5064\n",
      "Epoch 111, Training Loss: 0.3513, Validation Loss: 0.2666, Test Loss: 0.4653\n",
      "Epoch 121, Training Loss: 0.5724, Validation Loss: 0.4454, Test Loss: 0.7283\n",
      "Epoch 131, Training Loss: 0.2798, Validation Loss: 0.1923, Test Loss: 0.3751\n",
      "Epoch 141, Training Loss: 0.2965, Validation Loss: 0.2168, Test Loss: 0.4062\n",
      "Epoch 151, Training Loss: 0.4638, Validation Loss: 0.4095, Test Loss: 0.5071\n",
      "Epoch 161, Training Loss: 0.3323, Validation Loss: 0.2624, Test Loss: 0.4109\n",
      "Epoch 171, Training Loss: 0.5291, Validation Loss: 0.4497, Test Loss: 0.6761\n",
      "Epoch 181, Training Loss: 0.2900, Validation Loss: 0.2208, Test Loss: 0.3556\n",
      "Epoch 191, Training Loss: 0.2728, Validation Loss: 0.2127, Test Loss: 0.3681\n",
      "Epoch 201, Training Loss: 0.2716, Validation Loss: 0.2104, Test Loss: 0.3596\n",
      "Epoch 211, Training Loss: 0.2713, Validation Loss: 0.2173, Test Loss: 0.3739\n",
      "Epoch 221, Training Loss: 0.2388, Validation Loss: 0.1890, Test Loss: 0.3166\n",
      "Epoch 231, Training Loss: 0.3523, Validation Loss: 0.3262, Test Loss: 0.4461\n",
      "Epoch 241, Training Loss: 0.6376, Validation Loss: 0.4982, Test Loss: 0.6423\n",
      "Epoch 251, Training Loss: 0.3149, Validation Loss: 0.2528, Test Loss: 0.3669\n",
      "Epoch 261, Training Loss: 0.2350, Validation Loss: 0.1766, Test Loss: 0.2941\n",
      "Epoch 271, Training Loss: 0.3384, Validation Loss: 0.2711, Test Loss: 0.3825\n",
      "Epoch 281, Training Loss: 0.7343, Validation Loss: 0.5979, Test Loss: 0.7366\n",
      "Epoch 291, Training Loss: 0.4570, Validation Loss: 0.3955, Test Loss: 0.5795\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.4570\n",
      "Validation loss: 0.3955\n",
      "Test loss: 0.5795\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 37/243 with parameters:\n",
      "Config indices: (0, 1, 1, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7052.8320, Validation Loss: 6447.6743, Test Loss: 7060.7529\n",
      "Epoch 11, Training Loss: 32.6458, Validation Loss: 30.2443, Test Loss: 31.5749\n",
      "Epoch 21, Training Loss: 1.7134, Validation Loss: 1.1174, Test Loss: 2.1017\n",
      "Epoch 31, Training Loss: 0.6590, Validation Loss: 0.3620, Test Loss: 0.9072\n",
      "Epoch 41, Training Loss: 0.5336, Validation Loss: 0.3184, Test Loss: 0.7102\n",
      "Epoch 51, Training Loss: 0.5147, Validation Loss: 0.3370, Test Loss: 0.6752\n",
      "Epoch 61, Training Loss: 0.4707, Validation Loss: 0.3033, Test Loss: 0.6011\n",
      "Epoch 71, Training Loss: 0.4359, Validation Loss: 0.2901, Test Loss: 0.5502\n",
      "Epoch 81, Training Loss: 0.4184, Validation Loss: 0.2738, Test Loss: 0.5072\n",
      "Epoch 91, Training Loss: 0.5354, Validation Loss: 0.3925, Test Loss: 0.5888\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.5354\n",
      "Validation loss: 0.3925\n",
      "Test loss: 0.5888\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 38/243 with parameters:\n",
      "Config indices: (0, 1, 1, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6989.5981, Validation Loss: 6388.8750, Test Loss: 6998.2222\n",
      "Epoch 11, Training Loss: 16.2021, Validation Loss: 15.0387, Test Loss: 15.4402\n",
      "Epoch 21, Training Loss: 1.2930, Validation Loss: 0.7427, Test Loss: 1.7189\n",
      "Epoch 31, Training Loss: 0.6843, Validation Loss: 0.3851, Test Loss: 0.9909\n",
      "Epoch 41, Training Loss: 0.5663, Validation Loss: 0.3412, Test Loss: 0.7938\n",
      "Epoch 51, Training Loss: 0.5636, Validation Loss: 0.3433, Test Loss: 0.7605\n",
      "Epoch 61, Training Loss: 0.5123, Validation Loss: 0.3316, Test Loss: 0.6555\n",
      "Epoch 71, Training Loss: 0.5110, Validation Loss: 0.3435, Test Loss: 0.6371\n",
      "Epoch 81, Training Loss: 0.4076, Validation Loss: 0.2680, Test Loss: 0.5543\n",
      "Epoch 91, Training Loss: 0.3992, Validation Loss: 0.2726, Test Loss: 0.5186\n",
      "Epoch 101, Training Loss: 0.5089, Validation Loss: 0.3785, Test Loss: 0.6890\n",
      "Epoch 111, Training Loss: 0.3551, Validation Loss: 0.2379, Test Loss: 0.4828\n",
      "Epoch 121, Training Loss: 0.3165, Validation Loss: 0.2218, Test Loss: 0.4519\n",
      "Epoch 131, Training Loss: 0.2857, Validation Loss: 0.2006, Test Loss: 0.4128\n",
      "Epoch 141, Training Loss: 0.2527, Validation Loss: 0.1836, Test Loss: 0.3632\n",
      "Epoch 151, Training Loss: 0.2719, Validation Loss: 0.2202, Test Loss: 0.3883\n",
      "Epoch 161, Training Loss: 0.3010, Validation Loss: 0.2387, Test Loss: 0.4405\n",
      "Epoch 171, Training Loss: 0.3199, Validation Loss: 0.2470, Test Loss: 0.3892\n",
      "Epoch 181, Training Loss: 0.2592, Validation Loss: 0.1914, Test Loss: 0.3288\n",
      "Epoch 191, Training Loss: 0.3021, Validation Loss: 0.2389, Test Loss: 0.4165\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.3021\n",
      "Validation loss: 0.2389\n",
      "Test loss: 0.4165\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 39/243 with parameters:\n",
      "Config indices: (0, 1, 1, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7028.3296, Validation Loss: 6422.1948, Test Loss: 7036.4668\n",
      "Epoch 11, Training Loss: 34.3363, Validation Loss: 32.5313, Test Loss: 33.0987\n",
      "Epoch 21, Training Loss: 1.6683, Validation Loss: 1.0958, Test Loss: 2.1136\n",
      "Epoch 31, Training Loss: 0.7241, Validation Loss: 0.4366, Test Loss: 1.0093\n",
      "Epoch 41, Training Loss: 0.5520, Validation Loss: 0.3508, Test Loss: 0.7514\n",
      "Epoch 51, Training Loss: 0.4944, Validation Loss: 0.3118, Test Loss: 0.6438\n",
      "Epoch 61, Training Loss: 0.4583, Validation Loss: 0.2958, Test Loss: 0.5907\n",
      "Epoch 71, Training Loss: 0.4707, Validation Loss: 0.2956, Test Loss: 0.5602\n",
      "Epoch 81, Training Loss: 0.3978, Validation Loss: 0.2768, Test Loss: 0.5182\n",
      "Epoch 91, Training Loss: 0.3515, Validation Loss: 0.2320, Test Loss: 0.4719\n",
      "Epoch 101, Training Loss: 0.3434, Validation Loss: 0.2307, Test Loss: 0.4372\n",
      "Epoch 111, Training Loss: 0.3992, Validation Loss: 0.3109, Test Loss: 0.4956\n",
      "Epoch 121, Training Loss: 0.3056, Validation Loss: 0.2205, Test Loss: 0.3904\n",
      "Epoch 131, Training Loss: 0.3840, Validation Loss: 0.2945, Test Loss: 0.5083\n",
      "Epoch 141, Training Loss: 0.3137, Validation Loss: 0.2260, Test Loss: 0.3836\n",
      "Epoch 151, Training Loss: 0.2594, Validation Loss: 0.1916, Test Loss: 0.3738\n",
      "Epoch 161, Training Loss: 0.3599, Validation Loss: 0.3098, Test Loss: 0.4780\n",
      "Epoch 171, Training Loss: 0.3056, Validation Loss: 0.2423, Test Loss: 0.3695\n",
      "Epoch 181, Training Loss: 0.2326, Validation Loss: 0.1656, Test Loss: 0.3111\n",
      "Epoch 191, Training Loss: 0.2447, Validation Loss: 0.1931, Test Loss: 0.3514\n",
      "Epoch 201, Training Loss: 0.2343, Validation Loss: 0.1748, Test Loss: 0.3045\n",
      "Epoch 211, Training Loss: 0.2493, Validation Loss: 0.1942, Test Loss: 0.3413\n",
      "Epoch 221, Training Loss: 0.2281, Validation Loss: 0.1757, Test Loss: 0.3217\n",
      "Epoch 231, Training Loss: 0.3509, Validation Loss: 0.2940, Test Loss: 0.4787\n",
      "Epoch 241, Training Loss: 0.2626, Validation Loss: 0.1898, Test Loss: 0.3162\n",
      "Epoch 251, Training Loss: 0.3468, Validation Loss: 0.2929, Test Loss: 0.4683\n",
      "Epoch 261, Training Loss: 0.2483, Validation Loss: 0.2024, Test Loss: 0.3510\n",
      "Epoch 271, Training Loss: 0.2641, Validation Loss: 0.1987, Test Loss: 0.3183\n",
      "Epoch 281, Training Loss: 0.2241, Validation Loss: 0.1782, Test Loss: 0.2978\n",
      "Epoch 291, Training Loss: 0.2481, Validation Loss: 0.1960, Test Loss: 0.3099\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2481\n",
      "Validation loss: 0.1960\n",
      "Test loss: 0.3099\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 40/243 with parameters:\n",
      "Config indices: (0, 1, 1, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7148.9893, Validation Loss: 6538.8628, Test Loss: 7155.8682\n",
      "Epoch 11, Training Loss: 277.6716, Validation Loss: 282.1231, Test Loss: 282.1819\n",
      "Epoch 21, Training Loss: 8.4116, Validation Loss: 8.2290, Test Loss: 7.4638\n",
      "Epoch 31, Training Loss: 2.2600, Validation Loss: 2.3050, Test Loss: 2.3972\n",
      "Epoch 41, Training Loss: 1.3387, Validation Loss: 1.3314, Test Loss: 1.4679\n",
      "Epoch 51, Training Loss: 0.9214, Validation Loss: 0.8819, Test Loss: 1.0308\n",
      "Epoch 61, Training Loss: 0.7347, Validation Loss: 0.6578, Test Loss: 0.8380\n",
      "Epoch 71, Training Loss: 0.6601, Validation Loss: 0.5591, Test Loss: 0.7595\n",
      "Epoch 81, Training Loss: 0.5812, Validation Loss: 0.4349, Test Loss: 0.6920\n",
      "Epoch 91, Training Loss: 0.5506, Validation Loss: 0.3970, Test Loss: 0.6668\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.5506\n",
      "Validation loss: 0.3970\n",
      "Test loss: 0.6668\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 41/243 with parameters:\n",
      "Config indices: (0, 1, 1, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7155.9146, Validation Loss: 6545.5825, Test Loss: 7162.7656\n",
      "Epoch 11, Training Loss: 482.8790, Validation Loss: 474.4600, Test Loss: 501.1681\n",
      "Epoch 21, Training Loss: 39.2831, Validation Loss: 38.1547, Test Loss: 37.0665\n",
      "Epoch 31, Training Loss: 4.0865, Validation Loss: 3.2663, Test Loss: 4.2999\n",
      "Epoch 41, Training Loss: 1.3820, Validation Loss: 0.9223, Test Loss: 1.6846\n",
      "Epoch 51, Training Loss: 0.7781, Validation Loss: 0.4870, Test Loss: 1.0403\n",
      "Epoch 61, Training Loss: 0.6027, Validation Loss: 0.3640, Test Loss: 0.8224\n",
      "Epoch 71, Training Loss: 0.5495, Validation Loss: 0.3369, Test Loss: 0.7307\n",
      "Epoch 81, Training Loss: 0.5187, Validation Loss: 0.3219, Test Loss: 0.6981\n",
      "Epoch 91, Training Loss: 0.5001, Validation Loss: 0.3130, Test Loss: 0.6539\n",
      "Epoch 101, Training Loss: 0.4847, Validation Loss: 0.3094, Test Loss: 0.6333\n",
      "Epoch 111, Training Loss: 0.5489, Validation Loss: 0.3616, Test Loss: 0.7291\n",
      "Epoch 121, Training Loss: 0.4550, Validation Loss: 0.2935, Test Loss: 0.5923\n",
      "Epoch 131, Training Loss: 0.4550, Validation Loss: 0.3005, Test Loss: 0.6085\n",
      "Epoch 141, Training Loss: 0.4486, Validation Loss: 0.2969, Test Loss: 0.6027\n",
      "Epoch 151, Training Loss: 0.5324, Validation Loss: 0.3651, Test Loss: 0.7027\n",
      "Epoch 161, Training Loss: 0.4069, Validation Loss: 0.2724, Test Loss: 0.5536\n",
      "Epoch 171, Training Loss: 0.3755, Validation Loss: 0.2404, Test Loss: 0.4782\n",
      "Epoch 181, Training Loss: 0.3498, Validation Loss: 0.2301, Test Loss: 0.4595\n",
      "Epoch 191, Training Loss: 0.3596, Validation Loss: 0.2446, Test Loss: 0.4938\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.3596\n",
      "Validation loss: 0.2446\n",
      "Test loss: 0.4938\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 42/243 with parameters:\n",
      "Config indices: (0, 1, 1, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7156.4302, Validation Loss: 6545.8354, Test Loss: 7163.2310\n",
      "Epoch 11, Training Loss: 383.9655, Validation Loss: 372.4290, Test Loss: 415.6407\n",
      "Epoch 21, Training Loss: 88.5479, Validation Loss: 87.7537, Test Loss: 88.3361\n",
      "Epoch 31, Training Loss: 13.6666, Validation Loss: 12.3734, Test Loss: 13.8095\n",
      "Epoch 41, Training Loss: 1.6433, Validation Loss: 1.1342, Test Loss: 2.0828\n",
      "Epoch 51, Training Loss: 0.8875, Validation Loss: 0.5034, Test Loss: 1.2542\n",
      "Epoch 61, Training Loss: 0.7086, Validation Loss: 0.4292, Test Loss: 1.0238\n",
      "Epoch 71, Training Loss: 0.6263, Validation Loss: 0.3998, Test Loss: 0.8674\n",
      "Epoch 81, Training Loss: 0.6018, Validation Loss: 0.3994, Test Loss: 0.8267\n",
      "Epoch 91, Training Loss: 0.5451, Validation Loss: 0.3411, Test Loss: 0.7172\n",
      "Epoch 101, Training Loss: 0.5142, Validation Loss: 0.3229, Test Loss: 0.6805\n",
      "Epoch 111, Training Loss: 0.4922, Validation Loss: 0.3086, Test Loss: 0.6581\n",
      "Epoch 121, Training Loss: 0.5031, Validation Loss: 0.3363, Test Loss: 0.6356\n",
      "Epoch 131, Training Loss: 0.4886, Validation Loss: 0.3157, Test Loss: 0.6586\n",
      "Epoch 141, Training Loss: 0.4483, Validation Loss: 0.2875, Test Loss: 0.6078\n",
      "Epoch 151, Training Loss: 0.4293, Validation Loss: 0.2775, Test Loss: 0.5892\n",
      "Epoch 161, Training Loss: 0.4037, Validation Loss: 0.2630, Test Loss: 0.5529\n",
      "Epoch 171, Training Loss: 0.4057, Validation Loss: 0.2756, Test Loss: 0.5241\n",
      "Epoch 181, Training Loss: 0.4126, Validation Loss: 0.2848, Test Loss: 0.5141\n",
      "Epoch 191, Training Loss: 0.3512, Validation Loss: 0.2357, Test Loss: 0.4883\n",
      "Epoch 201, Training Loss: 0.3522, Validation Loss: 0.2344, Test Loss: 0.4884\n",
      "Epoch 211, Training Loss: 0.3415, Validation Loss: 0.2469, Test Loss: 0.4521\n",
      "Epoch 221, Training Loss: 0.2983, Validation Loss: 0.2066, Test Loss: 0.4248\n",
      "Epoch 231, Training Loss: 0.3058, Validation Loss: 0.2124, Test Loss: 0.4411\n",
      "Epoch 241, Training Loss: 0.2816, Validation Loss: 0.1913, Test Loss: 0.3781\n",
      "Epoch 251, Training Loss: 0.2687, Validation Loss: 0.1818, Test Loss: 0.3701\n",
      "Epoch 261, Training Loss: 0.2579, Validation Loss: 0.1861, Test Loss: 0.3588\n",
      "Epoch 271, Training Loss: 0.3263, Validation Loss: 0.2495, Test Loss: 0.4603\n",
      "Epoch 281, Training Loss: 0.2719, Validation Loss: 0.1987, Test Loss: 0.3523\n",
      "Epoch 291, Training Loss: 0.2348, Validation Loss: 0.1679, Test Loss: 0.3376\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.2348\n",
      "Validation loss: 0.1679\n",
      "Test loss: 0.3376\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 43/243 with parameters:\n",
      "Config indices: (0, 1, 1, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7158.4473, Validation Loss: 6548.0190, Test Loss: 7165.2593\n",
      "Epoch 11, Training Loss: 919.2603, Validation Loss: 851.0730, Test Loss: 963.9108\n",
      "Epoch 21, Training Loss: 304.1261, Validation Loss: 305.4017, Test Loss: 311.8074\n",
      "Epoch 31, Training Loss: 74.0836, Validation Loss: 73.7433, Test Loss: 70.1953\n",
      "Epoch 41, Training Loss: 15.3020, Validation Loss: 14.0844, Test Loss: 15.0333\n",
      "Epoch 51, Training Loss: 5.5236, Validation Loss: 4.6208, Test Loss: 5.9089\n",
      "Epoch 61, Training Loss: 2.5985, Validation Loss: 1.9507, Test Loss: 2.9854\n",
      "Epoch 71, Training Loss: 1.5754, Validation Loss: 1.1255, Test Loss: 1.9437\n",
      "Epoch 81, Training Loss: 1.0642, Validation Loss: 0.7202, Test Loss: 1.3782\n",
      "Epoch 91, Training Loss: 0.6486, Validation Loss: 0.4009, Test Loss: 0.8909\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.6486\n",
      "Validation loss: 0.4009\n",
      "Test loss: 0.8909\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 44/243 with parameters:\n",
      "Config indices: (0, 1, 1, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7163.0615, Validation Loss: 6552.2993, Test Loss: 7169.8706\n",
      "Epoch 11, Training Loss: 3387.6218, Validation Loss: 2950.7302, Test Loss: 3409.3748\n",
      "Epoch 21, Training Loss: 382.4013, Validation Loss: 377.3445, Test Loss: 413.3397\n",
      "Epoch 31, Training Loss: 123.5655, Validation Loss: 125.0450, Test Loss: 125.7698\n",
      "Epoch 41, Training Loss: 33.7103, Validation Loss: 34.6894, Test Loss: 30.1952\n",
      "Epoch 51, Training Loss: 11.2417, Validation Loss: 12.0475, Test Loss: 9.1959\n",
      "Epoch 61, Training Loss: 6.3044, Validation Loss: 6.9837, Test Loss: 5.4182\n",
      "Epoch 71, Training Loss: 4.3516, Validation Loss: 4.9695, Test Loss: 3.8328\n",
      "Epoch 81, Training Loss: 3.1653, Validation Loss: 3.6195, Test Loss: 2.7818\n",
      "Epoch 91, Training Loss: 2.3483, Validation Loss: 2.6792, Test Loss: 2.0704\n",
      "Epoch 101, Training Loss: 1.7381, Validation Loss: 1.9631, Test Loss: 1.5458\n",
      "Epoch 111, Training Loss: 1.2919, Validation Loss: 1.4299, Test Loss: 1.1690\n",
      "Epoch 121, Training Loss: 0.9835, Validation Loss: 1.0450, Test Loss: 0.9030\n",
      "Epoch 131, Training Loss: 0.7932, Validation Loss: 0.8229, Test Loss: 0.7560\n",
      "Epoch 141, Training Loss: 0.6847, Validation Loss: 0.6882, Test Loss: 0.6966\n",
      "Epoch 151, Training Loss: 0.6233, Validation Loss: 0.6128, Test Loss: 0.6567\n",
      "Epoch 161, Training Loss: 0.5800, Validation Loss: 0.5567, Test Loss: 0.6413\n",
      "Epoch 171, Training Loss: 0.5489, Validation Loss: 0.5235, Test Loss: 0.6148\n",
      "Epoch 181, Training Loss: 0.5228, Validation Loss: 0.4937, Test Loss: 0.5975\n",
      "Epoch 191, Training Loss: 0.4982, Validation Loss: 0.4566, Test Loss: 0.5965\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.4982\n",
      "Validation loss: 0.4566\n",
      "Test loss: 0.5965\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 45/243 with parameters:\n",
      "Config indices: (0, 1, 1, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7160.5586, Validation Loss: 6550.0249, Test Loss: 7167.3867\n",
      "Epoch 11, Training Loss: 893.2410, Validation Loss: 835.0581, Test Loss: 948.6405\n",
      "Epoch 21, Training Loss: 339.1697, Validation Loss: 339.4629, Test Loss: 348.6264\n",
      "Epoch 31, Training Loss: 72.8178, Validation Loss: 71.3875, Test Loss: 68.6836\n",
      "Epoch 41, Training Loss: 11.1775, Validation Loss: 9.7660, Test Loss: 11.1476\n",
      "Epoch 51, Training Loss: 2.6530, Validation Loss: 1.9043, Test Loss: 3.0862\n",
      "Epoch 61, Training Loss: 1.3211, Validation Loss: 0.8321, Test Loss: 1.7294\n",
      "Epoch 71, Training Loss: 0.9461, Validation Loss: 0.5892, Test Loss: 1.2868\n",
      "Epoch 81, Training Loss: 0.7768, Validation Loss: 0.4855, Test Loss: 1.0813\n",
      "Epoch 91, Training Loss: 0.6813, Validation Loss: 0.4176, Test Loss: 0.9392\n",
      "Epoch 101, Training Loss: 0.6290, Validation Loss: 0.3812, Test Loss: 0.8599\n",
      "Epoch 111, Training Loss: 0.5893, Validation Loss: 0.3648, Test Loss: 0.8043\n",
      "Epoch 121, Training Loss: 0.5808, Validation Loss: 0.3708, Test Loss: 0.8046\n",
      "Epoch 131, Training Loss: 0.5478, Validation Loss: 0.3459, Test Loss: 0.7367\n",
      "Epoch 141, Training Loss: 0.5311, Validation Loss: 0.3332, Test Loss: 0.7178\n",
      "Epoch 151, Training Loss: 0.5264, Validation Loss: 0.3292, Test Loss: 0.7209\n",
      "Epoch 161, Training Loss: 0.5158, Validation Loss: 0.3354, Test Loss: 0.6885\n",
      "Epoch 171, Training Loss: 0.4940, Validation Loss: 0.3114, Test Loss: 0.6571\n",
      "Epoch 181, Training Loss: 0.4805, Validation Loss: 0.3001, Test Loss: 0.6442\n",
      "Epoch 191, Training Loss: 0.4721, Validation Loss: 0.2953, Test Loss: 0.6440\n",
      "Epoch 201, Training Loss: 0.4747, Validation Loss: 0.2976, Test Loss: 0.6522\n",
      "Epoch 211, Training Loss: 0.4808, Validation Loss: 0.3032, Test Loss: 0.6688\n",
      "Epoch 221, Training Loss: 0.4518, Validation Loss: 0.2942, Test Loss: 0.6171\n",
      "Epoch 231, Training Loss: 0.4219, Validation Loss: 0.2673, Test Loss: 0.5771\n",
      "Epoch 241, Training Loss: 0.4150, Validation Loss: 0.2700, Test Loss: 0.5702\n",
      "Epoch 251, Training Loss: 0.4422, Validation Loss: 0.2900, Test Loss: 0.6240\n",
      "Epoch 261, Training Loss: 0.3990, Validation Loss: 0.2584, Test Loss: 0.5625\n",
      "Epoch 271, Training Loss: 0.3868, Validation Loss: 0.2558, Test Loss: 0.5224\n",
      "Epoch 281, Training Loss: 0.3661, Validation Loss: 0.2391, Test Loss: 0.5111\n",
      "Epoch 291, Training Loss: 0.3657, Validation Loss: 0.2380, Test Loss: 0.5185\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.3657\n",
      "Validation loss: 0.2380\n",
      "Test loss: 0.5185\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 46/243 with parameters:\n",
      "Config indices: (0, 1, 2, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7165.6606, Validation Loss: 6554.7812, Test Loss: 7172.4766\n",
      "Epoch 11, Training Loss: 6801.4409, Validation Loss: 6204.6802, Test Loss: 6812.4453\n",
      "Epoch 21, Training Loss: 3849.4558, Validation Loss: 3411.5789, Test Loss: 3885.6118\n",
      "Epoch 31, Training Loss: 1103.4661, Validation Loss: 981.7257, Test Loss: 1133.7490\n",
      "Epoch 41, Training Loss: 641.3298, Validation Loss: 610.5472, Test Loss: 674.9995\n",
      "Epoch 51, Training Loss: 468.1613, Validation Loss: 455.4529, Test Loss: 491.6009\n",
      "Epoch 61, Training Loss: 300.3129, Validation Loss: 296.5424, Test Loss: 310.3206\n",
      "Epoch 71, Training Loss: 181.9697, Validation Loss: 180.9910, Test Loss: 181.4997\n",
      "Epoch 81, Training Loss: 101.9153, Validation Loss: 101.3691, Test Loss: 98.0536\n",
      "Epoch 91, Training Loss: 49.0496, Validation Loss: 47.5350, Test Loss: 45.9378\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 49.0496\n",
      "Validation loss: 47.5350\n",
      "Test loss: 45.9378\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 47/243 with parameters:\n",
      "Config indices: (0, 1, 2, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7165.4951, Validation Loss: 6554.6348, Test Loss: 7172.3101\n",
      "Epoch 11, Training Loss: 6899.7686, Validation Loss: 6304.2563, Test Loss: 6909.6860\n",
      "Epoch 21, Training Loss: 4568.6626, Validation Loss: 4113.5669, Test Loss: 4606.0381\n",
      "Epoch 31, Training Loss: 1482.2191, Validation Loss: 1313.6154, Test Loss: 1523.8475\n",
      "Epoch 41, Training Loss: 729.1512, Validation Loss: 693.1404, Test Loss: 764.6594\n",
      "Epoch 51, Training Loss: 578.7437, Validation Loss: 564.1290, Test Loss: 604.8649\n",
      "Epoch 61, Training Loss: 420.5552, Validation Loss: 418.2253, Test Loss: 433.1114\n",
      "Epoch 71, Training Loss: 272.6797, Validation Loss: 276.5819, Test Loss: 271.8172\n",
      "Epoch 81, Training Loss: 170.2455, Validation Loss: 173.9650, Test Loss: 162.3879\n",
      "Epoch 91, Training Loss: 95.0253, Validation Loss: 95.8862, Test Loss: 87.8164\n",
      "Epoch 101, Training Loss: 42.8080, Validation Loss: 40.9855, Test Loss: 38.6737\n",
      "Epoch 111, Training Loss: 16.3174, Validation Loss: 14.3368, Test Loss: 14.9731\n",
      "Epoch 121, Training Loss: 6.6673, Validation Loss: 5.3783, Test Loss: 6.5465\n",
      "Epoch 131, Training Loss: 2.9884, Validation Loss: 2.1033, Test Loss: 3.2478\n",
      "Epoch 141, Training Loss: 1.6131, Validation Loss: 1.0103, Test Loss: 1.9462\n",
      "Epoch 151, Training Loss: 1.0792, Validation Loss: 0.6632, Test Loss: 1.3861\n",
      "Epoch 161, Training Loss: 0.8204, Validation Loss: 0.4953, Test Loss: 1.0961\n",
      "Epoch 171, Training Loss: 0.7118, Validation Loss: 0.4408, Test Loss: 0.9322\n",
      "Epoch 181, Training Loss: 0.6361, Validation Loss: 0.3921, Test Loss: 0.8366\n",
      "Epoch 191, Training Loss: 0.6017, Validation Loss: 0.3740, Test Loss: 0.7680\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.6017\n",
      "Validation loss: 0.3740\n",
      "Test loss: 0.7680\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 48/243 with parameters:\n",
      "Config indices: (0, 1, 2, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7162.9033, Validation Loss: 6552.1533, Test Loss: 7169.7437\n",
      "Epoch 11, Training Loss: 6346.5967, Validation Loss: 5781.8433, Test Loss: 6364.0801\n",
      "Epoch 21, Training Loss: 1597.2167, Validation Loss: 1420.7054, Test Loss: 1650.2035\n",
      "Epoch 31, Training Loss: 719.2691, Validation Loss: 690.5310, Test Loss: 759.4133\n",
      "Epoch 41, Training Loss: 520.8170, Validation Loss: 508.1910, Test Loss: 541.2157\n",
      "Epoch 51, Training Loss: 316.1773, Validation Loss: 314.9290, Test Loss: 317.7175\n",
      "Epoch 61, Training Loss: 158.1642, Validation Loss: 158.8993, Test Loss: 151.2159\n",
      "Epoch 71, Training Loss: 63.9093, Validation Loss: 61.8951, Test Loss: 59.5351\n",
      "Epoch 81, Training Loss: 25.7371, Validation Loss: 23.7288, Test Loss: 24.3063\n",
      "Epoch 91, Training Loss: 10.5653, Validation Loss: 8.8290, Test Loss: 10.3084\n",
      "Epoch 101, Training Loss: 4.4745, Validation Loss: 3.2453, Test Loss: 4.7016\n",
      "Epoch 111, Training Loss: 2.2804, Validation Loss: 1.4312, Test Loss: 2.6379\n",
      "Epoch 121, Training Loss: 1.3693, Validation Loss: 0.7526, Test Loss: 1.7569\n",
      "Epoch 131, Training Loss: 0.9537, Validation Loss: 0.4930, Test Loss: 1.3163\n",
      "Epoch 141, Training Loss: 0.7701, Validation Loss: 0.4134, Test Loss: 1.0801\n",
      "Epoch 151, Training Loss: 0.6607, Validation Loss: 0.3675, Test Loss: 0.9103\n",
      "Epoch 161, Training Loss: 0.6067, Validation Loss: 0.3548, Test Loss: 0.8185\n",
      "Epoch 171, Training Loss: 0.5966, Validation Loss: 0.3685, Test Loss: 0.7710\n",
      "Epoch 181, Training Loss: 0.5309, Validation Loss: 0.3265, Test Loss: 0.7163\n",
      "Epoch 191, Training Loss: 0.5326, Validation Loss: 0.3327, Test Loss: 0.7299\n",
      "Epoch 201, Training Loss: 0.5818, Validation Loss: 0.3771, Test Loss: 0.7899\n",
      "Epoch 211, Training Loss: 0.4913, Validation Loss: 0.3118, Test Loss: 0.6440\n",
      "Epoch 221, Training Loss: 0.4864, Validation Loss: 0.3187, Test Loss: 0.6367\n",
      "Epoch 231, Training Loss: 0.4811, Validation Loss: 0.3034, Test Loss: 0.6426\n",
      "Epoch 241, Training Loss: 0.4664, Validation Loss: 0.2964, Test Loss: 0.6001\n",
      "Epoch 251, Training Loss: 0.4580, Validation Loss: 0.2905, Test Loss: 0.6043\n",
      "Epoch 261, Training Loss: 0.4434, Validation Loss: 0.2846, Test Loss: 0.5785\n",
      "Epoch 271, Training Loss: 0.4349, Validation Loss: 0.2797, Test Loss: 0.5686\n",
      "Epoch 281, Training Loss: 0.4339, Validation Loss: 0.2755, Test Loss: 0.5757\n",
      "Epoch 291, Training Loss: 0.4239, Validation Loss: 0.2727, Test Loss: 0.5632\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.4239\n",
      "Validation loss: 0.2727\n",
      "Test loss: 0.5632\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 49/243 with parameters:\n",
      "Config indices: (0, 1, 2, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.5156, Validation Loss: 6555.5693, Test Loss: 7173.3164\n",
      "Epoch 11, Training Loss: 7146.5171, Validation Loss: 6536.3687, Test Loss: 7153.4453\n",
      "Epoch 21, Training Loss: 6952.9771, Validation Loss: 6352.0601, Test Loss: 6962.5410\n",
      "Epoch 31, Training Loss: 6190.7183, Validation Loss: 5628.2017, Test Loss: 6211.2734\n",
      "Epoch 41, Training Loss: 4626.5957, Validation Loss: 4156.4639, Test Loss: 4666.3462\n",
      "Epoch 51, Training Loss: 2744.0068, Validation Loss: 2419.4883, Test Loss: 2796.4790\n",
      "Epoch 61, Training Loss: 1432.5448, Validation Loss: 1259.9967, Test Loss: 1483.8103\n",
      "Epoch 71, Training Loss: 906.4470, Validation Loss: 825.8728, Test Loss: 957.2968\n",
      "Epoch 81, Training Loss: 757.9055, Validation Loss: 712.1162, Test Loss: 808.0634\n",
      "Epoch 91, Training Loss: 678.4910, Validation Loss: 645.9608, Test Loss: 723.9808\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 678.4910\n",
      "Validation loss: 645.9608\n",
      "Test loss: 723.9808\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 50/243 with parameters:\n",
      "Config indices: (0, 1, 2, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7163.1235, Validation Loss: 6552.3545, Test Loss: 7169.9619\n",
      "Epoch 11, Training Loss: 7045.3213, Validation Loss: 6441.8232, Test Loss: 7053.4512\n",
      "Epoch 21, Training Loss: 6035.5542, Validation Loss: 5496.7236, Test Loss: 6057.9775\n",
      "Epoch 31, Training Loss: 3457.3645, Validation Loss: 3103.3171, Test Loss: 3511.0627\n",
      "Epoch 41, Training Loss: 1303.9998, Validation Loss: 1163.2990, Test Loss: 1364.5527\n",
      "Epoch 51, Training Loss: 821.1144, Validation Loss: 768.2615, Test Loss: 874.9014\n",
      "Epoch 61, Training Loss: 708.2869, Validation Loss: 675.0594, Test Loss: 753.9996\n",
      "Epoch 71, Training Loss: 595.5717, Validation Loss: 573.0299, Test Loss: 630.4957\n",
      "Epoch 81, Training Loss: 468.8009, Validation Loss: 456.3883, Test Loss: 492.1264\n",
      "Epoch 91, Training Loss: 343.2785, Validation Loss: 338.6941, Test Loss: 354.5260\n",
      "Epoch 101, Training Loss: 245.5015, Validation Loss: 244.0413, Test Loss: 246.6119\n",
      "Epoch 111, Training Loss: 171.1092, Validation Loss: 170.5759, Test Loss: 166.8128\n",
      "Epoch 121, Training Loss: 111.2620, Validation Loss: 110.5027, Test Loss: 106.2726\n",
      "Epoch 131, Training Loss: 66.5990, Validation Loss: 64.8665, Test Loss: 62.7193\n",
      "Epoch 141, Training Loss: 37.4535, Validation Loss: 34.9705, Test Loss: 35.2087\n",
      "Epoch 151, Training Loss: 21.1369, Validation Loss: 18.8386, Test Loss: 20.1020\n",
      "Epoch 161, Training Loss: 12.4560, Validation Loss: 10.5076, Test Loss: 12.2356\n",
      "Epoch 171, Training Loss: 7.7817, Validation Loss: 6.1648, Test Loss: 7.9773\n",
      "Epoch 181, Training Loss: 5.1602, Validation Loss: 3.8476, Test Loss: 5.5189\n",
      "Epoch 191, Training Loss: 3.6299, Validation Loss: 2.6083, Test Loss: 4.0595\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 3.6299\n",
      "Validation loss: 2.6083\n",
      "Test loss: 4.0595\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 51/243 with parameters:\n",
      "Config indices: (0, 1, 2, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7165.8247, Validation Loss: 6554.9434, Test Loss: 7172.6489\n",
      "Epoch 11, Training Loss: 7099.5737, Validation Loss: 6491.4604, Test Loss: 7106.9722\n",
      "Epoch 21, Training Loss: 6456.1704, Validation Loss: 5876.6860, Test Loss: 6471.7842\n",
      "Epoch 31, Training Loss: 4502.5532, Validation Loss: 4034.8372, Test Loss: 4538.1348\n",
      "Epoch 41, Training Loss: 2146.0476, Validation Loss: 1884.8407, Test Loss: 2185.0859\n",
      "Epoch 51, Training Loss: 981.3559, Validation Loss: 890.3694, Test Loss: 1017.1689\n",
      "Epoch 61, Training Loss: 718.5565, Validation Loss: 683.2893, Test Loss: 755.6747\n",
      "Epoch 71, Training Loss: 611.8776, Validation Loss: 591.9962, Test Loss: 644.1429\n",
      "Epoch 81, Training Loss: 499.0049, Validation Loss: 489.5571, Test Loss: 522.8447\n",
      "Epoch 91, Training Loss: 381.4254, Validation Loss: 380.0763, Test Loss: 395.9863\n",
      "Epoch 101, Training Loss: 282.9924, Validation Loss: 284.7988, Test Loss: 288.3639\n",
      "Epoch 111, Training Loss: 212.9226, Validation Loss: 215.2799, Test Loss: 210.8328\n",
      "Epoch 121, Training Loss: 155.8847, Validation Loss: 157.8206, Test Loss: 150.2659\n",
      "Epoch 131, Training Loss: 108.4072, Validation Loss: 109.1062, Test Loss: 102.3919\n",
      "Epoch 141, Training Loss: 70.6792, Validation Loss: 69.6682, Test Loss: 66.0212\n",
      "Epoch 151, Training Loss: 42.5772, Validation Loss: 40.8496, Test Loss: 39.3829\n",
      "Epoch 161, Training Loss: 24.6089, Validation Loss: 22.6853, Test Loss: 23.0253\n",
      "Epoch 171, Training Loss: 14.5269, Validation Loss: 12.8187, Test Loss: 13.9752\n",
      "Epoch 181, Training Loss: 8.9039, Validation Loss: 7.4659, Test Loss: 8.8181\n",
      "Epoch 191, Training Loss: 5.6107, Validation Loss: 4.4847, Test Loss: 5.7975\n",
      "Epoch 201, Training Loss: 3.5939, Validation Loss: 2.7342, Test Loss: 3.9166\n",
      "Epoch 211, Training Loss: 2.3952, Validation Loss: 1.7174, Test Loss: 2.7713\n",
      "Epoch 221, Training Loss: 1.7126, Validation Loss: 1.1571, Test Loss: 2.0975\n",
      "Epoch 231, Training Loss: 1.3054, Validation Loss: 0.8195, Test Loss: 1.7007\n",
      "Epoch 241, Training Loss: 1.0606, Validation Loss: 0.6271, Test Loss: 1.4581\n",
      "Epoch 251, Training Loss: 0.9069, Validation Loss: 0.5226, Test Loss: 1.2767\n",
      "Epoch 261, Training Loss: 0.8105, Validation Loss: 0.4697, Test Loss: 1.1572\n",
      "Epoch 271, Training Loss: 0.7483, Validation Loss: 0.4388, Test Loss: 1.0558\n",
      "Epoch 281, Training Loss: 0.6983, Validation Loss: 0.4146, Test Loss: 0.9890\n",
      "Epoch 291, Training Loss: 0.6666, Validation Loss: 0.3992, Test Loss: 0.9458\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.6666\n",
      "Validation loss: 0.3992\n",
      "Test loss: 0.9458\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 52/243 with parameters:\n",
      "Config indices: (0, 1, 2, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7163.8618, Validation Loss: 6553.1421, Test Loss: 7170.7188\n",
      "Epoch 11, Training Loss: 7141.1606, Validation Loss: 6531.4160, Test Loss: 7148.2158\n",
      "Epoch 21, Training Loss: 7034.1987, Validation Loss: 6428.7393, Test Loss: 7042.5728\n",
      "Epoch 31, Training Loss: 6710.8628, Validation Loss: 6117.3311, Test Loss: 6723.5190\n",
      "Epoch 41, Training Loss: 6023.1094, Validation Loss: 5455.7163, Test Loss: 6044.1934\n",
      "Epoch 51, Training Loss: 4949.6401, Validation Loss: 4430.6909, Test Loss: 4979.8135\n",
      "Epoch 61, Training Loss: 3669.4873, Validation Loss: 3233.6038, Test Loss: 3701.3083\n",
      "Epoch 71, Training Loss: 2509.6807, Validation Loss: 2190.3728, Test Loss: 2534.8093\n",
      "Epoch 81, Training Loss: 1628.5863, Validation Loss: 1429.3108, Test Loss: 1651.4772\n",
      "Epoch 91, Training Loss: 1062.7570, Validation Loss: 953.1068, Test Loss: 1090.7231\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 1062.7570\n",
      "Validation loss: 953.1068\n",
      "Test loss: 1090.7231\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 53/243 with parameters:\n",
      "Config indices: (0, 1, 2, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.5869, Validation Loss: 6555.6611, Test Loss: 7173.4009\n",
      "Epoch 11, Training Loss: 7158.4365, Validation Loss: 6547.6064, Test Loss: 7165.2817\n",
      "Epoch 21, Training Loss: 7131.8003, Validation Loss: 6520.9600, Test Loss: 7138.8188\n",
      "Epoch 31, Training Loss: 7034.2339, Validation Loss: 6423.9238, Test Loss: 7042.2646\n",
      "Epoch 41, Training Loss: 6786.5049, Validation Loss: 6177.6768, Test Loss: 6797.3193\n",
      "Epoch 51, Training Loss: 6320.3130, Validation Loss: 5716.0557, Test Loss: 6335.8032\n",
      "Epoch 61, Training Loss: 5626.6646, Validation Loss: 5036.4502, Test Loss: 5646.4194\n",
      "Epoch 71, Training Loss: 4776.3984, Validation Loss: 4219.7612, Test Loss: 4795.6011\n",
      "Epoch 81, Training Loss: 3899.9971, Validation Loss: 3405.5459, Test Loss: 3911.5930\n",
      "Epoch 91, Training Loss: 3073.0195, Validation Loss: 2668.6753, Test Loss: 3075.6021\n",
      "Epoch 101, Training Loss: 2311.5779, Validation Loss: 2008.2402, Test Loss: 2312.6299\n",
      "Epoch 111, Training Loss: 1645.8855, Validation Loss: 1436.9385, Test Loss: 1652.2839\n",
      "Epoch 121, Training Loss: 1134.2052, Validation Loss: 1002.7442, Test Loss: 1148.4634\n",
      "Epoch 131, Training Loss: 800.3444, Validation Loss: 723.8275, Test Loss: 821.5823\n",
      "Epoch 141, Training Loss: 612.6996, Validation Loss: 571.4036, Test Loss: 638.8740\n",
      "Epoch 151, Training Loss: 510.0139, Validation Loss: 488.2699, Test Loss: 538.5101\n",
      "Epoch 161, Training Loss: 442.0897, Validation Loss: 430.6699, Test Loss: 470.4678\n",
      "Epoch 171, Training Loss: 391.2046, Validation Loss: 385.0157, Test Loss: 417.6005\n",
      "Epoch 181, Training Loss: 350.7796, Validation Loss: 347.0379, Test Loss: 373.9059\n",
      "Epoch 191, Training Loss: 318.4747, Validation Loss: 316.1152, Test Loss: 338.3176\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 318.4747\n",
      "Validation loss: 316.1152\n",
      "Test loss: 338.3176\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 54/243 with parameters:\n",
      "Config indices: (0, 1, 2, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7165.0503, Validation Loss: 6554.1997, Test Loss: 7171.8896\n",
      "Epoch 11, Training Loss: 7144.7476, Validation Loss: 6534.9590, Test Loss: 7151.6885\n",
      "Epoch 21, Training Loss: 7054.9360, Validation Loss: 6449.8960, Test Loss: 7062.8447\n",
      "Epoch 31, Training Loss: 6760.2095, Validation Loss: 6171.8145, Test Loss: 6772.1411\n",
      "Epoch 41, Training Loss: 6093.2788, Validation Loss: 5544.2939, Test Loss: 6114.6064\n",
      "Epoch 51, Training Loss: 4989.4014, Validation Loss: 4509.8057, Test Loss: 5025.1934\n",
      "Epoch 61, Training Loss: 3588.8296, Validation Loss: 3208.9089, Test Loss: 3639.6179\n",
      "Epoch 71, Training Loss: 2262.0374, Validation Loss: 1997.4026, Test Loss: 2320.8369\n",
      "Epoch 81, Training Loss: 1378.4688, Validation Loss: 1216.4973, Test Loss: 1436.6072\n",
      "Epoch 91, Training Loss: 972.6486, Validation Loss: 878.3591, Test Loss: 1028.2217\n",
      "Epoch 101, Training Loss: 827.9374, Validation Loss: 766.8278, Test Loss: 883.1688\n",
      "Epoch 111, Training Loss: 762.1650, Validation Loss: 715.8032, Test Loss: 815.5944\n",
      "Epoch 121, Training Loss: 707.9392, Validation Loss: 669.4287, Test Loss: 757.5482\n",
      "Epoch 131, Training Loss: 652.4410, Validation Loss: 619.8711, Test Loss: 697.4084\n",
      "Epoch 141, Training Loss: 593.1022, Validation Loss: 565.8121, Test Loss: 632.6183\n",
      "Epoch 151, Training Loss: 529.7520, Validation Loss: 507.1121, Test Loss: 563.6439\n",
      "Epoch 161, Training Loss: 463.4168, Validation Loss: 446.1692, Test Loss: 491.5190\n",
      "Epoch 171, Training Loss: 396.7355, Validation Loss: 384.3414, Test Loss: 418.1124\n",
      "Epoch 181, Training Loss: 333.0026, Validation Loss: 324.4742, Test Loss: 348.1648\n",
      "Epoch 191, Training Loss: 275.9220, Validation Loss: 269.9711, Test Loss: 286.0299\n",
      "Epoch 201, Training Loss: 227.2063, Validation Loss: 223.0679, Test Loss: 233.1525\n",
      "Epoch 211, Training Loss: 187.7824, Validation Loss: 184.4561, Test Loss: 190.2522\n",
      "Epoch 221, Training Loss: 155.4098, Validation Loss: 152.3061, Test Loss: 155.5294\n",
      "Epoch 231, Training Loss: 126.9681, Validation Loss: 123.9190, Test Loss: 125.7478\n",
      "Epoch 241, Training Loss: 101.9819, Validation Loss: 98.5011, Test Loss: 100.3704\n",
      "Epoch 251, Training Loss: 80.1159, Validation Loss: 76.2894, Test Loss: 78.7476\n",
      "Epoch 261, Training Loss: 61.9713, Validation Loss: 58.1328, Test Loss: 60.9950\n",
      "Epoch 271, Training Loss: 47.2443, Validation Loss: 43.5333, Test Loss: 46.6715\n",
      "Epoch 281, Training Loss: 35.8806, Validation Loss: 32.3120, Test Loss: 35.6788\n",
      "Epoch 291, Training Loss: 27.2388, Validation Loss: 23.9445, Test Loss: 27.2447\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 27.2388\n",
      "Validation loss: 23.9445\n",
      "Test loss: 27.2447\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 55/243 with parameters:\n",
      "Config indices: (0, 2, 0, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 3912.0300, Validation Loss: 3406.6260, Test Loss: 3933.6611\n",
      "Epoch 11, Training Loss: 91.8899, Validation Loss: 89.9484, Test Loss: 95.0653\n",
      "Epoch 21, Training Loss: 18.4427, Validation Loss: 18.1942, Test Loss: 17.0903\n",
      "Epoch 31, Training Loss: 6.7754, Validation Loss: 6.8856, Test Loss: 6.0275\n",
      "Epoch 41, Training Loss: 4.0301, Validation Loss: 4.1778, Test Loss: 3.7171\n",
      "Epoch 51, Training Loss: 2.9864, Validation Loss: 3.0332, Test Loss: 2.8170\n",
      "Epoch 61, Training Loss: 2.4096, Validation Loss: 2.4674, Test Loss: 2.3167\n",
      "Epoch 71, Training Loss: 2.0489, Validation Loss: 2.1031, Test Loss: 2.0025\n",
      "Epoch 81, Training Loss: 1.7799, Validation Loss: 1.7904, Test Loss: 1.7627\n",
      "Epoch 91, Training Loss: 1.5751, Validation Loss: 1.5605, Test Loss: 1.5812\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 1.5751\n",
      "Validation loss: 1.5605\n",
      "Test loss: 1.5812\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 56/243 with parameters:\n",
      "Config indices: (0, 2, 0, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 5082.8042, Validation Loss: 4607.1196, Test Loss: 5118.5254\n",
      "Epoch 11, Training Loss: 278.8557, Validation Loss: 279.4086, Test Loss: 286.5797\n",
      "Epoch 21, Training Loss: 86.3600, Validation Loss: 85.1436, Test Loss: 82.5578\n",
      "Epoch 31, Training Loss: 25.4781, Validation Loss: 23.4544, Test Loss: 24.5254\n",
      "Epoch 41, Training Loss: 11.1520, Validation Loss: 9.8400, Test Loss: 11.2675\n",
      "Epoch 51, Training Loss: 6.4884, Validation Loss: 5.4574, Test Loss: 6.7915\n",
      "Epoch 61, Training Loss: 4.2474, Validation Loss: 3.4374, Test Loss: 4.5680\n",
      "Epoch 71, Training Loss: 2.9413, Validation Loss: 2.2503, Test Loss: 3.2529\n",
      "Epoch 81, Training Loss: 2.1525, Validation Loss: 1.5430, Test Loss: 2.4568\n",
      "Epoch 91, Training Loss: 1.6967, Validation Loss: 1.1643, Test Loss: 2.0127\n",
      "Epoch 101, Training Loss: 1.4018, Validation Loss: 0.9284, Test Loss: 1.7285\n",
      "Epoch 111, Training Loss: 1.2078, Validation Loss: 0.7770, Test Loss: 1.5424\n",
      "Epoch 121, Training Loss: 1.0721, Validation Loss: 0.6704, Test Loss: 1.3907\n",
      "Epoch 131, Training Loss: 0.9386, Validation Loss: 0.5734, Test Loss: 1.2469\n",
      "Epoch 141, Training Loss: 0.8109, Validation Loss: 0.4791, Test Loss: 1.1040\n",
      "Epoch 151, Training Loss: 0.7163, Validation Loss: 0.4167, Test Loss: 0.9910\n",
      "Epoch 161, Training Loss: 0.6540, Validation Loss: 0.3794, Test Loss: 0.9164\n",
      "Epoch 171, Training Loss: 0.6122, Validation Loss: 0.3584, Test Loss: 0.8601\n",
      "Epoch 181, Training Loss: 0.5861, Validation Loss: 0.3453, Test Loss: 0.8227\n",
      "Epoch 191, Training Loss: 0.5691, Validation Loss: 0.3380, Test Loss: 0.7976\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.5691\n",
      "Validation loss: 0.3380\n",
      "Test loss: 0.7976\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 57/243 with parameters:\n",
      "Config indices: (0, 2, 0, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 3474.0139, Validation Loss: 3063.9297, Test Loss: 3512.7075\n",
      "Epoch 11, Training Loss: 123.6846, Validation Loss: 125.2521, Test Loss: 122.4942\n",
      "Epoch 21, Training Loss: 18.6428, Validation Loss: 18.3855, Test Loss: 16.9974\n",
      "Epoch 31, Training Loss: 5.0352, Validation Loss: 5.0252, Test Loss: 4.9253\n",
      "Epoch 41, Training Loss: 2.9774, Validation Loss: 3.0466, Test Loss: 3.0276\n",
      "Epoch 51, Training Loss: 2.2586, Validation Loss: 2.3030, Test Loss: 2.3545\n",
      "Epoch 61, Training Loss: 1.8787, Validation Loss: 1.8626, Test Loss: 1.9939\n",
      "Epoch 71, Training Loss: 1.6244, Validation Loss: 1.5265, Test Loss: 1.7452\n",
      "Epoch 81, Training Loss: 1.4477, Validation Loss: 1.3088, Test Loss: 1.5662\n",
      "Epoch 91, Training Loss: 1.3158, Validation Loss: 1.1653, Test Loss: 1.4478\n",
      "Epoch 101, Training Loss: 1.2203, Validation Loss: 1.0484, Test Loss: 1.3555\n",
      "Epoch 111, Training Loss: 1.1435, Validation Loss: 0.9693, Test Loss: 1.2714\n",
      "Epoch 121, Training Loss: 1.0769, Validation Loss: 0.9092, Test Loss: 1.2044\n",
      "Epoch 131, Training Loss: 1.0213, Validation Loss: 0.8558, Test Loss: 1.1520\n",
      "Epoch 141, Training Loss: 0.9740, Validation Loss: 0.8112, Test Loss: 1.1040\n",
      "Epoch 151, Training Loss: 0.9337, Validation Loss: 0.7694, Test Loss: 1.0642\n",
      "Epoch 161, Training Loss: 0.8985, Validation Loss: 0.7302, Test Loss: 1.0264\n",
      "Epoch 171, Training Loss: 0.8685, Validation Loss: 0.6972, Test Loss: 0.9944\n",
      "Epoch 181, Training Loss: 0.8422, Validation Loss: 0.6689, Test Loss: 0.9658\n",
      "Epoch 191, Training Loss: 0.8174, Validation Loss: 0.6407, Test Loss: 0.9448\n",
      "Epoch 201, Training Loss: 0.7967, Validation Loss: 0.6231, Test Loss: 0.9181\n",
      "Epoch 211, Training Loss: 0.7775, Validation Loss: 0.5991, Test Loss: 0.9015\n",
      "Epoch 221, Training Loss: 0.7620, Validation Loss: 0.5829, Test Loss: 0.8850\n",
      "Epoch 231, Training Loss: 0.7504, Validation Loss: 0.5662, Test Loss: 0.8653\n",
      "Epoch 241, Training Loss: 0.7353, Validation Loss: 0.5490, Test Loss: 0.8564\n",
      "Epoch 251, Training Loss: 0.7247, Validation Loss: 0.5369, Test Loss: 0.8404\n",
      "Epoch 261, Training Loss: 0.7132, Validation Loss: 0.5184, Test Loss: 0.8337\n",
      "Epoch 271, Training Loss: 0.7032, Validation Loss: 0.5114, Test Loss: 0.8193\n",
      "Epoch 281, Training Loss: 0.6934, Validation Loss: 0.5023, Test Loss: 0.8121\n",
      "Epoch 291, Training Loss: 0.6853, Validation Loss: 0.4905, Test Loss: 0.8085\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.6853\n",
      "Validation loss: 0.4905\n",
      "Test loss: 0.8085\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 58/243 with parameters:\n",
      "Config indices: (0, 2, 0, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6559.5396, Validation Loss: 5984.3003, Test Loss: 6574.3486\n",
      "Epoch 11, Training Loss: 503.7047, Validation Loss: 485.3275, Test Loss: 533.1692\n",
      "Epoch 21, Training Loss: 149.2495, Validation Loss: 148.3762, Test Loss: 150.5495\n",
      "Epoch 31, Training Loss: 39.5584, Validation Loss: 38.3774, Test Loss: 37.2948\n",
      "Epoch 41, Training Loss: 13.7327, Validation Loss: 13.4337, Test Loss: 12.7222\n",
      "Epoch 51, Training Loss: 7.3093, Validation Loss: 7.1424, Test Loss: 7.1335\n",
      "Epoch 61, Training Loss: 4.4803, Validation Loss: 4.3665, Test Loss: 4.6035\n",
      "Epoch 71, Training Loss: 3.0175, Validation Loss: 2.9941, Test Loss: 3.2536\n",
      "Epoch 81, Training Loss: 2.2129, Validation Loss: 2.1756, Test Loss: 2.4692\n",
      "Epoch 91, Training Loss: 1.7302, Validation Loss: 1.6638, Test Loss: 1.9775\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 1.7302\n",
      "Validation loss: 1.6638\n",
      "Test loss: 1.9775\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 59/243 with parameters:\n",
      "Config indices: (0, 2, 0, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6190.8501, Validation Loss: 5634.1104, Test Loss: 6211.2544\n",
      "Epoch 11, Training Loss: 449.4604, Validation Loss: 440.4501, Test Loss: 471.2386\n",
      "Epoch 21, Training Loss: 165.1601, Validation Loss: 165.9044, Test Loss: 162.2691\n",
      "Epoch 31, Training Loss: 67.6860, Validation Loss: 65.9655, Test Loss: 64.4141\n",
      "Epoch 41, Training Loss: 29.0491, Validation Loss: 27.3083, Test Loss: 27.8450\n",
      "Epoch 51, Training Loss: 14.6738, Validation Loss: 13.1956, Test Loss: 14.4307\n",
      "Epoch 61, Training Loss: 8.1612, Validation Loss: 6.9253, Test Loss: 8.2042\n",
      "Epoch 71, Training Loss: 5.0121, Validation Loss: 3.9633, Test Loss: 5.2261\n",
      "Epoch 81, Training Loss: 3.4496, Validation Loss: 2.5772, Test Loss: 3.7524\n",
      "Epoch 91, Training Loss: 2.5807, Validation Loss: 1.8334, Test Loss: 2.9092\n",
      "Epoch 101, Training Loss: 2.0769, Validation Loss: 1.4312, Test Loss: 2.4211\n",
      "Epoch 111, Training Loss: 1.7577, Validation Loss: 1.1830, Test Loss: 2.1026\n",
      "Epoch 121, Training Loss: 1.5306, Validation Loss: 1.0164, Test Loss: 1.8777\n",
      "Epoch 131, Training Loss: 1.3687, Validation Loss: 0.9004, Test Loss: 1.7086\n",
      "Epoch 141, Training Loss: 1.2439, Validation Loss: 0.8050, Test Loss: 1.5768\n",
      "Epoch 151, Training Loss: 1.1430, Validation Loss: 0.7268, Test Loss: 1.4646\n",
      "Epoch 161, Training Loss: 1.0596, Validation Loss: 0.6614, Test Loss: 1.3720\n",
      "Epoch 171, Training Loss: 0.9844, Validation Loss: 0.6049, Test Loss: 1.2853\n",
      "Epoch 181, Training Loss: 0.9092, Validation Loss: 0.5514, Test Loss: 1.2025\n",
      "Epoch 191, Training Loss: 0.8432, Validation Loss: 0.5044, Test Loss: 1.1273\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.8432\n",
      "Validation loss: 0.5044\n",
      "Test loss: 1.1273\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 60/243 with parameters:\n",
      "Config indices: (0, 2, 0, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6578.0068, Validation Loss: 5987.5327, Test Loss: 6591.8491\n",
      "Epoch 11, Training Loss: 423.9079, Validation Loss: 410.6814, Test Loss: 452.1364\n",
      "Epoch 21, Training Loss: 129.6847, Validation Loss: 129.0219, Test Loss: 132.5330\n",
      "Epoch 31, Training Loss: 41.8203, Validation Loss: 41.5863, Test Loss: 39.3346\n",
      "Epoch 41, Training Loss: 15.2505, Validation Loss: 15.1367, Test Loss: 13.2048\n",
      "Epoch 51, Training Loss: 7.7471, Validation Loss: 7.7907, Test Loss: 6.5257\n",
      "Epoch 61, Training Loss: 4.9519, Validation Loss: 5.1005, Test Loss: 4.2635\n",
      "Epoch 71, Training Loss: 3.5352, Validation Loss: 3.7049, Test Loss: 3.1152\n",
      "Epoch 81, Training Loss: 2.6899, Validation Loss: 2.8369, Test Loss: 2.4418\n",
      "Epoch 91, Training Loss: 2.1610, Validation Loss: 2.2840, Test Loss: 2.0339\n",
      "Epoch 101, Training Loss: 1.8094, Validation Loss: 1.9283, Test Loss: 1.7669\n",
      "Epoch 111, Training Loss: 1.5730, Validation Loss: 1.6753, Test Loss: 1.5837\n",
      "Epoch 121, Training Loss: 1.4069, Validation Loss: 1.4916, Test Loss: 1.4562\n",
      "Epoch 131, Training Loss: 1.2877, Validation Loss: 1.3551, Test Loss: 1.3632\n",
      "Epoch 141, Training Loss: 1.1961, Validation Loss: 1.2492, Test Loss: 1.2863\n",
      "Epoch 151, Training Loss: 1.1241, Validation Loss: 1.1700, Test Loss: 1.2252\n",
      "Epoch 161, Training Loss: 1.0662, Validation Loss: 1.1039, Test Loss: 1.1716\n",
      "Epoch 171, Training Loss: 1.0184, Validation Loss: 1.0508, Test Loss: 1.1310\n",
      "Epoch 181, Training Loss: 0.9791, Validation Loss: 1.0085, Test Loss: 1.0931\n",
      "Epoch 191, Training Loss: 0.9443, Validation Loss: 0.9681, Test Loss: 1.0548\n",
      "Epoch 201, Training Loss: 0.9155, Validation Loss: 0.9377, Test Loss: 1.0281\n",
      "Epoch 211, Training Loss: 0.8907, Validation Loss: 0.9129, Test Loss: 1.0058\n",
      "Epoch 221, Training Loss: 0.8672, Validation Loss: 0.8870, Test Loss: 0.9758\n",
      "Epoch 231, Training Loss: 0.8467, Validation Loss: 0.8658, Test Loss: 0.9533\n",
      "Epoch 241, Training Loss: 0.8288, Validation Loss: 0.8473, Test Loss: 0.9356\n",
      "Epoch 251, Training Loss: 0.8127, Validation Loss: 0.8303, Test Loss: 0.9158\n",
      "Epoch 261, Training Loss: 0.7962, Validation Loss: 0.8114, Test Loss: 0.8972\n",
      "Epoch 271, Training Loss: 0.7821, Validation Loss: 0.7966, Test Loss: 0.8824\n",
      "Epoch 281, Training Loss: 0.7693, Validation Loss: 0.7819, Test Loss: 0.8667\n",
      "Epoch 291, Training Loss: 0.7570, Validation Loss: 0.7670, Test Loss: 0.8515\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.7570\n",
      "Validation loss: 0.7670\n",
      "Test loss: 0.8515\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 61/243 with parameters:\n",
      "Config indices: (0, 2, 0, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6901.7012, Validation Loss: 6302.8413, Test Loss: 6911.4014\n",
      "Epoch 11, Training Loss: 623.3236, Validation Loss: 595.4494, Test Loss: 661.1916\n",
      "Epoch 21, Training Loss: 323.1834, Validation Loss: 317.6725, Test Loss: 339.3247\n",
      "Epoch 31, Training Loss: 197.9418, Validation Loss: 195.0845, Test Loss: 200.4275\n",
      "Epoch 41, Training Loss: 131.4128, Validation Loss: 129.3085, Test Loss: 129.1132\n",
      "Epoch 51, Training Loss: 86.0271, Validation Loss: 84.0371, Test Loss: 83.4124\n",
      "Epoch 61, Training Loss: 55.8986, Validation Loss: 53.6002, Test Loss: 53.3741\n",
      "Epoch 71, Training Loss: 37.0238, Validation Loss: 34.8156, Test Loss: 35.3488\n",
      "Epoch 81, Training Loss: 24.9398, Validation Loss: 22.8343, Test Loss: 23.9049\n",
      "Epoch 91, Training Loss: 16.4423, Validation Loss: 14.4598, Test Loss: 15.8912\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 16.4423\n",
      "Validation loss: 14.4598\n",
      "Test loss: 15.8912\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 62/243 with parameters:\n",
      "Config indices: (0, 2, 0, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6881.3540, Validation Loss: 6291.6792, Test Loss: 6891.4575\n",
      "Epoch 11, Training Loss: 621.3899, Validation Loss: 594.2187, Test Loss: 663.3513\n",
      "Epoch 21, Training Loss: 301.5164, Validation Loss: 297.8466, Test Loss: 315.5379\n",
      "Epoch 31, Training Loss: 168.6084, Validation Loss: 167.0828, Test Loss: 168.1358\n",
      "Epoch 41, Training Loss: 98.6009, Validation Loss: 96.5542, Test Loss: 95.9712\n",
      "Epoch 51, Training Loss: 55.6006, Validation Loss: 52.7906, Test Loss: 53.4792\n",
      "Epoch 61, Training Loss: 31.7829, Validation Loss: 28.7801, Test Loss: 30.6932\n",
      "Epoch 71, Training Loss: 19.6445, Validation Loss: 16.9574, Test Loss: 19.2934\n",
      "Epoch 81, Training Loss: 13.5675, Validation Loss: 11.2308, Test Loss: 13.5044\n",
      "Epoch 91, Training Loss: 10.1509, Validation Loss: 8.1485, Test Loss: 10.2902\n",
      "Epoch 101, Training Loss: 7.7906, Validation Loss: 6.0813, Test Loss: 8.1560\n",
      "Epoch 111, Training Loss: 5.7063, Validation Loss: 4.2450, Test Loss: 6.1338\n",
      "Epoch 121, Training Loss: 4.2324, Validation Loss: 2.9799, Test Loss: 4.6770\n",
      "Epoch 131, Training Loss: 3.1505, Validation Loss: 2.1043, Test Loss: 3.6022\n",
      "Epoch 141, Training Loss: 2.3983, Validation Loss: 1.5465, Test Loss: 2.8325\n",
      "Epoch 151, Training Loss: 1.9145, Validation Loss: 1.2018, Test Loss: 2.3102\n",
      "Epoch 161, Training Loss: 1.5884, Validation Loss: 1.0045, Test Loss: 1.9557\n",
      "Epoch 171, Training Loss: 1.3749, Validation Loss: 0.8924, Test Loss: 1.7121\n",
      "Epoch 181, Training Loss: 1.2213, Validation Loss: 0.8048, Test Loss: 1.5196\n",
      "Epoch 191, Training Loss: 1.1097, Validation Loss: 0.7567, Test Loss: 1.3926\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 1.1097\n",
      "Validation loss: 0.7567\n",
      "Test loss: 1.3926\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 63/243 with parameters:\n",
      "Config indices: (0, 2, 0, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7107.3232, Validation Loss: 6498.6445, Test Loss: 7114.5063\n",
      "Epoch 11, Training Loss: 869.7321, Validation Loss: 797.0945, Test Loss: 902.4377\n",
      "Epoch 21, Training Loss: 492.1455, Validation Loss: 483.0748, Test Loss: 521.6437\n",
      "Epoch 31, Training Loss: 305.8709, Validation Loss: 307.8719, Test Loss: 319.0597\n",
      "Epoch 41, Training Loss: 223.1551, Validation Loss: 226.2840, Test Loss: 226.2735\n",
      "Epoch 51, Training Loss: 167.1866, Validation Loss: 169.2757, Test Loss: 166.5402\n",
      "Epoch 61, Training Loss: 123.9705, Validation Loss: 125.2071, Test Loss: 121.0970\n",
      "Epoch 71, Training Loss: 90.8278, Validation Loss: 91.1953, Test Loss: 87.6297\n",
      "Epoch 81, Training Loss: 67.1501, Validation Loss: 67.1359, Test Loss: 64.3516\n",
      "Epoch 91, Training Loss: 49.6117, Validation Loss: 49.3382, Test Loss: 47.4691\n",
      "Epoch 101, Training Loss: 36.3579, Validation Loss: 35.7953, Test Loss: 34.8376\n",
      "Epoch 111, Training Loss: 26.6971, Validation Loss: 25.8415, Test Loss: 25.6043\n",
      "Epoch 121, Training Loss: 19.6807, Validation Loss: 18.7776, Test Loss: 19.0536\n",
      "Epoch 131, Training Loss: 15.0862, Validation Loss: 14.2281, Test Loss: 14.6823\n",
      "Epoch 141, Training Loss: 12.0749, Validation Loss: 11.2510, Test Loss: 11.8010\n",
      "Epoch 151, Training Loss: 9.9267, Validation Loss: 9.1708, Test Loss: 9.7715\n",
      "Epoch 161, Training Loss: 8.3506, Validation Loss: 7.6512, Test Loss: 8.2797\n",
      "Epoch 171, Training Loss: 7.1291, Validation Loss: 6.4916, Test Loss: 7.1472\n",
      "Epoch 181, Training Loss: 6.1448, Validation Loss: 5.5671, Test Loss: 6.2036\n",
      "Epoch 191, Training Loss: 5.3419, Validation Loss: 4.7951, Test Loss: 5.4118\n",
      "Epoch 201, Training Loss: 4.6797, Validation Loss: 4.1703, Test Loss: 4.7873\n",
      "Epoch 211, Training Loss: 4.1236, Validation Loss: 3.6221, Test Loss: 4.2153\n",
      "Epoch 221, Training Loss: 3.6515, Validation Loss: 3.1698, Test Loss: 3.7613\n",
      "Epoch 231, Training Loss: 3.2580, Validation Loss: 2.7980, Test Loss: 3.3840\n",
      "Epoch 241, Training Loss: 2.9335, Validation Loss: 2.4911, Test Loss: 3.0502\n",
      "Epoch 251, Training Loss: 2.6710, Validation Loss: 2.2469, Test Loss: 2.8043\n",
      "Epoch 261, Training Loss: 2.4485, Validation Loss: 2.0331, Test Loss: 2.5811\n",
      "Epoch 271, Training Loss: 2.2540, Validation Loss: 1.8495, Test Loss: 2.4006\n",
      "Epoch 281, Training Loss: 2.0854, Validation Loss: 1.6894, Test Loss: 2.2445\n",
      "Epoch 291, Training Loss: 1.9374, Validation Loss: 1.5499, Test Loss: 2.1041\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 1.9374\n",
      "Validation loss: 1.5499\n",
      "Test loss: 2.1041\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 64/243 with parameters:\n",
      "Config indices: (0, 2, 1, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7161.4365, Validation Loss: 6550.8115, Test Loss: 7168.2544\n",
      "Epoch 11, Training Loss: 7075.6299, Validation Loss: 6470.9717, Test Loss: 7083.3423\n",
      "Epoch 21, Training Loss: 6899.3687, Validation Loss: 6307.6982, Test Loss: 6909.5693\n",
      "Epoch 31, Training Loss: 6621.2407, Validation Loss: 6049.3984, Test Loss: 6635.5942\n",
      "Epoch 41, Training Loss: 6244.2412, Validation Loss: 5698.7544, Test Loss: 6264.2373\n",
      "Epoch 51, Training Loss: 5789.2451, Validation Loss: 5275.6763, Test Loss: 5815.9507\n",
      "Epoch 61, Training Loss: 5277.9976, Validation Loss: 4800.6943, Test Loss: 5312.0220\n",
      "Epoch 71, Training Loss: 4733.2456, Validation Loss: 4295.3320, Test Loss: 4774.7163\n",
      "Epoch 81, Training Loss: 4178.3071, Validation Loss: 3781.6021, Test Loss: 4226.8789\n",
      "Epoch 91, Training Loss: 3635.2041, Validation Loss: 3280.2942, Test Loss: 3690.1028\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 3635.2041\n",
      "Validation loss: 3280.2942\n",
      "Test loss: 3690.1028\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 65/243 with parameters:\n",
      "Config indices: (0, 2, 1, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7156.7388, Validation Loss: 6546.1646, Test Loss: 7163.6152\n",
      "Epoch 11, Training Loss: 6978.6890, Validation Loss: 6376.3228, Test Loss: 6987.3555\n",
      "Epoch 21, Training Loss: 6579.5029, Validation Loss: 5994.6777, Test Loss: 6592.8398\n",
      "Epoch 31, Training Loss: 6004.1758, Validation Loss: 5445.9819, Test Loss: 6023.8301\n",
      "Epoch 41, Training Loss: 5315.4609, Validation Loss: 4793.0605, Test Loss: 5341.4087\n",
      "Epoch 51, Training Loss: 4579.0884, Validation Loss: 4101.0977, Test Loss: 4609.7314\n",
      "Epoch 61, Training Loss: 3858.6213, Validation Loss: 3432.3472, Test Loss: 3891.3020\n",
      "Epoch 71, Training Loss: 3200.8806, Validation Loss: 2831.2407, Test Loss: 3232.9380\n",
      "Epoch 81, Training Loss: 2632.8606, Validation Loss: 2321.4270, Test Loss: 2662.5706\n",
      "Epoch 91, Training Loss: 2161.3125, Validation Loss: 1905.9259, Test Loss: 2188.2910\n",
      "Epoch 101, Training Loss: 1780.5347, Validation Loss: 1575.8544, Test Loss: 1805.4187\n",
      "Epoch 111, Training Loss: 1481.1639, Validation Loss: 1319.8483, Test Loss: 1504.8644\n",
      "Epoch 121, Training Loss: 1252.2717, Validation Loss: 1126.2922, Test Loss: 1275.4938\n",
      "Epoch 131, Training Loss: 1082.3210, Validation Loss: 984.0891, Test Loss: 1105.4000\n",
      "Epoch 141, Training Loss: 959.1394, Validation Loss: 882.1440, Test Loss: 982.3090\n",
      "Epoch 151, Training Loss: 871.2847, Validation Loss: 810.2943, Test Loss: 894.6166\n",
      "Epoch 161, Training Loss: 808.7936, Validation Loss: 759.6738, Test Loss: 832.1726\n",
      "Epoch 171, Training Loss: 763.5702, Validation Loss: 723.3308, Test Loss: 786.9272\n",
      "Epoch 181, Training Loss: 729.7618, Validation Loss: 696.2023, Test Loss: 753.0522\n",
      "Epoch 191, Training Loss: 703.2654, Validation Loss: 674.7509, Test Loss: 726.4814\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 703.2654\n",
      "Validation loss: 674.7509\n",
      "Test loss: 726.4814\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 66/243 with parameters:\n",
      "Config indices: (0, 2, 1, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7160.4043, Validation Loss: 6549.8701, Test Loss: 7167.2422\n",
      "Epoch 11, Training Loss: 7078.2720, Validation Loss: 6471.0806, Test Loss: 7085.9292\n",
      "Epoch 21, Training Loss: 6898.4790, Validation Loss: 6298.1265, Test Loss: 6908.3726\n",
      "Epoch 31, Training Loss: 6621.6885, Validation Loss: 6032.0508, Test Loss: 6635.0645\n",
      "Epoch 41, Training Loss: 6262.2080, Validation Loss: 5687.2236, Test Loss: 6279.9702\n",
      "Epoch 51, Training Loss: 5837.0166, Validation Loss: 5280.7148, Test Loss: 5859.5752\n",
      "Epoch 61, Training Loss: 5365.5469, Validation Loss: 4832.0249, Test Loss: 5392.8926\n",
      "Epoch 71, Training Loss: 4868.9604, Validation Loss: 4362.2363, Test Loss: 4900.4634\n",
      "Epoch 81, Training Loss: 4367.3882, Validation Loss: 3891.3110, Test Loss: 4402.0527\n",
      "Epoch 91, Training Loss: 3879.4509, Validation Loss: 3437.3774, Test Loss: 3916.0503\n",
      "Epoch 101, Training Loss: 3420.0056, Validation Loss: 3014.5388, Test Loss: 3457.3284\n",
      "Epoch 111, Training Loss: 2999.0439, Validation Loss: 2631.8320, Test Loss: 3036.1140\n",
      "Epoch 121, Training Loss: 2621.8994, Validation Loss: 2293.4373, Test Loss: 2658.0840\n",
      "Epoch 131, Training Loss: 2289.9167, Validation Loss: 1999.4020, Test Loss: 2324.9917\n",
      "Epoch 141, Training Loss: 2001.7448, Validation Loss: 1747.3522, Test Loss: 2035.7916\n",
      "Epoch 151, Training Loss: 1754.5958, Validation Loss: 1533.6436, Test Loss: 1787.9393\n",
      "Epoch 161, Training Loss: 1545.1971, Validation Loss: 1354.4517, Test Loss: 1578.2299\n",
      "Epoch 171, Training Loss: 1370.0596, Validation Loss: 1205.9838, Test Loss: 1403.1360\n",
      "Epoch 181, Training Loss: 1225.5829, Validation Loss: 1084.6191, Test Loss: 1258.9664\n",
      "Epoch 191, Training Loss: 1107.9130, Validation Loss: 986.6654, Test Loss: 1141.7777\n",
      "Epoch 201, Training Loss: 1013.1425, Validation Loss: 908.5632, Test Loss: 1047.4673\n",
      "Epoch 211, Training Loss: 937.5717, Validation Loss: 846.9747, Test Loss: 972.3312\n",
      "Epoch 221, Training Loss: 877.5933, Validation Loss: 798.5683, Test Loss: 912.7124\n",
      "Epoch 231, Training Loss: 830.0721, Validation Loss: 760.5804, Test Loss: 865.4315\n",
      "Epoch 241, Training Loss: 792.1897, Validation Loss: 730.5217, Test Loss: 827.7541\n",
      "Epoch 251, Training Loss: 761.6735, Validation Loss: 706.4216, Test Loss: 797.3782\n",
      "Epoch 261, Training Loss: 736.6466, Validation Loss: 686.7083, Test Loss: 772.4185\n",
      "Epoch 271, Training Loss: 715.6832, Validation Loss: 670.1702, Test Loss: 751.4875\n",
      "Epoch 281, Training Loss: 697.7006, Validation Loss: 655.8994, Test Loss: 733.4461\n",
      "Epoch 291, Training Loss: 681.8977, Validation Loss: 643.2523, Test Loss: 717.4992\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 681.8977\n",
      "Validation loss: 643.2523\n",
      "Test loss: 717.4992\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 67/243 with parameters:\n",
      "Config indices: (0, 2, 1, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7162.8784, Validation Loss: 6552.0918, Test Loss: 7169.7119\n",
      "Epoch 11, Training Loss: 7137.3213, Validation Loss: 6527.9111, Test Loss: 7144.3486\n",
      "Epoch 21, Training Loss: 7101.3481, Validation Loss: 6493.8247, Test Loss: 7108.7544\n",
      "Epoch 31, Training Loss: 7045.5918, Validation Loss: 6440.4624, Test Loss: 7053.6948\n",
      "Epoch 41, Training Loss: 6971.7466, Validation Loss: 6369.4697, Test Loss: 6980.8506\n",
      "Epoch 51, Training Loss: 6880.6909, Validation Loss: 6281.7827, Test Loss: 6891.0396\n",
      "Epoch 61, Training Loss: 6773.3408, Validation Loss: 6178.2852, Test Loss: 6785.1396\n",
      "Epoch 71, Training Loss: 6650.7754, Validation Loss: 6060.0459, Test Loss: 6664.2031\n",
      "Epoch 81, Training Loss: 6513.8262, Validation Loss: 5927.9746, Test Loss: 6529.0278\n",
      "Epoch 91, Training Loss: 6363.9243, Validation Loss: 5783.4907, Test Loss: 6381.0029\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 6363.9243\n",
      "Validation loss: 5783.4907\n",
      "Test loss: 6381.0029\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 68/243 with parameters:\n",
      "Config indices: (0, 2, 1, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7164.0186, Validation Loss: 6553.2261, Test Loss: 7170.8369\n",
      "Epoch 11, Training Loss: 7137.3135, Validation Loss: 6528.0762, Test Loss: 7144.3667\n",
      "Epoch 21, Training Loss: 7081.8506, Validation Loss: 6475.9102, Test Loss: 7089.6240\n",
      "Epoch 31, Training Loss: 6991.4009, Validation Loss: 6390.7769, Test Loss: 7000.4292\n",
      "Epoch 41, Training Loss: 6863.5747, Validation Loss: 6270.6392, Test Loss: 6874.4390\n",
      "Epoch 51, Training Loss: 6703.1431, Validation Loss: 6119.9087, Test Loss: 6716.3506\n",
      "Epoch 61, Training Loss: 6511.9072, Validation Loss: 5940.3828, Test Loss: 6527.9014\n",
      "Epoch 71, Training Loss: 6292.8447, Validation Loss: 5734.9111, Test Loss: 6312.0176\n",
      "Epoch 81, Training Loss: 6049.0220, Validation Loss: 5506.4688, Test Loss: 6071.6772\n",
      "Epoch 91, Training Loss: 5784.2920, Validation Loss: 5258.7432, Test Loss: 5810.6450\n",
      "Epoch 101, Training Loss: 5502.9033, Validation Loss: 4995.7285, Test Loss: 5533.0752\n",
      "Epoch 111, Training Loss: 5208.5898, Validation Loss: 4721.0127, Test Loss: 5242.6138\n",
      "Epoch 121, Training Loss: 4904.9580, Validation Loss: 4438.1133, Test Loss: 4942.7925\n",
      "Epoch 131, Training Loss: 4596.0278, Validation Loss: 4150.8516, Test Loss: 4637.5522\n",
      "Epoch 141, Training Loss: 4286.8120, Validation Loss: 3863.9001, Test Loss: 4331.8037\n",
      "Epoch 151, Training Loss: 3980.7361, Validation Loss: 3580.5342, Test Loss: 4028.9041\n",
      "Epoch 161, Training Loss: 3681.5405, Validation Loss: 3304.2795, Test Loss: 3732.5461\n",
      "Epoch 171, Training Loss: 3392.4524, Validation Loss: 3038.1707, Test Loss: 3445.9158\n",
      "Epoch 181, Training Loss: 3115.9224, Validation Loss: 2784.5085, Test Loss: 3171.4402\n",
      "Epoch 191, Training Loss: 2855.0305, Validation Loss: 2546.0950, Test Loss: 2912.1755\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 2855.0305\n",
      "Validation loss: 2546.0950\n",
      "Test loss: 2912.1755\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 69/243 with parameters:\n",
      "Config indices: (0, 2, 1, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7163.9692, Validation Loss: 6553.1611, Test Loss: 7170.7891\n",
      "Epoch 11, Training Loss: 7140.0615, Validation Loss: 6530.3384, Test Loss: 7147.0381\n",
      "Epoch 21, Training Loss: 7093.3247, Validation Loss: 6485.9810, Test Loss: 7100.8315\n",
      "Epoch 31, Training Loss: 7020.0908, Validation Loss: 6416.4712, Test Loss: 7028.5283\n",
      "Epoch 41, Training Loss: 6915.7427, Validation Loss: 6317.5742, Test Loss: 6925.5884\n",
      "Epoch 51, Training Loss: 6778.1929, Validation Loss: 6187.5508, Test Loss: 6789.9341\n",
      "Epoch 61, Training Loss: 6610.9609, Validation Loss: 6029.6001, Test Loss: 6625.0566\n",
      "Epoch 71, Training Loss: 6416.2344, Validation Loss: 5845.8838, Test Loss: 6433.0737\n",
      "Epoch 81, Training Loss: 6197.5518, Validation Loss: 5639.7798, Test Loss: 6217.4512\n",
      "Epoch 91, Training Loss: 5957.2583, Validation Loss: 5413.5693, Test Loss: 5980.4531\n",
      "Epoch 101, Training Loss: 5699.0854, Validation Loss: 5170.8540, Test Loss: 5725.7402\n",
      "Epoch 111, Training Loss: 5426.7529, Validation Loss: 4915.1982, Test Loss: 5456.9463\n",
      "Epoch 121, Training Loss: 5143.0088, Validation Loss: 4649.3438, Test Loss: 5176.7554\n",
      "Epoch 131, Training Loss: 4851.9399, Validation Loss: 4377.1543, Test Loss: 4889.1611\n",
      "Epoch 141, Training Loss: 4557.0366, Validation Loss: 4101.9893, Test Loss: 4597.5771\n",
      "Epoch 151, Training Loss: 4262.3018, Validation Loss: 3827.6660, Test Loss: 4305.9434\n",
      "Epoch 161, Training Loss: 3970.3401, Validation Loss: 3556.7002, Test Loss: 4016.8147\n",
      "Epoch 171, Training Loss: 3684.7158, Validation Loss: 3292.4531, Test Loss: 3733.7061\n",
      "Epoch 181, Training Loss: 3408.6379, Validation Loss: 3037.9148, Test Loss: 3459.7891\n",
      "Epoch 191, Training Loss: 3144.4531, Validation Loss: 2795.2751, Test Loss: 3197.3977\n",
      "Epoch 201, Training Loss: 2894.0037, Validation Loss: 2566.2222, Test Loss: 2948.3630\n",
      "Epoch 211, Training Loss: 2659.0667, Validation Loss: 2352.3792, Test Loss: 2714.4841\n",
      "Epoch 221, Training Loss: 2441.0359, Validation Loss: 2154.9460, Test Loss: 2497.1704\n",
      "Epoch 231, Training Loss: 2240.1968, Validation Loss: 1974.1127, Test Loss: 2296.7463\n",
      "Epoch 241, Training Loss: 2057.2163, Validation Loss: 1810.3727, Test Loss: 2113.9153\n",
      "Epoch 251, Training Loss: 1891.9375, Validation Loss: 1663.4485, Test Loss: 1948.5626\n",
      "Epoch 261, Training Loss: 1743.3843, Validation Loss: 1532.3453, Test Loss: 1799.7732\n",
      "Epoch 271, Training Loss: 1611.5514, Validation Loss: 1416.8885, Test Loss: 1667.5848\n",
      "Epoch 281, Training Loss: 1494.9570, Validation Loss: 1315.6245, Test Loss: 1550.5610\n",
      "Epoch 291, Training Loss: 1392.8674, Validation Loss: 1227.7227, Test Loss: 1448.0255\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 1392.8674\n",
      "Validation loss: 1227.7227\n",
      "Test loss: 1448.0255\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 70/243 with parameters:\n",
      "Config indices: (0, 2, 1, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7164.7080, Validation Loss: 6553.8413, Test Loss: 7171.5269\n",
      "Epoch 11, Training Loss: 7154.4043, Validation Loss: 6543.9634, Test Loss: 7161.2607\n",
      "Epoch 21, Training Loss: 7139.5552, Validation Loss: 6529.7251, Test Loss: 7146.5630\n",
      "Epoch 31, Training Loss: 7115.5542, Validation Loss: 6506.7393, Test Loss: 7122.8398\n",
      "Epoch 41, Training Loss: 7082.6763, Validation Loss: 6475.3354, Test Loss: 7090.3428\n",
      "Epoch 51, Training Loss: 7041.2549, Validation Loss: 6435.8682, Test Loss: 7049.4609\n",
      "Epoch 61, Training Loss: 6991.7793, Validation Loss: 6388.7944, Test Loss: 7000.6592\n",
      "Epoch 71, Training Loss: 6934.1445, Validation Loss: 6334.0273, Test Loss: 6943.8252\n",
      "Epoch 81, Training Loss: 6868.8481, Validation Loss: 6271.9717, Test Loss: 6879.4448\n",
      "Epoch 91, Training Loss: 6795.8911, Validation Loss: 6202.6313, Test Loss: 6807.5083\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 6795.8911\n",
      "Validation loss: 6202.6313\n",
      "Test loss: 6807.5083\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 71/243 with parameters:\n",
      "Config indices: (0, 2, 1, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7164.8442, Validation Loss: 6553.9268, Test Loss: 7171.6587\n",
      "Epoch 11, Training Loss: 7154.6226, Validation Loss: 6543.8164, Test Loss: 7161.4722\n",
      "Epoch 21, Training Loss: 7140.4316, Validation Loss: 6529.6597, Test Loss: 7147.3730\n",
      "Epoch 31, Training Loss: 7120.8760, Validation Loss: 6510.2559, Test Loss: 7127.9888\n",
      "Epoch 41, Training Loss: 7094.5708, Validation Loss: 6484.2656, Test Loss: 7101.9561\n",
      "Epoch 51, Training Loss: 7060.8652, Validation Loss: 6451.0962, Test Loss: 7068.6104\n",
      "Epoch 61, Training Loss: 7020.4746, Validation Loss: 6411.4165, Test Loss: 7028.6665\n",
      "Epoch 71, Training Loss: 6973.3540, Validation Loss: 6365.1982, Test Loss: 6982.0884\n",
      "Epoch 81, Training Loss: 6919.4658, Validation Loss: 6312.4448, Test Loss: 6928.8335\n",
      "Epoch 91, Training Loss: 6858.9414, Validation Loss: 6253.2900, Test Loss: 6869.0225\n",
      "Epoch 101, Training Loss: 6791.9351, Validation Loss: 6187.9146, Test Loss: 6802.8140\n",
      "Epoch 111, Training Loss: 6718.6743, Validation Loss: 6116.5571, Test Loss: 6730.4438\n",
      "Epoch 121, Training Loss: 6639.7622, Validation Loss: 6039.7783, Test Loss: 6652.4951\n",
      "Epoch 131, Training Loss: 6555.5630, Validation Loss: 5957.9033, Test Loss: 6569.2998\n",
      "Epoch 141, Training Loss: 6465.6445, Validation Loss: 5870.6460, Test Loss: 6480.4355\n",
      "Epoch 151, Training Loss: 6371.0366, Validation Loss: 5778.9521, Test Loss: 6386.9180\n",
      "Epoch 161, Training Loss: 6271.7505, Validation Loss: 5682.8574, Test Loss: 6288.7529\n",
      "Epoch 171, Training Loss: 6167.7080, Validation Loss: 5582.3750, Test Loss: 6185.8540\n",
      "Epoch 181, Training Loss: 6056.9712, Validation Loss: 5475.6553, Test Loss: 6076.2661\n",
      "Epoch 191, Training Loss: 5941.7524, Validation Loss: 5364.7417, Test Loss: 5962.2090\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 5941.7524\n",
      "Validation loss: 5364.7417\n",
      "Test loss: 5962.2090\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 72/243 with parameters:\n",
      "Config indices: (0, 2, 1, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7164.5308, Validation Loss: 6553.6255, Test Loss: 7171.3564\n",
      "Epoch 11, Training Loss: 7152.8579, Validation Loss: 6542.1377, Test Loss: 7159.7598\n",
      "Epoch 21, Training Loss: 7135.4141, Validation Loss: 6524.9609, Test Loss: 7142.4917\n",
      "Epoch 31, Training Loss: 7109.9976, Validation Loss: 6499.9585, Test Loss: 7117.3730\n",
      "Epoch 41, Training Loss: 7076.9780, Validation Loss: 6467.4951, Test Loss: 7084.7656\n",
      "Epoch 51, Training Loss: 7036.4043, Validation Loss: 6427.6060, Test Loss: 7044.7139\n",
      "Epoch 61, Training Loss: 6988.3887, Validation Loss: 6380.4082, Test Loss: 6997.3252\n",
      "Epoch 71, Training Loss: 6933.4692, Validation Loss: 6326.4316, Test Loss: 6943.1216\n",
      "Epoch 81, Training Loss: 6871.4912, Validation Loss: 6265.5557, Test Loss: 6881.9429\n",
      "Epoch 91, Training Loss: 6802.9746, Validation Loss: 6198.3071, Test Loss: 6814.3037\n",
      "Epoch 101, Training Loss: 6728.1050, Validation Loss: 6124.8970, Test Loss: 6740.3867\n",
      "Epoch 111, Training Loss: 6647.1143, Validation Loss: 6045.5850, Test Loss: 6660.4028\n",
      "Epoch 121, Training Loss: 6560.2686, Validation Loss: 5960.6357, Test Loss: 6574.6147\n",
      "Epoch 131, Training Loss: 6468.1318, Validation Loss: 5870.5845, Test Loss: 6483.5757\n",
      "Epoch 141, Training Loss: 6371.2109, Validation Loss: 5775.9639, Test Loss: 6387.7705\n",
      "Epoch 151, Training Loss: 6269.3174, Validation Loss: 5676.6567, Test Loss: 6287.0078\n",
      "Epoch 161, Training Loss: 6162.9990, Validation Loss: 5573.2070, Test Loss: 6181.8188\n",
      "Epoch 171, Training Loss: 6053.0454, Validation Loss: 5466.3784, Test Loss: 6072.9800\n",
      "Epoch 181, Training Loss: 5939.5093, Validation Loss: 5356.3091, Test Loss: 5960.5317\n",
      "Epoch 191, Training Loss: 5823.0439, Validation Loss: 5243.5757, Test Loss: 5845.1172\n",
      "Epoch 201, Training Loss: 5704.3569, Validation Loss: 5128.9048, Test Loss: 5727.4233\n",
      "Epoch 211, Training Loss: 5583.0361, Validation Loss: 5011.9751, Test Loss: 5607.0444\n",
      "Epoch 221, Training Loss: 5460.0991, Validation Loss: 4893.7324, Test Loss: 5484.9707\n",
      "Epoch 231, Training Loss: 5335.3110, Validation Loss: 4774.0405, Test Loss: 5360.9746\n",
      "Epoch 241, Training Loss: 5209.8662, Validation Loss: 4654.0024, Test Loss: 5236.2246\n",
      "Epoch 251, Training Loss: 5083.6714, Validation Loss: 4533.5879, Test Loss: 5110.6323\n",
      "Epoch 261, Training Loss: 4957.4644, Validation Loss: 4413.4717, Test Loss: 4984.9146\n",
      "Epoch 271, Training Loss: 4831.2612, Validation Loss: 4293.7505, Test Loss: 4859.1035\n",
      "Epoch 281, Training Loss: 4705.6914, Validation Loss: 4175.0083, Test Loss: 4733.8193\n",
      "Epoch 291, Training Loss: 4580.6265, Validation Loss: 4057.1477, Test Loss: 4608.9302\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 4580.6265\n",
      "Validation loss: 4057.1477\n",
      "Test loss: 4608.9302\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 73/243 with parameters:\n",
      "Config indices: (0, 2, 2, 0, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.5835, Validation Loss: 6555.6519, Test Loss: 7173.3931\n",
      "Epoch 11, Training Loss: 7165.2070, Validation Loss: 6554.3516, Test Loss: 7172.0176\n",
      "Epoch 21, Training Loss: 7164.5015, Validation Loss: 6553.6841, Test Loss: 7171.3164\n",
      "Epoch 31, Training Loss: 7163.9521, Validation Loss: 6553.1641, Test Loss: 7170.7671\n",
      "Epoch 41, Training Loss: 7163.4678, Validation Loss: 6552.7026, Test Loss: 7170.2803\n",
      "Epoch 51, Training Loss: 7163.0112, Validation Loss: 6552.2720, Test Loss: 7169.8271\n",
      "Epoch 61, Training Loss: 7162.5762, Validation Loss: 6551.8555, Test Loss: 7169.3931\n",
      "Epoch 71, Training Loss: 7162.1494, Validation Loss: 6551.4482, Test Loss: 7168.9668\n",
      "Epoch 81, Training Loss: 7161.7339, Validation Loss: 6551.0527, Test Loss: 7168.5537\n",
      "Epoch 91, Training Loss: 7161.3286, Validation Loss: 6550.6665, Test Loss: 7168.1494\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 100\n",
      "Training loss: 7161.3286\n",
      "Validation loss: 6550.6665\n",
      "Test loss: 7168.1494\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 74/243 with parameters:\n",
      "Config indices: (0, 2, 2, 0, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.6216, Validation Loss: 6555.6777, Test Loss: 7173.4385\n",
      "Epoch 11, Training Loss: 7165.5566, Validation Loss: 6554.6270, Test Loss: 7172.3711\n",
      "Epoch 21, Training Loss: 7164.6299, Validation Loss: 6553.7471, Test Loss: 7171.4492\n",
      "Epoch 31, Training Loss: 7163.9580, Validation Loss: 6553.1045, Test Loss: 7170.7754\n",
      "Epoch 41, Training Loss: 7163.3931, Validation Loss: 6552.5635, Test Loss: 7170.2090\n",
      "Epoch 51, Training Loss: 7162.8740, Validation Loss: 6552.0693, Test Loss: 7169.6904\n",
      "Epoch 61, Training Loss: 7162.3882, Validation Loss: 6551.6001, Test Loss: 7169.2026\n",
      "Epoch 71, Training Loss: 7161.9170, Validation Loss: 6551.1514, Test Loss: 7168.7344\n",
      "Epoch 81, Training Loss: 7161.4634, Validation Loss: 6550.7153, Test Loss: 7168.2798\n",
      "Epoch 91, Training Loss: 7161.0195, Validation Loss: 6550.2896, Test Loss: 7167.8345\n",
      "Epoch 101, Training Loss: 7160.5781, Validation Loss: 6549.8682, Test Loss: 7167.3936\n",
      "Epoch 111, Training Loss: 7160.1406, Validation Loss: 6549.4478, Test Loss: 7166.9590\n",
      "Epoch 121, Training Loss: 7159.7012, Validation Loss: 6549.0273, Test Loss: 7166.5220\n",
      "Epoch 131, Training Loss: 7159.1729, Validation Loss: 6548.5337, Test Loss: 7165.9980\n",
      "Epoch 141, Training Loss: 7158.6509, Validation Loss: 6548.0376, Test Loss: 7165.4761\n",
      "Epoch 151, Training Loss: 7158.1333, Validation Loss: 6547.5488, Test Loss: 7164.9619\n",
      "Epoch 161, Training Loss: 7157.6167, Validation Loss: 6547.0605, Test Loss: 7164.4492\n",
      "Epoch 171, Training Loss: 7157.1006, Validation Loss: 6546.5723, Test Loss: 7163.9355\n",
      "Epoch 181, Training Loss: 7156.5859, Validation Loss: 6546.0840, Test Loss: 7163.4209\n",
      "Epoch 191, Training Loss: 7156.0674, Validation Loss: 6545.5923, Test Loss: 7162.9058\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 200\n",
      "Training loss: 7156.0674\n",
      "Validation loss: 6545.5923\n",
      "Test loss: 7162.9058\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 75/243 with parameters:\n",
      "Config indices: (0, 2, 2, 0, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.9287, Validation Loss: 6555.9673, Test Loss: 7173.7437\n",
      "Epoch 11, Training Loss: 7165.7637, Validation Loss: 6554.8682, Test Loss: 7172.5801\n",
      "Epoch 21, Training Loss: 7165.1201, Validation Loss: 6554.2607, Test Loss: 7171.9414\n",
      "Epoch 31, Training Loss: 7164.6265, Validation Loss: 6553.7896, Test Loss: 7171.4463\n",
      "Epoch 41, Training Loss: 7164.1875, Validation Loss: 6553.3716, Test Loss: 7171.0073\n",
      "Epoch 51, Training Loss: 7163.7744, Validation Loss: 6552.9761, Test Loss: 7170.5933\n",
      "Epoch 61, Training Loss: 7163.3618, Validation Loss: 6552.5840, Test Loss: 7170.1807\n",
      "Epoch 71, Training Loss: 7162.9561, Validation Loss: 6552.1982, Test Loss: 7169.7769\n",
      "Epoch 81, Training Loss: 7162.5610, Validation Loss: 6551.8232, Test Loss: 7169.3809\n",
      "Epoch 91, Training Loss: 7162.1689, Validation Loss: 6551.4512, Test Loss: 7168.9893\n",
      "Epoch 101, Training Loss: 7161.7759, Validation Loss: 6551.0791, Test Loss: 7168.5986\n",
      "Epoch 111, Training Loss: 7161.3652, Validation Loss: 6550.6914, Test Loss: 7168.1899\n",
      "Epoch 121, Training Loss: 7160.9448, Validation Loss: 6550.2910, Test Loss: 7167.7676\n",
      "Epoch 131, Training Loss: 7160.5244, Validation Loss: 6549.8936, Test Loss: 7167.3506\n",
      "Epoch 141, Training Loss: 7160.1040, Validation Loss: 6549.4917, Test Loss: 7166.9297\n",
      "Epoch 151, Training Loss: 7159.6704, Validation Loss: 6549.0791, Test Loss: 7166.5000\n",
      "Epoch 161, Training Loss: 7159.2222, Validation Loss: 6548.6523, Test Loss: 7166.0522\n",
      "Epoch 171, Training Loss: 7158.7759, Validation Loss: 6548.2280, Test Loss: 7165.6099\n",
      "Epoch 181, Training Loss: 7158.3306, Validation Loss: 6547.8037, Test Loss: 7165.1655\n",
      "Epoch 191, Training Loss: 7157.8799, Validation Loss: 6547.3755, Test Loss: 7164.7168\n",
      "Epoch 201, Training Loss: 7157.4219, Validation Loss: 6546.9409, Test Loss: 7164.2642\n",
      "Epoch 211, Training Loss: 7156.9609, Validation Loss: 6546.5015, Test Loss: 7163.8052\n",
      "Epoch 221, Training Loss: 7156.4937, Validation Loss: 6546.0557, Test Loss: 7163.3408\n",
      "Epoch 231, Training Loss: 7156.0205, Validation Loss: 6545.6055, Test Loss: 7162.8696\n",
      "Epoch 241, Training Loss: 7155.5391, Validation Loss: 6545.1455, Test Loss: 7162.3931\n",
      "Epoch 251, Training Loss: 7155.0469, Validation Loss: 6544.6792, Test Loss: 7161.9072\n",
      "Epoch 261, Training Loss: 7154.5537, Validation Loss: 6544.2061, Test Loss: 7161.4141\n",
      "Epoch 271, Training Loss: 7154.0479, Validation Loss: 6543.7246, Test Loss: 7160.9121\n",
      "Epoch 281, Training Loss: 7153.5283, Validation Loss: 6543.2310, Test Loss: 7160.3989\n",
      "Epoch 291, Training Loss: 7153.0015, Validation Loss: 6542.7280, Test Loss: 7159.8755\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 16, Epochs: 300\n",
      "Training loss: 7153.0015\n",
      "Validation loss: 6542.7280\n",
      "Test loss: 7159.8755\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 76/243 with parameters:\n",
      "Config indices: (0, 2, 2, 1, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.6040, Validation Loss: 6555.6978, Test Loss: 7173.4233\n",
      "Epoch 11, Training Loss: 7165.8091, Validation Loss: 6554.9253, Test Loss: 7172.6304\n",
      "Epoch 21, Training Loss: 7165.3716, Validation Loss: 6554.5044, Test Loss: 7172.1948\n",
      "Epoch 31, Training Loss: 7165.0195, Validation Loss: 6554.1689, Test Loss: 7171.8442\n",
      "Epoch 41, Training Loss: 7164.7095, Validation Loss: 6553.8735, Test Loss: 7171.5327\n",
      "Epoch 51, Training Loss: 7164.4204, Validation Loss: 6553.5977, Test Loss: 7171.2427\n",
      "Epoch 61, Training Loss: 7164.1362, Validation Loss: 6553.3306, Test Loss: 7170.9609\n",
      "Epoch 71, Training Loss: 7163.8701, Validation Loss: 6553.0762, Test Loss: 7170.6929\n",
      "Epoch 81, Training Loss: 7163.6113, Validation Loss: 6552.8306, Test Loss: 7170.4326\n",
      "Epoch 91, Training Loss: 7163.3579, Validation Loss: 6552.5903, Test Loss: 7170.1797\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 100\n",
      "Training loss: 7163.3579\n",
      "Validation loss: 6552.5903\n",
      "Test loss: 7170.1797\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 77/243 with parameters:\n",
      "Config indices: (0, 2, 2, 1, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7168.0996, Validation Loss: 6557.0312, Test Loss: 7174.9121\n",
      "Epoch 11, Training Loss: 7166.8979, Validation Loss: 6555.9243, Test Loss: 7173.7188\n",
      "Epoch 21, Training Loss: 7166.3159, Validation Loss: 6555.3799, Test Loss: 7173.1309\n",
      "Epoch 31, Training Loss: 7165.9019, Validation Loss: 6554.9941, Test Loss: 7172.7153\n",
      "Epoch 41, Training Loss: 7165.6118, Validation Loss: 6554.7168, Test Loss: 7172.4214\n",
      "Epoch 51, Training Loss: 7165.3813, Validation Loss: 6554.4980, Test Loss: 7172.1890\n",
      "Epoch 61, Training Loss: 7165.1689, Validation Loss: 6554.2954, Test Loss: 7171.9775\n",
      "Epoch 71, Training Loss: 7164.9697, Validation Loss: 6554.1025, Test Loss: 7171.7769\n",
      "Epoch 81, Training Loss: 7164.7754, Validation Loss: 6553.9160, Test Loss: 7171.5801\n",
      "Epoch 91, Training Loss: 7164.5806, Validation Loss: 6553.7290, Test Loss: 7171.3838\n",
      "Epoch 101, Training Loss: 7164.3809, Validation Loss: 6553.5400, Test Loss: 7171.1821\n",
      "Epoch 111, Training Loss: 7164.1504, Validation Loss: 6553.3262, Test Loss: 7170.9517\n",
      "Epoch 121, Training Loss: 7163.8921, Validation Loss: 6553.0825, Test Loss: 7170.6948\n",
      "Epoch 131, Training Loss: 7163.6533, Validation Loss: 6552.8564, Test Loss: 7170.4546\n",
      "Epoch 141, Training Loss: 7163.4268, Validation Loss: 6552.6382, Test Loss: 7170.2275\n",
      "Epoch 151, Training Loss: 7163.2056, Validation Loss: 6552.4272, Test Loss: 7170.0073\n",
      "Epoch 161, Training Loss: 7162.9917, Validation Loss: 6552.2212, Test Loss: 7169.7920\n",
      "Epoch 171, Training Loss: 7162.7817, Validation Loss: 6552.0205, Test Loss: 7169.5835\n",
      "Epoch 181, Training Loss: 7162.5771, Validation Loss: 6551.8228, Test Loss: 7169.3784\n",
      "Epoch 191, Training Loss: 7162.3765, Validation Loss: 6551.6299, Test Loss: 7169.1787\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 200\n",
      "Training loss: 7162.3765\n",
      "Validation loss: 6551.6299\n",
      "Test loss: 7169.1787\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 78/243 with parameters:\n",
      "Config indices: (0, 2, 2, 1, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.6348, Validation Loss: 6555.7188, Test Loss: 7173.4429\n",
      "Epoch 11, Training Loss: 7165.7778, Validation Loss: 6554.9204, Test Loss: 7172.5835\n",
      "Epoch 21, Training Loss: 7165.1948, Validation Loss: 6554.3701, Test Loss: 7172.0083\n",
      "Epoch 31, Training Loss: 7164.7095, Validation Loss: 6553.9023, Test Loss: 7171.5249\n",
      "Epoch 41, Training Loss: 7164.2769, Validation Loss: 6553.4868, Test Loss: 7171.0928\n",
      "Epoch 51, Training Loss: 7163.8799, Validation Loss: 6553.1060, Test Loss: 7170.6992\n",
      "Epoch 61, Training Loss: 7163.5132, Validation Loss: 6552.7529, Test Loss: 7170.3315\n",
      "Epoch 71, Training Loss: 7163.1655, Validation Loss: 6552.4209, Test Loss: 7169.9844\n",
      "Epoch 81, Training Loss: 7162.8330, Validation Loss: 6552.1040, Test Loss: 7169.6558\n",
      "Epoch 91, Training Loss: 7162.5137, Validation Loss: 6551.7988, Test Loss: 7169.3359\n",
      "Epoch 101, Training Loss: 7162.2021, Validation Loss: 6551.4995, Test Loss: 7169.0225\n",
      "Epoch 111, Training Loss: 7161.8945, Validation Loss: 6551.2046, Test Loss: 7168.7183\n",
      "Epoch 121, Training Loss: 7161.5928, Validation Loss: 6550.9160, Test Loss: 7168.4170\n",
      "Epoch 131, Training Loss: 7161.2969, Validation Loss: 6550.6318, Test Loss: 7168.1211\n",
      "Epoch 141, Training Loss: 7161.0015, Validation Loss: 6550.3501, Test Loss: 7167.8271\n",
      "Epoch 151, Training Loss: 7160.7085, Validation Loss: 6550.0723, Test Loss: 7167.5366\n",
      "Epoch 161, Training Loss: 7160.4204, Validation Loss: 6549.7944, Test Loss: 7167.2471\n",
      "Epoch 171, Training Loss: 7160.1309, Validation Loss: 6549.5205, Test Loss: 7166.9609\n",
      "Epoch 181, Training Loss: 7159.8418, Validation Loss: 6549.2485, Test Loss: 7166.6748\n",
      "Epoch 191, Training Loss: 7159.5625, Validation Loss: 6548.9780, Test Loss: 7166.3931\n",
      "Epoch 201, Training Loss: 7159.2793, Validation Loss: 6548.7090, Test Loss: 7166.1118\n",
      "Epoch 211, Training Loss: 7158.9966, Validation Loss: 6548.4414, Test Loss: 7165.8330\n",
      "Epoch 221, Training Loss: 7158.7158, Validation Loss: 6548.1743, Test Loss: 7165.5532\n",
      "Epoch 231, Training Loss: 7158.4351, Validation Loss: 6547.9067, Test Loss: 7165.2734\n",
      "Epoch 241, Training Loss: 7158.1543, Validation Loss: 6547.6411, Test Loss: 7164.9941\n",
      "Epoch 251, Training Loss: 7157.8735, Validation Loss: 6547.3735, Test Loss: 7164.7139\n",
      "Epoch 261, Training Loss: 7157.5903, Validation Loss: 6547.1055, Test Loss: 7164.4341\n",
      "Epoch 271, Training Loss: 7157.3086, Validation Loss: 6546.8369, Test Loss: 7164.1514\n",
      "Epoch 281, Training Loss: 7157.0195, Validation Loss: 6546.5635, Test Loss: 7163.8643\n",
      "Epoch 291, Training Loss: 7156.7246, Validation Loss: 6546.2837, Test Loss: 7163.5703\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 32, Epochs: 300\n",
      "Training loss: 7156.7246\n",
      "Validation loss: 6546.2837\n",
      "Test loss: 7163.5703\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 79/243 with parameters:\n",
      "Config indices: (0, 2, 2, 2, 0)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.6152, Validation Loss: 6555.6943, Test Loss: 7173.4370\n",
      "Epoch 11, Training Loss: 7166.0845, Validation Loss: 6555.1909, Test Loss: 7172.9058\n",
      "Epoch 21, Training Loss: 7165.7939, Validation Loss: 6554.9141, Test Loss: 7172.6162\n",
      "Epoch 31, Training Loss: 7165.5635, Validation Loss: 6554.6958, Test Loss: 7172.3887\n",
      "Epoch 41, Training Loss: 7165.3687, Validation Loss: 6554.5093, Test Loss: 7172.1919\n",
      "Epoch 51, Training Loss: 7165.1934, Validation Loss: 6554.3433, Test Loss: 7172.0171\n",
      "Epoch 61, Training Loss: 7165.0322, Validation Loss: 6554.1885, Test Loss: 7171.8555\n",
      "Epoch 71, Training Loss: 7164.8809, Validation Loss: 6554.0454, Test Loss: 7171.7056\n",
      "Epoch 81, Training Loss: 7164.7378, Validation Loss: 6553.9097, Test Loss: 7171.5625\n",
      "Epoch 91, Training Loss: 7164.6006, Validation Loss: 6553.7798, Test Loss: 7171.4272\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 100\n",
      "Training loss: 7164.6006\n",
      "Validation loss: 6553.7798\n",
      "Test loss: 7171.4272\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 80/243 with parameters:\n",
      "Config indices: (0, 2, 2, 2, 1)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.9043, Validation Loss: 6555.9731, Test Loss: 7173.7344\n",
      "Epoch 11, Training Loss: 7166.1714, Validation Loss: 6555.2852, Test Loss: 7172.9980\n",
      "Epoch 21, Training Loss: 7165.7583, Validation Loss: 6554.8960, Test Loss: 7172.5850\n",
      "Epoch 31, Training Loss: 7165.4365, Validation Loss: 6554.5933, Test Loss: 7172.2637\n",
      "Epoch 41, Training Loss: 7165.1655, Validation Loss: 6554.3384, Test Loss: 7171.9922\n",
      "Epoch 51, Training Loss: 7164.9263, Validation Loss: 6554.1152, Test Loss: 7171.7549\n",
      "Epoch 61, Training Loss: 7164.7109, Validation Loss: 6553.9102, Test Loss: 7171.5381\n",
      "Epoch 71, Training Loss: 7164.5146, Validation Loss: 6553.7251, Test Loss: 7171.3384\n",
      "Epoch 81, Training Loss: 7164.3281, Validation Loss: 6553.5522, Test Loss: 7171.1553\n",
      "Epoch 91, Training Loss: 7164.1528, Validation Loss: 6553.3872, Test Loss: 7170.9780\n",
      "Epoch 101, Training Loss: 7163.9854, Validation Loss: 6553.2275, Test Loss: 7170.8081\n",
      "Epoch 111, Training Loss: 7163.8213, Validation Loss: 6553.0747, Test Loss: 7170.6426\n",
      "Epoch 121, Training Loss: 7163.6621, Validation Loss: 6552.9253, Test Loss: 7170.4854\n",
      "Epoch 131, Training Loss: 7163.5083, Validation Loss: 6552.7803, Test Loss: 7170.3286\n",
      "Epoch 141, Training Loss: 7163.3540, Validation Loss: 6552.6382, Test Loss: 7170.1758\n",
      "Epoch 151, Training Loss: 7163.2046, Validation Loss: 6552.4971, Test Loss: 7170.0264\n",
      "Epoch 161, Training Loss: 7163.0552, Validation Loss: 6552.3589, Test Loss: 7169.8760\n",
      "Epoch 171, Training Loss: 7162.9058, Validation Loss: 6552.2202, Test Loss: 7169.7285\n",
      "Epoch 181, Training Loss: 7162.7612, Validation Loss: 6552.0835, Test Loss: 7169.5830\n",
      "Epoch 191, Training Loss: 7162.6167, Validation Loss: 6551.9478, Test Loss: 7169.4360\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 200\n",
      "Training loss: 7162.6167\n",
      "Validation loss: 6551.9478\n",
      "Test loss: 7169.4360\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 81/243 with parameters:\n",
      "Config indices: (0, 2, 2, 2, 2)\n",
      "{'hidden_sizes': [32, 16, 8], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.5762, Validation Loss: 6555.6313, Test Loss: 7173.3916\n",
      "Epoch 11, Training Loss: 7166.0171, Validation Loss: 6555.0938, Test Loss: 7172.8364\n",
      "Epoch 21, Training Loss: 7165.7012, Validation Loss: 6554.7910, Test Loss: 7172.5234\n",
      "Epoch 31, Training Loss: 7165.4385, Validation Loss: 6554.5396, Test Loss: 7172.2612\n",
      "Epoch 41, Training Loss: 7165.2095, Validation Loss: 6554.3228, Test Loss: 7172.0342\n",
      "Epoch 51, Training Loss: 7165.0049, Validation Loss: 6554.1265, Test Loss: 7171.8291\n",
      "Epoch 61, Training Loss: 7164.8101, Validation Loss: 6553.9443, Test Loss: 7171.6372\n",
      "Epoch 71, Training Loss: 7164.6313, Validation Loss: 6553.7720, Test Loss: 7171.4575\n",
      "Epoch 81, Training Loss: 7164.4604, Validation Loss: 6553.6089, Test Loss: 7171.2866\n",
      "Epoch 91, Training Loss: 7164.2979, Validation Loss: 6553.4536, Test Loss: 7171.1216\n",
      "Epoch 101, Training Loss: 7164.1411, Validation Loss: 6553.3027, Test Loss: 7170.9653\n",
      "Epoch 111, Training Loss: 7163.9902, Validation Loss: 6553.1582, Test Loss: 7170.8140\n",
      "Epoch 121, Training Loss: 7163.8403, Validation Loss: 6553.0176, Test Loss: 7170.6650\n",
      "Epoch 131, Training Loss: 7163.6958, Validation Loss: 6552.8809, Test Loss: 7170.5205\n",
      "Epoch 141, Training Loss: 7163.5552, Validation Loss: 6552.7466, Test Loss: 7170.3794\n",
      "Epoch 151, Training Loss: 7163.4170, Validation Loss: 6552.6143, Test Loss: 7170.2417\n",
      "Epoch 161, Training Loss: 7163.2808, Validation Loss: 6552.4844, Test Loss: 7170.1050\n",
      "Epoch 171, Training Loss: 7163.1470, Validation Loss: 6552.3574, Test Loss: 7169.9712\n",
      "Epoch 181, Training Loss: 7163.0122, Validation Loss: 6552.2310, Test Loss: 7169.8394\n",
      "Epoch 191, Training Loss: 7162.8813, Validation Loss: 6552.1064, Test Loss: 7169.7070\n",
      "Epoch 201, Training Loss: 7162.7549, Validation Loss: 6551.9854, Test Loss: 7169.5801\n",
      "Epoch 211, Training Loss: 7162.6274, Validation Loss: 6551.8628, Test Loss: 7169.4531\n",
      "Epoch 221, Training Loss: 7162.5034, Validation Loss: 6551.7437, Test Loss: 7169.3267\n",
      "Epoch 231, Training Loss: 7162.3760, Validation Loss: 6551.6245, Test Loss: 7169.2017\n",
      "Epoch 241, Training Loss: 7162.2515, Validation Loss: 6551.5068, Test Loss: 7169.0781\n",
      "Epoch 251, Training Loss: 7162.1299, Validation Loss: 6551.3892, Test Loss: 7168.9541\n",
      "Epoch 261, Training Loss: 7162.0063, Validation Loss: 6551.2734, Test Loss: 7168.8330\n",
      "Epoch 271, Training Loss: 7161.8857, Validation Loss: 6551.1577, Test Loss: 7168.7104\n",
      "Epoch 281, Training Loss: 7161.7637, Validation Loss: 6551.0425, Test Loss: 7168.5908\n",
      "Epoch 291, Training Loss: 7161.6431, Validation Loss: 6550.9287, Test Loss: 7168.4683\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [32, 16, 8], Batch size: 64, Epochs: 300\n",
      "Training loss: 7161.6431\n",
      "Validation loss: 6550.9287\n",
      "Test loss: 7168.4683\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 82/243 with parameters:\n",
      "Config indices: (1, 0, 0, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 854.4489, Validation Loss: 814.0302, Test Loss: 819.1223\n",
      "Epoch 11, Training Loss: 808.9902, Validation Loss: 819.1022, Test Loss: 769.9313\n",
      "Epoch 21, Training Loss: 809.1052, Validation Loss: 817.9811, Test Loss: 770.1372\n",
      "Epoch 31, Training Loss: 808.9838, Validation Loss: 819.1877, Test Loss: 769.9178\n",
      "Epoch 41, Training Loss: 808.9136, Validation Loss: 821.7284, Test Loss: 769.6548\n",
      "Epoch 51, Training Loss: 809.0026, Validation Loss: 818.9494, Test Loss: 769.9558\n",
      "Epoch 61, Training Loss: 809.1577, Validation Loss: 817.5946, Test Loss: 770.2225\n",
      "Epoch 71, Training Loss: 808.9797, Validation Loss: 823.3784, Test Loss: 769.6040\n",
      "Epoch 81, Training Loss: 808.9147, Validation Loss: 821.7925, Test Loss: 769.6513\n",
      "Epoch 91, Training Loss: 808.9149, Validation Loss: 821.8022, Test Loss: 769.6509\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 808.9149\n",
      "Validation loss: 821.8022\n",
      "Test loss: 769.6509\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 83/243 with parameters:\n",
      "Config indices: (1, 0, 0, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 84/243 with parameters:\n",
      "Config indices: (1, 0, 0, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 201, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 211, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 221, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 231, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 241, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 251, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 261, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 271, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 281, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 291, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 85/243 with parameters:\n",
      "Config indices: (1, 0, 0, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 86/243 with parameters:\n",
      "Config indices: (1, 0, 0, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 87/243 with parameters:\n",
      "Config indices: (1, 0, 0, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 3780.0090, Validation Loss: 3366.2729, Test Loss: 3772.2595\n",
      "Epoch 11, Training Loss: 808.9120, Validation Loss: 821.6102, Test Loss: 769.6618\n",
      "Epoch 21, Training Loss: 808.9528, Validation Loss: 819.6614, Test Loss: 769.8498\n",
      "Epoch 31, Training Loss: 808.9139, Validation Loss: 821.7456, Test Loss: 769.6541\n",
      "Epoch 41, Training Loss: 808.9142, Validation Loss: 821.7636, Test Loss: 769.6531\n",
      "Epoch 51, Training Loss: 808.9107, Validation Loss: 821.4899, Test Loss: 769.6696\n",
      "Epoch 61, Training Loss: 808.9485, Validation Loss: 822.8135, Test Loss: 769.6120\n",
      "Epoch 71, Training Loss: 808.9638, Validation Loss: 823.1122, Test Loss: 769.6068\n",
      "Epoch 81, Training Loss: 808.9178, Validation Loss: 821.9496, Test Loss: 769.6431\n",
      "Epoch 91, Training Loss: 808.9539, Validation Loss: 822.9250, Test Loss: 769.6097\n",
      "Epoch 101, Training Loss: 808.9199, Validation Loss: 820.4631, Test Loss: 769.7552\n",
      "Epoch 111, Training Loss: 808.9732, Validation Loss: 823.2729, Test Loss: 769.6049\n",
      "Epoch 121, Training Loss: 808.9106, Validation Loss: 821.0045, Test Loss: 769.7052\n",
      "Epoch 131, Training Loss: 808.9165, Validation Loss: 821.8847, Test Loss: 769.6464\n",
      "Epoch 141, Training Loss: 808.9136, Validation Loss: 820.7648, Test Loss: 769.7258\n",
      "Epoch 151, Training Loss: 808.9352, Validation Loss: 822.5132, Test Loss: 769.6202\n",
      "Epoch 161, Training Loss: 808.9410, Validation Loss: 819.8862, Test Loss: 769.8206\n",
      "Epoch 171, Training Loss: 808.9280, Validation Loss: 822.3083, Test Loss: 769.6273\n",
      "Epoch 181, Training Loss: 808.9118, Validation Loss: 820.8884, Test Loss: 769.7150\n",
      "Epoch 191, Training Loss: 809.0101, Validation Loss: 818.8649, Test Loss: 769.9700\n",
      "Epoch 201, Training Loss: 808.9253, Validation Loss: 820.2817, Test Loss: 769.7744\n",
      "Epoch 211, Training Loss: 808.9291, Validation Loss: 820.1705, Test Loss: 769.7866\n",
      "Epoch 221, Training Loss: 808.9186, Validation Loss: 821.9828, Test Loss: 769.6415\n",
      "Epoch 231, Training Loss: 808.9141, Validation Loss: 820.7334, Test Loss: 769.7288\n",
      "Epoch 241, Training Loss: 808.9222, Validation Loss: 822.1204, Test Loss: 769.6351\n",
      "Epoch 251, Training Loss: 809.0096, Validation Loss: 823.8072, Test Loss: 769.6042\n",
      "Epoch 261, Training Loss: 808.9104, Validation Loss: 821.4448, Test Loss: 769.6724\n",
      "Epoch 271, Training Loss: 808.9813, Validation Loss: 819.2220, Test Loss: 769.9126\n",
      "Epoch 281, Training Loss: 808.9605, Validation Loss: 823.0527, Test Loss: 769.6075\n",
      "Epoch 291, Training Loss: 808.9102, Validation Loss: 821.3969, Test Loss: 769.6757\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 808.9102\n",
      "Validation loss: 821.3969\n",
      "Test loss: 769.6757\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 88/243 with parameters:\n",
      "Config indices: (1, 0, 0, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 89/243 with parameters:\n",
      "Config indices: (1, 0, 0, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 90/243 with parameters:\n",
      "Config indices: (1, 0, 0, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 201, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 211, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 221, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 231, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 241, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 251, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 261, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 271, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 281, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 291, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 91/243 with parameters:\n",
      "Config indices: (1, 0, 1, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 479.1599, Validation Loss: 484.8188, Test Loss: 439.5541\n",
      "Epoch 11, Training Loss: 27.3853, Validation Loss: 23.1021, Test Loss: 25.6597\n",
      "Epoch 21, Training Loss: 17.7869, Validation Loss: 14.5342, Test Loss: 16.3428\n",
      "Epoch 31, Training Loss: 1.4843, Validation Loss: 1.0992, Test Loss: 1.4747\n",
      "Epoch 41, Training Loss: 3.1502, Validation Loss: 2.1648, Test Loss: 3.1258\n",
      "Epoch 51, Training Loss: 1.2952, Validation Loss: 1.0352, Test Loss: 1.2684\n",
      "Epoch 61, Training Loss: 0.8868, Validation Loss: 0.5346, Test Loss: 1.1251\n",
      "Epoch 71, Training Loss: 0.7785, Validation Loss: 0.4611, Test Loss: 0.9753\n",
      "Epoch 81, Training Loss: 1.2456, Validation Loss: 1.0495, Test Loss: 1.5131\n",
      "Epoch 91, Training Loss: 1.1084, Validation Loss: 0.9559, Test Loss: 1.3780\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 1.1084\n",
      "Validation loss: 0.9559\n",
      "Test loss: 1.3780\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 92/243 with parameters:\n",
      "Config indices: (1, 0, 1, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 385.4914, Validation Loss: 344.2382, Test Loss: 369.3503\n",
      "Epoch 11, Training Loss: 2.2739, Validation Loss: 1.6811, Test Loss: 2.9268\n",
      "Epoch 21, Training Loss: 1.1173, Validation Loss: 0.9229, Test Loss: 1.2151\n",
      "Epoch 31, Training Loss: 0.6501, Validation Loss: 0.4521, Test Loss: 0.8319\n",
      "Epoch 41, Training Loss: 0.5055, Validation Loss: 0.3038, Test Loss: 0.6717\n",
      "Epoch 51, Training Loss: 0.7681, Validation Loss: 0.6111, Test Loss: 0.9596\n",
      "Epoch 61, Training Loss: 0.8305, Validation Loss: 0.7208, Test Loss: 0.8755\n",
      "Epoch 71, Training Loss: 0.6379, Validation Loss: 0.4573, Test Loss: 0.7495\n",
      "Epoch 81, Training Loss: 21.7661, Validation Loss: 18.6530, Test Loss: 21.5029\n",
      "Epoch 91, Training Loss: 0.4043, Validation Loss: 0.2830, Test Loss: 0.5264\n",
      "Epoch 101, Training Loss: 0.5747, Validation Loss: 0.4595, Test Loss: 0.6054\n",
      "Epoch 111, Training Loss: 0.4205, Validation Loss: 0.2781, Test Loss: 0.5289\n",
      "Epoch 121, Training Loss: 0.9874, Validation Loss: 0.7807, Test Loss: 0.9891\n",
      "Epoch 131, Training Loss: 0.7691, Validation Loss: 0.7084, Test Loss: 0.9103\n",
      "Epoch 141, Training Loss: 0.3997, Validation Loss: 0.3218, Test Loss: 0.5078\n",
      "Epoch 151, Training Loss: 0.3555, Validation Loss: 0.2931, Test Loss: 0.4499\n",
      "Epoch 161, Training Loss: 1.2790, Validation Loss: 1.2864, Test Loss: 1.3982\n",
      "Epoch 171, Training Loss: 0.4110, Validation Loss: 0.3018, Test Loss: 0.4649\n",
      "Epoch 181, Training Loss: 0.4147, Validation Loss: 0.3636, Test Loss: 0.5185\n",
      "Epoch 191, Training Loss: 3.4335, Validation Loss: 2.9385, Test Loss: 3.3470\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 3.4335\n",
      "Validation loss: 2.9385\n",
      "Test loss: 3.3470\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 93/243 with parameters:\n",
      "Config indices: (1, 0, 1, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 312.2629, Validation Loss: 322.4391, Test Loss: 295.6548\n",
      "Epoch 11, Training Loss: 26.0614, Validation Loss: 20.8310, Test Loss: 23.9324\n",
      "Epoch 21, Training Loss: 48.0769, Validation Loss: 44.4141, Test Loss: 50.4310\n",
      "Epoch 31, Training Loss: 14.3544, Validation Loss: 11.6058, Test Loss: 14.0746\n",
      "Epoch 41, Training Loss: 36.6374, Validation Loss: 31.7453, Test Loss: 37.0167\n",
      "Epoch 51, Training Loss: 35.5241, Validation Loss: 32.5471, Test Loss: 35.8078\n",
      "Epoch 61, Training Loss: 20.7616, Validation Loss: 19.2604, Test Loss: 21.9349\n",
      "Epoch 71, Training Loss: 5.2978, Validation Loss: 4.3366, Test Loss: 5.2077\n",
      "Epoch 81, Training Loss: 3.3430, Validation Loss: 2.3496, Test Loss: 2.9466\n",
      "Epoch 91, Training Loss: 6.6981, Validation Loss: 6.1729, Test Loss: 6.6180\n",
      "Epoch 101, Training Loss: 15.4187, Validation Loss: 13.9247, Test Loss: 16.3888\n",
      "Epoch 111, Training Loss: 47.6903, Validation Loss: 44.1144, Test Loss: 49.2198\n",
      "Epoch 121, Training Loss: 6.0106, Validation Loss: 5.2851, Test Loss: 6.0324\n",
      "Epoch 131, Training Loss: 22.1648, Validation Loss: 21.7001, Test Loss: 22.0175\n",
      "Epoch 141, Training Loss: 2.2420, Validation Loss: 1.6418, Test Loss: 1.9584\n",
      "Epoch 151, Training Loss: 1.5918, Validation Loss: 0.9743, Test Loss: 1.5793\n",
      "Epoch 161, Training Loss: 25.3593, Validation Loss: 23.2431, Test Loss: 25.1118\n",
      "Epoch 171, Training Loss: 1.3273, Validation Loss: 0.9130, Test Loss: 1.2844\n",
      "Epoch 181, Training Loss: 1.8302, Validation Loss: 1.1036, Test Loss: 1.8179\n",
      "Epoch 191, Training Loss: 2.0803, Validation Loss: 1.5400, Test Loss: 2.0473\n",
      "Epoch 201, Training Loss: 7.4989, Validation Loss: 5.2027, Test Loss: 7.9552\n",
      "Epoch 211, Training Loss: 34.8688, Validation Loss: 30.5598, Test Loss: 34.9056\n",
      "Epoch 221, Training Loss: 7.4297, Validation Loss: 6.8729, Test Loss: 7.8433\n",
      "Epoch 231, Training Loss: 74.1510, Validation Loss: 68.2376, Test Loss: 74.0296\n",
      "Epoch 241, Training Loss: 18.4489, Validation Loss: 17.8937, Test Loss: 19.0699\n",
      "Epoch 251, Training Loss: 3.7075, Validation Loss: 3.2419, Test Loss: 3.8021\n",
      "Epoch 261, Training Loss: 2.6055, Validation Loss: 2.2815, Test Loss: 2.8483\n",
      "Epoch 271, Training Loss: 2.3215, Validation Loss: 2.0843, Test Loss: 2.5293\n",
      "Epoch 281, Training Loss: 0.8939, Validation Loss: 0.6276, Test Loss: 0.9922\n",
      "Epoch 291, Training Loss: 0.7394, Validation Loss: 0.4936, Test Loss: 0.9215\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.7394\n",
      "Validation loss: 0.4936\n",
      "Test loss: 0.9215\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 94/243 with parameters:\n",
      "Config indices: (1, 0, 1, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6095.3354, Validation Loss: 5543.2100, Test Loss: 6098.3975\n",
      "Epoch 11, Training Loss: 66.1337, Validation Loss: 57.7721, Test Loss: 65.6856\n",
      "Epoch 21, Training Loss: 13.1082, Validation Loss: 12.2180, Test Loss: 13.7521\n",
      "Epoch 31, Training Loss: 2.3577, Validation Loss: 2.1244, Test Loss: 2.4819\n",
      "Epoch 41, Training Loss: 4.0647, Validation Loss: 3.7980, Test Loss: 3.8222\n",
      "Epoch 51, Training Loss: 3.2833, Validation Loss: 3.0423, Test Loss: 3.5265\n",
      "Epoch 61, Training Loss: 9.1956, Validation Loss: 8.6581, Test Loss: 8.6669\n",
      "Epoch 71, Training Loss: 1.7808, Validation Loss: 1.3808, Test Loss: 1.9241\n",
      "Epoch 81, Training Loss: 8.3113, Validation Loss: 7.5774, Test Loss: 8.6223\n",
      "Epoch 91, Training Loss: 0.9176, Validation Loss: 0.6644, Test Loss: 0.9703\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.9176\n",
      "Validation loss: 0.6644\n",
      "Test loss: 0.9703\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 95/243 with parameters:\n",
      "Config indices: (1, 0, 1, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 2776.7471, Validation Loss: 2541.4851, Test Loss: 2714.7292\n",
      "Epoch 11, Training Loss: 3.3576, Validation Loss: 2.5299, Test Loss: 3.2400\n",
      "Epoch 21, Training Loss: 55.8609, Validation Loss: 49.0458, Test Loss: 55.2890\n",
      "Epoch 31, Training Loss: 22.7679, Validation Loss: 20.6828, Test Loss: 21.7616\n",
      "Epoch 41, Training Loss: 30.8406, Validation Loss: 27.5205, Test Loss: 31.3322\n",
      "Epoch 51, Training Loss: 17.7760, Validation Loss: 16.0666, Test Loss: 17.3058\n",
      "Epoch 61, Training Loss: 4.4582, Validation Loss: 4.2775, Test Loss: 4.7092\n",
      "Epoch 71, Training Loss: 1.9424, Validation Loss: 1.5381, Test Loss: 2.0238\n",
      "Epoch 81, Training Loss: 1.4139, Validation Loss: 1.0950, Test Loss: 1.4964\n",
      "Epoch 91, Training Loss: 0.9314, Validation Loss: 0.6749, Test Loss: 1.0311\n",
      "Epoch 101, Training Loss: 1.8076, Validation Loss: 1.5171, Test Loss: 1.9840\n",
      "Epoch 111, Training Loss: 9.5699, Validation Loss: 8.3694, Test Loss: 9.5915\n",
      "Epoch 121, Training Loss: 2.3758, Validation Loss: 2.0721, Test Loss: 2.4843\n",
      "Epoch 131, Training Loss: 2.4609, Validation Loss: 2.3296, Test Loss: 2.6455\n",
      "Epoch 141, Training Loss: 12.7453, Validation Loss: 11.4511, Test Loss: 12.9668\n",
      "Epoch 151, Training Loss: 1.8630, Validation Loss: 1.6169, Test Loss: 2.0786\n",
      "Epoch 161, Training Loss: 0.7233, Validation Loss: 0.6116, Test Loss: 0.7708\n",
      "Epoch 171, Training Loss: 1.9945, Validation Loss: 1.7451, Test Loss: 2.2303\n",
      "Epoch 181, Training Loss: 2.1368, Validation Loss: 1.9852, Test Loss: 2.1895\n",
      "Epoch 191, Training Loss: 17.4168, Validation Loss: 16.5161, Test Loss: 18.0485\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 17.4168\n",
      "Validation loss: 16.5161\n",
      "Test loss: 18.0485\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 96/243 with parameters:\n",
      "Config indices: (1, 0, 1, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6340.0615, Validation Loss: 5771.0566, Test Loss: 6343.7803\n",
      "Epoch 11, Training Loss: 1969.2476, Validation Loss: 1715.3148, Test Loss: 1949.6938\n",
      "Epoch 21, Training Loss: 1052.5970, Validation Loss: 942.9061, Test Loss: 1022.3876\n",
      "Epoch 31, Training Loss: 860.2423, Validation Loss: 816.5690, Test Loss: 825.1563\n",
      "Epoch 41, Training Loss: 819.4841, Validation Loss: 806.3960, Test Loss: 782.1387\n",
      "Epoch 51, Training Loss: 811.0347, Validation Loss: 811.9706, Test Loss: 772.6535\n",
      "Epoch 61, Training Loss: 809.3612, Validation Loss: 816.4395, Test Loss: 770.5262\n",
      "Epoch 71, Training Loss: 809.0087, Validation Loss: 818.8815, Test Loss: 769.9672\n",
      "Epoch 81, Training Loss: 808.9310, Validation Loss: 820.1241, Test Loss: 769.7921\n",
      "Epoch 91, Training Loss: 808.9159, Validation Loss: 820.6302, Test Loss: 769.7385\n",
      "Epoch 101, Training Loss: 808.9109, Validation Loss: 821.0029, Test Loss: 769.7054\n",
      "Epoch 111, Training Loss: 808.9104, Validation Loss: 821.0315, Test Loss: 769.7031\n",
      "Epoch 121, Training Loss: 808.9098, Validation Loss: 821.2089, Test Loss: 769.6891\n",
      "Epoch 131, Training Loss: 808.9109, Validation Loss: 820.9760, Test Loss: 769.7077\n",
      "Epoch 141, Training Loss: 808.9098, Validation Loss: 821.1873, Test Loss: 769.6908\n",
      "Epoch 151, Training Loss: 808.9099, Validation Loss: 821.1998, Test Loss: 769.6898\n",
      "Epoch 161, Training Loss: 808.9097, Validation Loss: 821.2257, Test Loss: 769.6879\n",
      "Epoch 171, Training Loss: 808.9099, Validation Loss: 821.2687, Test Loss: 769.6848\n",
      "Epoch 181, Training Loss: 808.9099, Validation Loss: 821.1516, Test Loss: 769.6934\n",
      "Epoch 191, Training Loss: 808.9100, Validation Loss: 821.1414, Test Loss: 769.6943\n",
      "Epoch 201, Training Loss: 808.9100, Validation Loss: 821.3453, Test Loss: 769.6793\n",
      "Epoch 211, Training Loss: 808.9108, Validation Loss: 821.4966, Test Loss: 769.6692\n",
      "Epoch 221, Training Loss: 808.9105, Validation Loss: 821.4351, Test Loss: 769.6731\n",
      "Epoch 231, Training Loss: 808.9119, Validation Loss: 821.6046, Test Loss: 769.6622\n",
      "Epoch 241, Training Loss: 808.9101, Validation Loss: 821.3598, Test Loss: 769.6782\n",
      "Epoch 251, Training Loss: 808.9103, Validation Loss: 821.4575, Test Loss: 769.6716\n",
      "Epoch 261, Training Loss: 808.9106, Validation Loss: 821.4578, Test Loss: 769.6716\n",
      "Epoch 271, Training Loss: 808.9114, Validation Loss: 821.5771, Test Loss: 769.6639\n",
      "Epoch 281, Training Loss: 808.9103, Validation Loss: 821.0524, Test Loss: 769.7014\n",
      "Epoch 291, Training Loss: 808.9099, Validation Loss: 821.2255, Test Loss: 769.6879\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 808.9099\n",
      "Validation loss: 821.2255\n",
      "Test loss: 769.6879\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 97/243 with parameters:\n",
      "Config indices: (1, 0, 1, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 5831.5649, Validation Loss: 5292.1226, Test Loss: 5834.0283\n",
      "Epoch 11, Training Loss: 61.3023, Validation Loss: 54.6046, Test Loss: 58.3353\n",
      "Epoch 21, Training Loss: 20.5081, Validation Loss: 15.9403, Test Loss: 18.9570\n",
      "Epoch 31, Training Loss: 19.0751, Validation Loss: 15.8154, Test Loss: 18.4171\n",
      "Epoch 41, Training Loss: 2.4324, Validation Loss: 1.7188, Test Loss: 2.5720\n",
      "Epoch 51, Training Loss: 4.0627, Validation Loss: 3.4245, Test Loss: 4.3726\n",
      "Epoch 61, Training Loss: 4.9606, Validation Loss: 3.8245, Test Loss: 5.1770\n",
      "Epoch 71, Training Loss: 4.9848, Validation Loss: 4.1325, Test Loss: 5.4540\n",
      "Epoch 81, Training Loss: 1.8644, Validation Loss: 1.4785, Test Loss: 2.1498\n",
      "Epoch 91, Training Loss: 5.6437, Validation Loss: 5.7287, Test Loss: 5.5029\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 5.6437\n",
      "Validation loss: 5.7287\n",
      "Test loss: 5.5029\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 98/243 with parameters:\n",
      "Config indices: (1, 0, 1, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6327.3281, Validation Loss: 5762.8618, Test Loss: 6331.7783\n",
      "Epoch 11, Training Loss: 347.0240, Validation Loss: 282.4293, Test Loss: 343.7878\n",
      "Epoch 21, Training Loss: 136.1182, Validation Loss: 121.9720, Test Loss: 134.0691\n",
      "Epoch 31, Training Loss: 94.5490, Validation Loss: 87.2126, Test Loss: 97.3947\n",
      "Epoch 41, Training Loss: 41.4537, Validation Loss: 38.1067, Test Loss: 42.9831\n",
      "Epoch 51, Training Loss: 10.4161, Validation Loss: 8.4349, Test Loss: 11.5160\n",
      "Epoch 61, Training Loss: 7.7530, Validation Loss: 6.4772, Test Loss: 7.9050\n",
      "Epoch 71, Training Loss: 18.2303, Validation Loss: 15.2321, Test Loss: 19.1320\n",
      "Epoch 81, Training Loss: 11.7564, Validation Loss: 10.0497, Test Loss: 12.5716\n",
      "Epoch 91, Training Loss: 5.2835, Validation Loss: 4.4694, Test Loss: 5.8526\n",
      "Epoch 101, Training Loss: 10.6865, Validation Loss: 8.7031, Test Loss: 10.5593\n",
      "Epoch 111, Training Loss: 10.5606, Validation Loss: 8.9075, Test Loss: 11.1549\n",
      "Epoch 121, Training Loss: 4.7869, Validation Loss: 4.0039, Test Loss: 4.6456\n",
      "Epoch 131, Training Loss: 3.3015, Validation Loss: 2.9814, Test Loss: 3.7785\n",
      "Epoch 141, Training Loss: 4.7713, Validation Loss: 3.7776, Test Loss: 4.8165\n",
      "Epoch 151, Training Loss: 0.6595, Validation Loss: 0.4060, Test Loss: 0.8048\n",
      "Epoch 161, Training Loss: 4.3937, Validation Loss: 3.5131, Test Loss: 4.2397\n",
      "Epoch 171, Training Loss: 0.6055, Validation Loss: 0.3962, Test Loss: 0.7732\n",
      "Epoch 181, Training Loss: 0.6274, Validation Loss: 0.4793, Test Loss: 0.7989\n",
      "Epoch 191, Training Loss: 7.6131, Validation Loss: 6.8283, Test Loss: 8.2854\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 7.6131\n",
      "Validation loss: 6.8283\n",
      "Test loss: 8.2854\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 99/243 with parameters:\n",
      "Config indices: (1, 0, 1, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 5751.1206, Validation Loss: 5221.0649, Test Loss: 5754.6040\n",
      "Epoch 11, Training Loss: 17.2991, Validation Loss: 18.1187, Test Loss: 17.0609\n",
      "Epoch 21, Training Loss: 35.6466, Validation Loss: 30.0144, Test Loss: 34.7704\n",
      "Epoch 31, Training Loss: 27.6876, Validation Loss: 25.9023, Test Loss: 27.1182\n",
      "Epoch 41, Training Loss: 11.7086, Validation Loss: 11.5445, Test Loss: 12.1327\n",
      "Epoch 51, Training Loss: 8.5814, Validation Loss: 8.0901, Test Loss: 8.7240\n",
      "Epoch 61, Training Loss: 30.0254, Validation Loss: 27.0038, Test Loss: 29.3730\n",
      "Epoch 71, Training Loss: 10.5832, Validation Loss: 9.9811, Test Loss: 11.1649\n",
      "Epoch 81, Training Loss: 2.4203, Validation Loss: 2.1389, Test Loss: 2.3919\n",
      "Epoch 91, Training Loss: 2.9314, Validation Loss: 2.8142, Test Loss: 2.8693\n",
      "Epoch 101, Training Loss: 3.4075, Validation Loss: 3.0162, Test Loss: 3.6485\n",
      "Epoch 111, Training Loss: 0.8372, Validation Loss: 0.7163, Test Loss: 0.8278\n",
      "Epoch 121, Training Loss: 0.5809, Validation Loss: 0.4494, Test Loss: 0.6305\n",
      "Epoch 131, Training Loss: 2.2015, Validation Loss: 1.8976, Test Loss: 2.1279\n",
      "Epoch 141, Training Loss: 3.1353, Validation Loss: 3.0025, Test Loss: 3.3151\n",
      "Epoch 151, Training Loss: 0.5864, Validation Loss: 0.4726, Test Loss: 0.6588\n",
      "Epoch 161, Training Loss: 2.3539, Validation Loss: 2.0142, Test Loss: 2.6003\n",
      "Epoch 171, Training Loss: 2.2608, Validation Loss: 1.9827, Test Loss: 2.5246\n",
      "Epoch 181, Training Loss: 1.5964, Validation Loss: 1.3631, Test Loss: 1.5465\n",
      "Epoch 191, Training Loss: 0.4552, Validation Loss: 0.3041, Test Loss: 0.5393\n",
      "Epoch 201, Training Loss: 0.4546, Validation Loss: 0.3554, Test Loss: 0.5362\n",
      "Epoch 211, Training Loss: 0.5116, Validation Loss: 0.4065, Test Loss: 0.6276\n",
      "Epoch 221, Training Loss: 2.0436, Validation Loss: 1.7659, Test Loss: 2.2864\n",
      "Epoch 231, Training Loss: 1.3512, Validation Loss: 1.1763, Test Loss: 1.5634\n",
      "Epoch 241, Training Loss: 1.1267, Validation Loss: 0.9823, Test Loss: 1.1009\n",
      "Epoch 251, Training Loss: 0.4484, Validation Loss: 0.3568, Test Loss: 0.5710\n",
      "Epoch 261, Training Loss: 0.4371, Validation Loss: 0.3291, Test Loss: 0.4897\n",
      "Epoch 271, Training Loss: 4.2337, Validation Loss: 4.0174, Test Loss: 4.0973\n",
      "Epoch 281, Training Loss: 0.8009, Validation Loss: 0.6914, Test Loss: 0.9703\n",
      "Epoch 291, Training Loss: 0.3898, Validation Loss: 0.2975, Test Loss: 0.4574\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.3898\n",
      "Validation loss: 0.2975\n",
      "Test loss: 0.4574\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 100/243 with parameters:\n",
      "Config indices: (1, 0, 2, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6965.4482, Validation Loss: 6364.4902, Test Loss: 6971.5327\n",
      "Epoch 11, Training Loss: 1.0787, Validation Loss: 0.7733, Test Loss: 1.1027\n",
      "Epoch 21, Training Loss: 0.6439, Validation Loss: 0.4662, Test Loss: 0.7574\n",
      "Epoch 31, Training Loss: 0.5745, Validation Loss: 0.3934, Test Loss: 0.6965\n",
      "Epoch 41, Training Loss: 0.5289, Validation Loss: 0.3669, Test Loss: 0.6276\n",
      "Epoch 51, Training Loss: 0.4708, Validation Loss: 0.3161, Test Loss: 0.5945\n",
      "Epoch 61, Training Loss: 0.4502, Validation Loss: 0.2863, Test Loss: 0.5793\n",
      "Epoch 71, Training Loss: 0.4767, Validation Loss: 0.3370, Test Loss: 0.5747\n",
      "Epoch 81, Training Loss: 0.4321, Validation Loss: 0.2879, Test Loss: 0.5364\n",
      "Epoch 91, Training Loss: 0.4029, Validation Loss: 0.2606, Test Loss: 0.5206\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.4029\n",
      "Validation loss: 0.2606\n",
      "Test loss: 0.5206\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 101/243 with parameters:\n",
      "Config indices: (1, 0, 2, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6900.7388, Validation Loss: 6302.7925, Test Loss: 6906.6890\n",
      "Epoch 11, Training Loss: 1.0086, Validation Loss: 0.7037, Test Loss: 1.0838\n",
      "Epoch 21, Training Loss: 0.6455, Validation Loss: 0.4436, Test Loss: 0.7407\n",
      "Epoch 31, Training Loss: 0.5291, Validation Loss: 0.3668, Test Loss: 0.6029\n",
      "Epoch 41, Training Loss: 0.4832, Validation Loss: 0.3275, Test Loss: 0.5920\n",
      "Epoch 51, Training Loss: 0.4812, Validation Loss: 0.3365, Test Loss: 0.5573\n",
      "Epoch 61, Training Loss: 0.4594, Validation Loss: 0.3180, Test Loss: 0.5889\n",
      "Epoch 71, Training Loss: 0.4603, Validation Loss: 0.3229, Test Loss: 0.5366\n",
      "Epoch 81, Training Loss: 0.4145, Validation Loss: 0.2952, Test Loss: 0.5222\n",
      "Epoch 91, Training Loss: 0.3998, Validation Loss: 0.2746, Test Loss: 0.4947\n",
      "Epoch 101, Training Loss: 0.3815, Validation Loss: 0.2633, Test Loss: 0.4957\n",
      "Epoch 111, Training Loss: 0.3712, Validation Loss: 0.2581, Test Loss: 0.4856\n",
      "Epoch 121, Training Loss: 0.3659, Validation Loss: 0.2538, Test Loss: 0.4847\n",
      "Epoch 131, Training Loss: 0.3626, Validation Loss: 0.2475, Test Loss: 0.4790\n",
      "Epoch 141, Training Loss: 0.3454, Validation Loss: 0.2440, Test Loss: 0.4619\n",
      "Epoch 151, Training Loss: 0.3502, Validation Loss: 0.2480, Test Loss: 0.4746\n",
      "Epoch 161, Training Loss: 0.3581, Validation Loss: 0.2506, Test Loss: 0.4413\n",
      "Epoch 171, Training Loss: 0.3352, Validation Loss: 0.2383, Test Loss: 0.4580\n",
      "Epoch 181, Training Loss: 0.3408, Validation Loss: 0.2355, Test Loss: 0.4179\n",
      "Epoch 191, Training Loss: 0.3074, Validation Loss: 0.2095, Test Loss: 0.4103\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.3074\n",
      "Validation loss: 0.2095\n",
      "Test loss: 0.4103\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 102/243 with parameters:\n",
      "Config indices: (1, 0, 2, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6930.0684, Validation Loss: 6331.0044, Test Loss: 6936.0459\n",
      "Epoch 11, Training Loss: 1.0357, Validation Loss: 0.6862, Test Loss: 1.0047\n",
      "Epoch 21, Training Loss: 0.6567, Validation Loss: 0.4163, Test Loss: 0.7195\n",
      "Epoch 31, Training Loss: 0.5741, Validation Loss: 0.3703, Test Loss: 0.6292\n",
      "Epoch 41, Training Loss: 0.6226, Validation Loss: 0.4180, Test Loss: 0.7192\n",
      "Epoch 51, Training Loss: 0.5675, Validation Loss: 0.3894, Test Loss: 0.6736\n",
      "Epoch 61, Training Loss: 0.5301, Validation Loss: 0.3520, Test Loss: 0.6281\n",
      "Epoch 71, Training Loss: 0.4798, Validation Loss: 0.3148, Test Loss: 0.5527\n",
      "Epoch 81, Training Loss: 0.4617, Validation Loss: 0.2967, Test Loss: 0.5406\n",
      "Epoch 91, Training Loss: 0.4451, Validation Loss: 0.2870, Test Loss: 0.5318\n",
      "Epoch 101, Training Loss: 0.4214, Validation Loss: 0.2807, Test Loss: 0.5007\n",
      "Epoch 111, Training Loss: 0.4922, Validation Loss: 0.3400, Test Loss: 0.5337\n",
      "Epoch 121, Training Loss: 0.3979, Validation Loss: 0.2546, Test Loss: 0.4751\n",
      "Epoch 131, Training Loss: 0.4567, Validation Loss: 0.3227, Test Loss: 0.5787\n",
      "Epoch 141, Training Loss: 0.3782, Validation Loss: 0.2399, Test Loss: 0.4524\n",
      "Epoch 151, Training Loss: 0.3649, Validation Loss: 0.2365, Test Loss: 0.4518\n",
      "Epoch 161, Training Loss: 0.3605, Validation Loss: 0.2325, Test Loss: 0.4471\n",
      "Epoch 171, Training Loss: 0.3584, Validation Loss: 0.2335, Test Loss: 0.4511\n",
      "Epoch 181, Training Loss: 0.3375, Validation Loss: 0.2237, Test Loss: 0.4202\n",
      "Epoch 191, Training Loss: 0.3353, Validation Loss: 0.2248, Test Loss: 0.4182\n",
      "Epoch 201, Training Loss: 0.3248, Validation Loss: 0.2104, Test Loss: 0.4051\n",
      "Epoch 211, Training Loss: 0.3238, Validation Loss: 0.2196, Test Loss: 0.3968\n",
      "Epoch 221, Training Loss: 0.3132, Validation Loss: 0.2075, Test Loss: 0.3910\n",
      "Epoch 231, Training Loss: 0.3115, Validation Loss: 0.2110, Test Loss: 0.4047\n",
      "Epoch 241, Training Loss: 0.3000, Validation Loss: 0.2088, Test Loss: 0.3883\n",
      "Epoch 251, Training Loss: 0.2934, Validation Loss: 0.2022, Test Loss: 0.3817\n",
      "Epoch 261, Training Loss: 0.2874, Validation Loss: 0.1953, Test Loss: 0.3727\n",
      "Epoch 271, Training Loss: 0.2819, Validation Loss: 0.1934, Test Loss: 0.3680\n",
      "Epoch 281, Training Loss: 0.2778, Validation Loss: 0.1904, Test Loss: 0.3643\n",
      "Epoch 291, Training Loss: 0.2776, Validation Loss: 0.1869, Test Loss: 0.3573\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2776\n",
      "Validation loss: 0.1869\n",
      "Test loss: 0.3573\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 103/243 with parameters:\n",
      "Config indices: (1, 0, 2, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7066.7832, Validation Loss: 6460.7593, Test Loss: 7073.2300\n",
      "Epoch 11, Training Loss: 2.6823, Validation Loss: 2.0355, Test Loss: 2.4977\n",
      "Epoch 21, Training Loss: 0.9058, Validation Loss: 0.5474, Test Loss: 1.0333\n",
      "Epoch 31, Training Loss: 0.7437, Validation Loss: 0.4411, Test Loss: 0.8956\n",
      "Epoch 41, Training Loss: 0.6619, Validation Loss: 0.3935, Test Loss: 0.8076\n",
      "Epoch 51, Training Loss: 0.6291, Validation Loss: 0.3881, Test Loss: 0.7423\n",
      "Epoch 61, Training Loss: 0.5878, Validation Loss: 0.3746, Test Loss: 0.7110\n",
      "Epoch 71, Training Loss: 0.5560, Validation Loss: 0.3489, Test Loss: 0.6912\n",
      "Epoch 81, Training Loss: 0.5571, Validation Loss: 0.3683, Test Loss: 0.6729\n",
      "Epoch 91, Training Loss: 0.5274, Validation Loss: 0.3388, Test Loss: 0.6703\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.5274\n",
      "Validation loss: 0.3388\n",
      "Test loss: 0.6703\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 104/243 with parameters:\n",
      "Config indices: (1, 0, 2, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7046.8979, Validation Loss: 6441.9463, Test Loss: 7053.2832\n",
      "Epoch 11, Training Loss: 1.9646, Validation Loss: 1.5420, Test Loss: 1.6074\n",
      "Epoch 21, Training Loss: 0.8450, Validation Loss: 0.6472, Test Loss: 0.8480\n",
      "Epoch 31, Training Loss: 0.7215, Validation Loss: 0.5592, Test Loss: 0.7561\n",
      "Epoch 41, Training Loss: 0.6689, Validation Loss: 0.5025, Test Loss: 0.7576\n",
      "Epoch 51, Training Loss: 0.6095, Validation Loss: 0.4679, Test Loss: 0.6778\n",
      "Epoch 61, Training Loss: 0.6111, Validation Loss: 0.4544, Test Loss: 0.7224\n",
      "Epoch 71, Training Loss: 0.5496, Validation Loss: 0.4053, Test Loss: 0.6421\n",
      "Epoch 81, Training Loss: 0.5664, Validation Loss: 0.4135, Test Loss: 0.6862\n",
      "Epoch 91, Training Loss: 0.4988, Validation Loss: 0.3603, Test Loss: 0.5906\n",
      "Epoch 101, Training Loss: 0.4860, Validation Loss: 0.3401, Test Loss: 0.5796\n",
      "Epoch 111, Training Loss: 0.4677, Validation Loss: 0.3333, Test Loss: 0.5686\n",
      "Epoch 121, Training Loss: 0.4696, Validation Loss: 0.3313, Test Loss: 0.5491\n",
      "Epoch 131, Training Loss: 0.4498, Validation Loss: 0.3109, Test Loss: 0.5427\n",
      "Epoch 141, Training Loss: 0.4370, Validation Loss: 0.2996, Test Loss: 0.5447\n",
      "Epoch 151, Training Loss: 0.4275, Validation Loss: 0.2975, Test Loss: 0.5329\n",
      "Epoch 161, Training Loss: 0.4167, Validation Loss: 0.2830, Test Loss: 0.5140\n",
      "Epoch 171, Training Loss: 0.4156, Validation Loss: 0.2820, Test Loss: 0.5018\n",
      "Epoch 181, Training Loss: 0.4067, Validation Loss: 0.2741, Test Loss: 0.5148\n",
      "Epoch 191, Training Loss: 0.4177, Validation Loss: 0.2808, Test Loss: 0.5309\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.4177\n",
      "Validation loss: 0.2808\n",
      "Test loss: 0.5309\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 105/243 with parameters:\n",
      "Config indices: (1, 0, 2, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7062.6089, Validation Loss: 6456.8105, Test Loss: 7069.0498\n",
      "Epoch 11, Training Loss: 2.8358, Validation Loss: 2.2252, Test Loss: 2.6327\n",
      "Epoch 21, Training Loss: 1.1923, Validation Loss: 0.8213, Test Loss: 1.2509\n",
      "Epoch 31, Training Loss: 0.8269, Validation Loss: 0.5364, Test Loss: 0.9350\n",
      "Epoch 41, Training Loss: 0.7904, Validation Loss: 0.5401, Test Loss: 0.8502\n",
      "Epoch 51, Training Loss: 0.6554, Validation Loss: 0.4048, Test Loss: 0.7894\n",
      "Epoch 61, Training Loss: 0.6144, Validation Loss: 0.4020, Test Loss: 0.7312\n",
      "Epoch 71, Training Loss: 0.6513, Validation Loss: 0.4183, Test Loss: 0.8151\n",
      "Epoch 81, Training Loss: 0.5717, Validation Loss: 0.3708, Test Loss: 0.6903\n",
      "Epoch 91, Training Loss: 0.5520, Validation Loss: 0.3556, Test Loss: 0.6770\n",
      "Epoch 101, Training Loss: 0.5398, Validation Loss: 0.3475, Test Loss: 0.6777\n",
      "Epoch 111, Training Loss: 0.5383, Validation Loss: 0.3572, Test Loss: 0.6551\n",
      "Epoch 121, Training Loss: 0.5461, Validation Loss: 0.3628, Test Loss: 0.6494\n",
      "Epoch 131, Training Loss: 0.5104, Validation Loss: 0.3291, Test Loss: 0.6508\n",
      "Epoch 141, Training Loss: 0.5136, Validation Loss: 0.3293, Test Loss: 0.6612\n",
      "Epoch 151, Training Loss: 0.5083, Validation Loss: 0.3251, Test Loss: 0.6555\n",
      "Epoch 161, Training Loss: 0.4833, Validation Loss: 0.3153, Test Loss: 0.5991\n",
      "Epoch 171, Training Loss: 0.4761, Validation Loss: 0.3155, Test Loss: 0.5982\n",
      "Epoch 181, Training Loss: 0.4644, Validation Loss: 0.3000, Test Loss: 0.5914\n",
      "Epoch 191, Training Loss: 0.4582, Validation Loss: 0.2955, Test Loss: 0.5755\n",
      "Epoch 201, Training Loss: 0.4638, Validation Loss: 0.2965, Test Loss: 0.6039\n",
      "Epoch 211, Training Loss: 0.4432, Validation Loss: 0.2869, Test Loss: 0.5716\n",
      "Epoch 221, Training Loss: 0.4374, Validation Loss: 0.2839, Test Loss: 0.5455\n",
      "Epoch 231, Training Loss: 0.4284, Validation Loss: 0.2798, Test Loss: 0.5394\n",
      "Epoch 241, Training Loss: 0.4191, Validation Loss: 0.2686, Test Loss: 0.5327\n",
      "Epoch 251, Training Loss: 0.4135, Validation Loss: 0.2652, Test Loss: 0.5248\n",
      "Epoch 261, Training Loss: 0.4054, Validation Loss: 0.2624, Test Loss: 0.5202\n",
      "Epoch 271, Training Loss: 0.4042, Validation Loss: 0.2726, Test Loss: 0.5214\n",
      "Epoch 281, Training Loss: 0.4011, Validation Loss: 0.2562, Test Loss: 0.5024\n",
      "Epoch 291, Training Loss: 0.4135, Validation Loss: 0.2802, Test Loss: 0.5073\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.4135\n",
      "Validation loss: 0.2802\n",
      "Test loss: 0.5073\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 106/243 with parameters:\n",
      "Config indices: (1, 0, 2, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7114.8311, Validation Loss: 6506.4224, Test Loss: 7121.4609\n",
      "Epoch 11, Training Loss: 221.6823, Validation Loss: 213.3985, Test Loss: 227.2802\n",
      "Epoch 21, Training Loss: 3.1837, Validation Loss: 2.7999, Test Loss: 2.6704\n",
      "Epoch 31, Training Loss: 1.7563, Validation Loss: 1.5344, Test Loss: 1.6365\n",
      "Epoch 41, Training Loss: 1.3180, Validation Loss: 1.0769, Test Loss: 1.3256\n",
      "Epoch 51, Training Loss: 1.0579, Validation Loss: 0.8409, Test Loss: 1.0959\n",
      "Epoch 61, Training Loss: 0.9523, Validation Loss: 0.6770, Test Loss: 1.1003\n",
      "Epoch 71, Training Loss: 0.8440, Validation Loss: 0.6488, Test Loss: 0.9288\n",
      "Epoch 81, Training Loss: 0.7603, Validation Loss: 0.5530, Test Loss: 0.8897\n",
      "Epoch 91, Training Loss: 0.7262, Validation Loss: 0.5071, Test Loss: 0.8736\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.7262\n",
      "Validation loss: 0.5071\n",
      "Test loss: 0.8736\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 107/243 with parameters:\n",
      "Config indices: (1, 0, 2, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7114.7705, Validation Loss: 6506.3940, Test Loss: 7121.3999\n",
      "Epoch 11, Training Loss: 202.1743, Validation Loss: 205.2160, Test Loss: 198.1381\n",
      "Epoch 21, Training Loss: 1.5252, Validation Loss: 1.1417, Test Loss: 1.2422\n",
      "Epoch 31, Training Loss: 0.8877, Validation Loss: 0.5778, Test Loss: 0.8525\n",
      "Epoch 41, Training Loss: 0.8352, Validation Loss: 0.5734, Test Loss: 0.8102\n",
      "Epoch 51, Training Loss: 0.6911, Validation Loss: 0.4676, Test Loss: 0.7140\n",
      "Epoch 61, Training Loss: 0.6547, Validation Loss: 0.4500, Test Loss: 0.6946\n",
      "Epoch 71, Training Loss: 0.6268, Validation Loss: 0.4259, Test Loss: 0.6662\n",
      "Epoch 81, Training Loss: 0.6161, Validation Loss: 0.4193, Test Loss: 0.6586\n",
      "Epoch 91, Training Loss: 0.5851, Validation Loss: 0.3943, Test Loss: 0.6300\n",
      "Epoch 101, Training Loss: 0.5794, Validation Loss: 0.3998, Test Loss: 0.6433\n",
      "Epoch 111, Training Loss: 0.5570, Validation Loss: 0.3764, Test Loss: 0.6165\n",
      "Epoch 121, Training Loss: 0.5553, Validation Loss: 0.3768, Test Loss: 0.5945\n",
      "Epoch 131, Training Loss: 0.5412, Validation Loss: 0.3686, Test Loss: 0.5858\n",
      "Epoch 141, Training Loss: 0.5214, Validation Loss: 0.3520, Test Loss: 0.5845\n",
      "Epoch 151, Training Loss: 0.5341, Validation Loss: 0.3568, Test Loss: 0.5740\n",
      "Epoch 161, Training Loss: 0.6234, Validation Loss: 0.4342, Test Loss: 0.7346\n",
      "Epoch 171, Training Loss: 0.5157, Validation Loss: 0.3494, Test Loss: 0.5973\n",
      "Epoch 181, Training Loss: 0.5047, Validation Loss: 0.3365, Test Loss: 0.5528\n",
      "Epoch 191, Training Loss: 0.4859, Validation Loss: 0.3278, Test Loss: 0.5605\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.4859\n",
      "Validation loss: 0.3278\n",
      "Test loss: 0.5605\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 108/243 with parameters:\n",
      "Config indices: (1, 0, 2, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7113.8765, Validation Loss: 6505.5000, Test Loss: 7120.4971\n",
      "Epoch 11, Training Loss: 137.7685, Validation Loss: 140.0353, Test Loss: 130.7375\n",
      "Epoch 21, Training Loss: 2.2059, Validation Loss: 1.7283, Test Loss: 1.8032\n",
      "Epoch 31, Training Loss: 1.0932, Validation Loss: 0.7660, Test Loss: 1.0260\n",
      "Epoch 41, Training Loss: 0.8228, Validation Loss: 0.5633, Test Loss: 0.8594\n",
      "Epoch 51, Training Loss: 0.7382, Validation Loss: 0.5120, Test Loss: 0.8328\n",
      "Epoch 61, Training Loss: 0.6626, Validation Loss: 0.4661, Test Loss: 0.7465\n",
      "Epoch 71, Training Loss: 0.6268, Validation Loss: 0.4401, Test Loss: 0.7032\n",
      "Epoch 81, Training Loss: 0.6115, Validation Loss: 0.4368, Test Loss: 0.6895\n",
      "Epoch 91, Training Loss: 0.5715, Validation Loss: 0.4009, Test Loss: 0.6744\n",
      "Epoch 101, Training Loss: 0.5522, Validation Loss: 0.3840, Test Loss: 0.6591\n",
      "Epoch 111, Training Loss: 0.5414, Validation Loss: 0.3767, Test Loss: 0.6539\n",
      "Epoch 121, Training Loss: 0.5866, Validation Loss: 0.4111, Test Loss: 0.7376\n",
      "Epoch 131, Training Loss: 0.5417, Validation Loss: 0.3748, Test Loss: 0.6770\n",
      "Epoch 141, Training Loss: 0.5235, Validation Loss: 0.3661, Test Loss: 0.6329\n",
      "Epoch 151, Training Loss: 0.5480, Validation Loss: 0.3929, Test Loss: 0.6325\n",
      "Epoch 161, Training Loss: 0.5047, Validation Loss: 0.3488, Test Loss: 0.6132\n",
      "Epoch 171, Training Loss: 0.5164, Validation Loss: 0.3603, Test Loss: 0.6431\n",
      "Epoch 181, Training Loss: 0.4924, Validation Loss: 0.3402, Test Loss: 0.6035\n",
      "Epoch 191, Training Loss: 0.4980, Validation Loss: 0.3486, Test Loss: 0.5955\n",
      "Epoch 201, Training Loss: 0.4934, Validation Loss: 0.3431, Test Loss: 0.5957\n",
      "Epoch 211, Training Loss: 0.4813, Validation Loss: 0.3332, Test Loss: 0.5854\n",
      "Epoch 221, Training Loss: 0.4745, Validation Loss: 0.3261, Test Loss: 0.5828\n",
      "Epoch 231, Training Loss: 0.4712, Validation Loss: 0.3252, Test Loss: 0.5762\n",
      "Epoch 241, Training Loss: 0.4918, Validation Loss: 0.3379, Test Loss: 0.6322\n",
      "Epoch 251, Training Loss: 0.4643, Validation Loss: 0.3230, Test Loss: 0.5695\n",
      "Epoch 261, Training Loss: 0.4576, Validation Loss: 0.3103, Test Loss: 0.5739\n",
      "Epoch 271, Training Loss: 0.4776, Validation Loss: 0.3277, Test Loss: 0.6156\n",
      "Epoch 281, Training Loss: 0.4572, Validation Loss: 0.3157, Test Loss: 0.5828\n",
      "Epoch 291, Training Loss: 0.4500, Validation Loss: 0.3077, Test Loss: 0.5757\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.4500\n",
      "Validation loss: 0.3077\n",
      "Test loss: 0.5757\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 109/243 with parameters:\n",
      "Config indices: (1, 1, 0, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 361.7279, Validation Loss: 362.0442, Test Loss: 391.9235\n",
      "Epoch 11, Training Loss: 0.5460, Validation Loss: 0.3360, Test Loss: 0.6910\n",
      "Epoch 21, Training Loss: 0.5693, Validation Loss: 0.3869, Test Loss: 0.6293\n",
      "Epoch 31, Training Loss: 0.6235, Validation Loss: 0.4513, Test Loss: 0.6581\n",
      "Epoch 41, Training Loss: 0.6312, Validation Loss: 0.4849, Test Loss: 0.7905\n",
      "Epoch 51, Training Loss: 0.7314, Validation Loss: 0.5851, Test Loss: 0.7259\n",
      "Epoch 61, Training Loss: 0.3608, Validation Loss: 0.2931, Test Loss: 0.4717\n",
      "Epoch 71, Training Loss: 0.6053, Validation Loss: 0.5713, Test Loss: 0.7311\n",
      "Epoch 81, Training Loss: 0.3242, Validation Loss: 0.2618, Test Loss: 0.4187\n",
      "Epoch 91, Training Loss: 0.2541, Validation Loss: 0.2063, Test Loss: 0.3530\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.2541\n",
      "Validation loss: 0.2063\n",
      "Test loss: 0.3530\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 110/243 with parameters:\n",
      "Config indices: (1, 1, 0, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 273.3723, Validation Loss: 274.3383, Test Loss: 271.4430\n",
      "Epoch 11, Training Loss: 0.5197, Validation Loss: 0.3588, Test Loss: 0.6371\n",
      "Epoch 21, Training Loss: 0.4909, Validation Loss: 0.3346, Test Loss: 0.6386\n",
      "Epoch 31, Training Loss: 0.3817, Validation Loss: 0.2593, Test Loss: 0.4642\n",
      "Epoch 41, Training Loss: 0.4265, Validation Loss: 0.3359, Test Loss: 0.5073\n",
      "Epoch 51, Training Loss: 1.8533, Validation Loss: 1.5396, Test Loss: 1.7884\n",
      "Epoch 61, Training Loss: 0.3093, Validation Loss: 0.2315, Test Loss: 0.3596\n",
      "Epoch 71, Training Loss: 0.4243, Validation Loss: 0.3880, Test Loss: 0.5236\n",
      "Epoch 81, Training Loss: 0.2597, Validation Loss: 0.1992, Test Loss: 0.3224\n",
      "Epoch 91, Training Loss: 0.5926, Validation Loss: 0.5172, Test Loss: 0.6883\n",
      "Epoch 101, Training Loss: 0.6715, Validation Loss: 0.5925, Test Loss: 0.8015\n",
      "Epoch 111, Training Loss: 0.3917, Validation Loss: 0.2992, Test Loss: 0.4213\n",
      "Epoch 121, Training Loss: 0.5682, Validation Loss: 0.5755, Test Loss: 0.6540\n",
      "Epoch 131, Training Loss: 0.2619, Validation Loss: 0.2081, Test Loss: 0.3254\n",
      "Epoch 141, Training Loss: 0.3826, Validation Loss: 0.3197, Test Loss: 0.4243\n",
      "Epoch 151, Training Loss: 0.3825, Validation Loss: 0.2876, Test Loss: 0.4054\n",
      "Epoch 161, Training Loss: 0.2299, Validation Loss: 0.1706, Test Loss: 0.2878\n",
      "Epoch 171, Training Loss: 0.8770, Validation Loss: 0.8381, Test Loss: 0.8920\n",
      "Epoch 181, Training Loss: 0.3008, Validation Loss: 0.2190, Test Loss: 0.3296\n",
      "Epoch 191, Training Loss: 0.6937, Validation Loss: 0.6981, Test Loss: 0.7851\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.6937\n",
      "Validation loss: 0.6981\n",
      "Test loss: 0.7851\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 111/243 with parameters:\n",
      "Config indices: (1, 1, 0, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 308.5295, Validation Loss: 308.2024, Test Loss: 323.0275\n",
      "Epoch 11, Training Loss: 0.6565, Validation Loss: 0.4430, Test Loss: 0.8631\n",
      "Epoch 21, Training Loss: 0.8083, Validation Loss: 0.6327, Test Loss: 0.8390\n",
      "Epoch 31, Training Loss: 0.6108, Validation Loss: 0.4242, Test Loss: 0.6455\n",
      "Epoch 41, Training Loss: 0.3800, Validation Loss: 0.2892, Test Loss: 0.5218\n",
      "Epoch 51, Training Loss: 1.0858, Validation Loss: 0.9339, Test Loss: 1.0924\n",
      "Epoch 61, Training Loss: 0.8068, Validation Loss: 0.7156, Test Loss: 0.9795\n",
      "Epoch 71, Training Loss: 0.3813, Validation Loss: 0.2843, Test Loss: 0.4099\n",
      "Epoch 81, Training Loss: 0.4333, Validation Loss: 0.3694, Test Loss: 0.5566\n",
      "Epoch 91, Training Loss: 1.1865, Validation Loss: 1.0521, Test Loss: 1.3678\n",
      "Epoch 101, Training Loss: 0.2157, Validation Loss: 0.1582, Test Loss: 0.2826\n",
      "Epoch 111, Training Loss: 1.0259, Validation Loss: 0.8803, Test Loss: 0.9894\n",
      "Epoch 121, Training Loss: 0.5144, Validation Loss: 0.4181, Test Loss: 0.5240\n",
      "Epoch 131, Training Loss: 0.7501, Validation Loss: 0.6647, Test Loss: 0.8841\n",
      "Epoch 141, Training Loss: 0.2811, Validation Loss: 0.2116, Test Loss: 0.3244\n",
      "Epoch 151, Training Loss: 0.9899, Validation Loss: 0.8260, Test Loss: 0.9636\n",
      "Epoch 161, Training Loss: 0.2597, Validation Loss: 0.1956, Test Loss: 0.3132\n",
      "Epoch 171, Training Loss: 0.5051, Validation Loss: 0.4381, Test Loss: 0.5936\n",
      "Epoch 181, Training Loss: 0.2966, Validation Loss: 0.2490, Test Loss: 0.3769\n",
      "Epoch 191, Training Loss: 2.1264, Validation Loss: 1.8897, Test Loss: 2.3032\n",
      "Epoch 201, Training Loss: 0.2522, Validation Loss: 0.1924, Test Loss: 0.3130\n",
      "Epoch 211, Training Loss: 0.5522, Validation Loss: 0.4796, Test Loss: 0.6764\n",
      "Epoch 221, Training Loss: 0.2195, Validation Loss: 0.1661, Test Loss: 0.2733\n",
      "Epoch 231, Training Loss: 0.2695, Validation Loss: 0.2331, Test Loss: 0.3511\n",
      "Epoch 241, Training Loss: 0.2550, Validation Loss: 0.1932, Test Loss: 0.3043\n",
      "Epoch 251, Training Loss: 0.4711, Validation Loss: 0.4109, Test Loss: 0.4873\n",
      "Epoch 261, Training Loss: 0.5628, Validation Loss: 0.4646, Test Loss: 0.5780\n",
      "Epoch 271, Training Loss: 0.3024, Validation Loss: 0.2695, Test Loss: 0.3742\n",
      "Epoch 281, Training Loss: 0.3292, Validation Loss: 0.2851, Test Loss: 0.4044\n",
      "Epoch 291, Training Loss: 0.2160, Validation Loss: 0.1656, Test Loss: 0.2831\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2160\n",
      "Validation loss: 0.1656\n",
      "Test loss: 0.2831\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 112/243 with parameters:\n",
      "Config indices: (1, 1, 0, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 1021.3127, Validation Loss: 1022.6877, Test Loss: 1007.5013\n",
      "Epoch 11, Training Loss: 0.6738, Validation Loss: 0.4361, Test Loss: 0.8207\n",
      "Epoch 21, Training Loss: 0.5990, Validation Loss: 0.3751, Test Loss: 0.7604\n",
      "Epoch 31, Training Loss: 0.5075, Validation Loss: 0.3173, Test Loss: 0.6373\n",
      "Epoch 41, Training Loss: 0.5528, Validation Loss: 0.3564, Test Loss: 0.6931\n",
      "Epoch 51, Training Loss: 0.7138, Validation Loss: 0.5259, Test Loss: 0.8956\n",
      "Epoch 61, Training Loss: 0.4455, Validation Loss: 0.3096, Test Loss: 0.5697\n",
      "Epoch 71, Training Loss: 0.4042, Validation Loss: 0.3098, Test Loss: 0.5357\n",
      "Epoch 81, Training Loss: 0.3650, Validation Loss: 0.2796, Test Loss: 0.4735\n",
      "Epoch 91, Training Loss: 0.3579, Validation Loss: 0.2966, Test Loss: 0.4811\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.3579\n",
      "Validation loss: 0.2966\n",
      "Test loss: 0.4811\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 113/243 with parameters:\n",
      "Config indices: (1, 1, 0, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 2494.1567, Validation Loss: 2186.6592, Test Loss: 2457.0664\n",
      "Epoch 11, Training Loss: 0.6870, Validation Loss: 0.4406, Test Loss: 0.8723\n",
      "Epoch 21, Training Loss: 0.5087, Validation Loss: 0.3196, Test Loss: 0.6687\n",
      "Epoch 31, Training Loss: 0.4630, Validation Loss: 0.2949, Test Loss: 0.6042\n",
      "Epoch 41, Training Loss: 0.4191, Validation Loss: 0.2739, Test Loss: 0.5527\n",
      "Epoch 51, Training Loss: 0.4776, Validation Loss: 0.3523, Test Loss: 0.6394\n",
      "Epoch 61, Training Loss: 0.3483, Validation Loss: 0.2371, Test Loss: 0.4394\n",
      "Epoch 71, Training Loss: 0.4021, Validation Loss: 0.2901, Test Loss: 0.5338\n",
      "Epoch 81, Training Loss: 0.5065, Validation Loss: 0.4001, Test Loss: 0.6619\n",
      "Epoch 91, Training Loss: 0.2680, Validation Loss: 0.2066, Test Loss: 0.3817\n",
      "Epoch 101, Training Loss: 0.3194, Validation Loss: 0.2322, Test Loss: 0.3667\n",
      "Epoch 111, Training Loss: 0.2991, Validation Loss: 0.2641, Test Loss: 0.3921\n",
      "Epoch 121, Training Loss: 0.8141, Validation Loss: 0.6384, Test Loss: 0.7944\n",
      "Epoch 131, Training Loss: 0.3192, Validation Loss: 0.2650, Test Loss: 0.3879\n",
      "Epoch 141, Training Loss: 0.4955, Validation Loss: 0.3820, Test Loss: 0.5098\n",
      "Epoch 151, Training Loss: 0.7253, Validation Loss: 0.7035, Test Loss: 0.7792\n",
      "Epoch 161, Training Loss: 0.3903, Validation Loss: 0.3027, Test Loss: 0.4336\n",
      "Epoch 171, Training Loss: 0.8005, Validation Loss: 0.6867, Test Loss: 0.8206\n",
      "Epoch 181, Training Loss: 0.2908, Validation Loss: 0.2390, Test Loss: 0.3550\n",
      "Epoch 191, Training Loss: 1.0478, Validation Loss: 0.9628, Test Loss: 1.0639\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 1.0478\n",
      "Validation loss: 0.9628\n",
      "Test loss: 1.0639\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 114/243 with parameters:\n",
      "Config indices: (1, 1, 0, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 1208.2590, Validation Loss: 1113.1213, Test Loss: 1200.6722\n",
      "Epoch 11, Training Loss: 0.5891, Validation Loss: 0.3593, Test Loss: 0.8001\n",
      "Epoch 21, Training Loss: 0.5271, Validation Loss: 0.3438, Test Loss: 0.6758\n",
      "Epoch 31, Training Loss: 0.4978, Validation Loss: 0.3454, Test Loss: 0.6392\n",
      "Epoch 41, Training Loss: 0.4721, Validation Loss: 0.3264, Test Loss: 0.6539\n",
      "Epoch 51, Training Loss: 0.4407, Validation Loss: 0.2835, Test Loss: 0.5318\n",
      "Epoch 61, Training Loss: 0.5420, Validation Loss: 0.3927, Test Loss: 0.7253\n",
      "Epoch 71, Training Loss: 0.3420, Validation Loss: 0.2509, Test Loss: 0.4473\n",
      "Epoch 81, Training Loss: 0.3179, Validation Loss: 0.2089, Test Loss: 0.3990\n",
      "Epoch 91, Training Loss: 0.3825, Validation Loss: 0.3032, Test Loss: 0.4588\n",
      "Epoch 101, Training Loss: 0.2910, Validation Loss: 0.2223, Test Loss: 0.4147\n",
      "Epoch 111, Training Loss: 0.4617, Validation Loss: 0.3525, Test Loss: 0.4820\n",
      "Epoch 121, Training Loss: 0.3599, Validation Loss: 0.3203, Test Loss: 0.4691\n",
      "Epoch 131, Training Loss: 0.2569, Validation Loss: 0.2050, Test Loss: 0.3140\n",
      "Epoch 141, Training Loss: 0.3147, Validation Loss: 0.2556, Test Loss: 0.4172\n",
      "Epoch 151, Training Loss: 0.2917, Validation Loss: 0.2393, Test Loss: 0.3966\n",
      "Epoch 161, Training Loss: 0.2064, Validation Loss: 0.1539, Test Loss: 0.2782\n",
      "Epoch 171, Training Loss: 0.3427, Validation Loss: 0.2914, Test Loss: 0.4457\n",
      "Epoch 181, Training Loss: 0.7948, Validation Loss: 0.6559, Test Loss: 0.7996\n",
      "Epoch 191, Training Loss: 0.6365, Validation Loss: 0.6431, Test Loss: 0.7248\n",
      "Epoch 201, Training Loss: 0.3988, Validation Loss: 0.3404, Test Loss: 0.5088\n",
      "Epoch 211, Training Loss: 1.3909, Validation Loss: 1.2509, Test Loss: 1.3622\n",
      "Epoch 221, Training Loss: 0.6672, Validation Loss: 0.6345, Test Loss: 0.6603\n",
      "Epoch 231, Training Loss: 1.0741, Validation Loss: 1.0212, Test Loss: 1.0365\n",
      "Epoch 241, Training Loss: 0.4380, Validation Loss: 0.3399, Test Loss: 0.4731\n",
      "Epoch 251, Training Loss: 0.4098, Validation Loss: 0.3388, Test Loss: 0.4502\n",
      "Epoch 261, Training Loss: 0.2910, Validation Loss: 0.2438, Test Loss: 0.3773\n",
      "Epoch 271, Training Loss: 1.1821, Validation Loss: 0.9648, Test Loss: 1.1284\n",
      "Epoch 281, Training Loss: 0.2139, Validation Loss: 0.1629, Test Loss: 0.2730\n",
      "Epoch 291, Training Loss: 0.2897, Validation Loss: 0.2201, Test Loss: 0.3211\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.2897\n",
      "Validation loss: 0.2201\n",
      "Test loss: 0.3211\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 115/243 with parameters:\n",
      "Config indices: (1, 1, 0, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6569.4497, Validation Loss: 5960.3872, Test Loss: 6581.0664\n",
      "Epoch 11, Training Loss: 2.3786, Validation Loss: 1.6866, Test Loss: 2.6461\n",
      "Epoch 21, Training Loss: 0.6463, Validation Loss: 0.3607, Test Loss: 0.8249\n",
      "Epoch 31, Training Loss: 0.5501, Validation Loss: 0.3434, Test Loss: 0.6703\n",
      "Epoch 41, Training Loss: 0.6362, Validation Loss: 0.4363, Test Loss: 0.8042\n",
      "Epoch 51, Training Loss: 0.5012, Validation Loss: 0.3341, Test Loss: 0.6406\n",
      "Epoch 61, Training Loss: 0.4599, Validation Loss: 0.3100, Test Loss: 0.5769\n",
      "Epoch 71, Training Loss: 0.4314, Validation Loss: 0.2683, Test Loss: 0.5365\n",
      "Epoch 81, Training Loss: 0.4096, Validation Loss: 0.2724, Test Loss: 0.5222\n",
      "Epoch 91, Training Loss: 0.3954, Validation Loss: 0.2594, Test Loss: 0.5089\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.3954\n",
      "Validation loss: 0.2594\n",
      "Test loss: 0.5089\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 116/243 with parameters:\n",
      "Config indices: (1, 1, 0, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6204.1841, Validation Loss: 5645.8140, Test Loss: 6220.6475\n",
      "Epoch 11, Training Loss: 1.5268, Validation Loss: 1.0839, Test Loss: 1.8226\n",
      "Epoch 21, Training Loss: 0.7389, Validation Loss: 0.4927, Test Loss: 0.9412\n",
      "Epoch 31, Training Loss: 0.5866, Validation Loss: 0.3596, Test Loss: 0.7277\n",
      "Epoch 41, Training Loss: 0.5398, Validation Loss: 0.3436, Test Loss: 0.6457\n",
      "Epoch 51, Training Loss: 0.6954, Validation Loss: 0.4846, Test Loss: 0.8645\n",
      "Epoch 61, Training Loss: 0.4794, Validation Loss: 0.3093, Test Loss: 0.5984\n",
      "Epoch 71, Training Loss: 0.4676, Validation Loss: 0.3150, Test Loss: 0.5607\n",
      "Epoch 81, Training Loss: 0.5308, Validation Loss: 0.3619, Test Loss: 0.6727\n",
      "Epoch 91, Training Loss: 0.6109, Validation Loss: 0.4652, Test Loss: 0.6563\n",
      "Epoch 101, Training Loss: 0.4008, Validation Loss: 0.2726, Test Loss: 0.4788\n",
      "Epoch 111, Training Loss: 0.3815, Validation Loss: 0.2390, Test Loss: 0.4543\n",
      "Epoch 121, Training Loss: 0.5477, Validation Loss: 0.4071, Test Loss: 0.6846\n",
      "Epoch 131, Training Loss: 0.3494, Validation Loss: 0.2484, Test Loss: 0.4568\n",
      "Epoch 141, Training Loss: 0.3269, Validation Loss: 0.2438, Test Loss: 0.4336\n",
      "Epoch 151, Training Loss: 0.5885, Validation Loss: 0.4621, Test Loss: 0.6063\n",
      "Epoch 161, Training Loss: 0.3088, Validation Loss: 0.2182, Test Loss: 0.3637\n",
      "Epoch 171, Training Loss: 0.2406, Validation Loss: 0.1680, Test Loss: 0.3217\n",
      "Epoch 181, Training Loss: 0.2501, Validation Loss: 0.1910, Test Loss: 0.3406\n",
      "Epoch 191, Training Loss: 0.2651, Validation Loss: 0.1995, Test Loss: 0.3601\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.2651\n",
      "Validation loss: 0.1995\n",
      "Test loss: 0.3601\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 117/243 with parameters:\n",
      "Config indices: (1, 1, 0, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6478.4512, Validation Loss: 5892.8428, Test Loss: 6490.6729\n",
      "Epoch 11, Training Loss: 2.6989, Validation Loss: 1.6935, Test Loss: 3.0463\n",
      "Epoch 21, Training Loss: 0.7136, Validation Loss: 0.3639, Test Loss: 0.8930\n",
      "Epoch 31, Training Loss: 0.5726, Validation Loss: 0.3420, Test Loss: 0.6614\n",
      "Epoch 41, Training Loss: 0.5592, Validation Loss: 0.3757, Test Loss: 0.6280\n",
      "Epoch 51, Training Loss: 0.4677, Validation Loss: 0.3007, Test Loss: 0.5664\n",
      "Epoch 61, Training Loss: 0.4779, Validation Loss: 0.3241, Test Loss: 0.5802\n",
      "Epoch 71, Training Loss: 0.4243, Validation Loss: 0.2793, Test Loss: 0.5129\n",
      "Epoch 81, Training Loss: 0.4039, Validation Loss: 0.2686, Test Loss: 0.4973\n",
      "Epoch 91, Training Loss: 0.3930, Validation Loss: 0.2626, Test Loss: 0.4859\n",
      "Epoch 101, Training Loss: 0.3649, Validation Loss: 0.2514, Test Loss: 0.4482\n",
      "Epoch 111, Training Loss: 0.4713, Validation Loss: 0.3344, Test Loss: 0.5105\n",
      "Epoch 121, Training Loss: 0.3238, Validation Loss: 0.2160, Test Loss: 0.3981\n",
      "Epoch 131, Training Loss: 0.3736, Validation Loss: 0.2574, Test Loss: 0.4248\n",
      "Epoch 141, Training Loss: 0.3060, Validation Loss: 0.2146, Test Loss: 0.3689\n",
      "Epoch 151, Training Loss: 0.3068, Validation Loss: 0.2329, Test Loss: 0.3908\n",
      "Epoch 161, Training Loss: 0.3701, Validation Loss: 0.2784, Test Loss: 0.4182\n",
      "Epoch 171, Training Loss: 0.7945, Validation Loss: 0.6874, Test Loss: 0.9534\n",
      "Epoch 181, Training Loss: 0.2810, Validation Loss: 0.2054, Test Loss: 0.3326\n",
      "Epoch 191, Training Loss: 0.5890, Validation Loss: 0.4916, Test Loss: 0.5856\n",
      "Epoch 201, Training Loss: 0.2572, Validation Loss: 0.2036, Test Loss: 0.3555\n",
      "Epoch 211, Training Loss: 0.2502, Validation Loss: 0.1840, Test Loss: 0.3040\n",
      "Epoch 221, Training Loss: 1.0168, Validation Loss: 0.8718, Test Loss: 1.0081\n",
      "Epoch 231, Training Loss: 0.2447, Validation Loss: 0.1905, Test Loss: 0.3067\n",
      "Epoch 241, Training Loss: 0.9791, Validation Loss: 0.8705, Test Loss: 1.1289\n",
      "Epoch 251, Training Loss: 0.2162, Validation Loss: 0.1581, Test Loss: 0.2801\n",
      "Epoch 261, Training Loss: 0.2563, Validation Loss: 0.2085, Test Loss: 0.3507\n",
      "Epoch 271, Training Loss: 0.6232, Validation Loss: 0.4857, Test Loss: 0.6127\n",
      "Epoch 281, Training Loss: 0.2062, Validation Loss: 0.1550, Test Loss: 0.2761\n",
      "Epoch 291, Training Loss: 0.3048, Validation Loss: 0.2532, Test Loss: 0.3553\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.3048\n",
      "Validation loss: 0.2532\n",
      "Test loss: 0.3553\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 118/243 with parameters:\n",
      "Config indices: (1, 1, 1, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7050.0356, Validation Loss: 6442.3701, Test Loss: 7057.6890\n",
      "Epoch 11, Training Loss: 42.6724, Validation Loss: 40.2977, Test Loss: 42.0147\n",
      "Epoch 21, Training Loss: 2.1048, Validation Loss: 1.2699, Test Loss: 2.6737\n",
      "Epoch 31, Training Loss: 0.8890, Validation Loss: 0.5022, Test Loss: 1.1965\n",
      "Epoch 41, Training Loss: 0.6990, Validation Loss: 0.4085, Test Loss: 0.9624\n",
      "Epoch 51, Training Loss: 0.6272, Validation Loss: 0.3718, Test Loss: 0.8314\n",
      "Epoch 61, Training Loss: 0.5854, Validation Loss: 0.3587, Test Loss: 0.7871\n",
      "Epoch 71, Training Loss: 0.5595, Validation Loss: 0.3489, Test Loss: 0.7508\n",
      "Epoch 81, Training Loss: 0.5260, Validation Loss: 0.3662, Test Loss: 0.6753\n",
      "Epoch 91, Training Loss: 0.4720, Validation Loss: 0.3174, Test Loss: 0.6540\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.4720\n",
      "Validation loss: 0.3174\n",
      "Test loss: 0.6540\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 119/243 with parameters:\n",
      "Config indices: (1, 1, 1, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7012.5293, Validation Loss: 6411.0332, Test Loss: 7020.7832\n",
      "Epoch 11, Training Loss: 28.4383, Validation Loss: 26.4665, Test Loss: 26.9890\n",
      "Epoch 21, Training Loss: 1.2964, Validation Loss: 0.7711, Test Loss: 1.6805\n",
      "Epoch 31, Training Loss: 0.7320, Validation Loss: 0.4281, Test Loss: 1.0036\n",
      "Epoch 41, Training Loss: 0.6477, Validation Loss: 0.3863, Test Loss: 0.8265\n",
      "Epoch 51, Training Loss: 0.6368, Validation Loss: 0.4162, Test Loss: 0.7754\n",
      "Epoch 61, Training Loss: 0.5739, Validation Loss: 0.3727, Test Loss: 0.7163\n",
      "Epoch 71, Training Loss: 0.6593, Validation Loss: 0.4403, Test Loss: 0.8530\n",
      "Epoch 81, Training Loss: 0.4765, Validation Loss: 0.3206, Test Loss: 0.6112\n",
      "Epoch 91, Training Loss: 0.4865, Validation Loss: 0.3196, Test Loss: 0.6236\n",
      "Epoch 101, Training Loss: 0.3942, Validation Loss: 0.2753, Test Loss: 0.5298\n",
      "Epoch 111, Training Loss: 0.4491, Validation Loss: 0.3183, Test Loss: 0.5204\n",
      "Epoch 121, Training Loss: 0.3259, Validation Loss: 0.2271, Test Loss: 0.4400\n",
      "Epoch 131, Training Loss: 0.3041, Validation Loss: 0.2123, Test Loss: 0.4254\n",
      "Epoch 141, Training Loss: 0.3581, Validation Loss: 0.2763, Test Loss: 0.4324\n",
      "Epoch 151, Training Loss: 0.3984, Validation Loss: 0.3007, Test Loss: 0.4558\n",
      "Epoch 161, Training Loss: 0.2945, Validation Loss: 0.2413, Test Loss: 0.4041\n",
      "Epoch 171, Training Loss: 0.4576, Validation Loss: 0.3721, Test Loss: 0.6087\n",
      "Epoch 181, Training Loss: 0.2988, Validation Loss: 0.2109, Test Loss: 0.3559\n",
      "Epoch 191, Training Loss: 0.3410, Validation Loss: 0.3123, Test Loss: 0.4209\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.3410\n",
      "Validation loss: 0.3123\n",
      "Test loss: 0.4209\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 120/243 with parameters:\n",
      "Config indices: (1, 1, 1, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7067.5684, Validation Loss: 6450.7500, Test Loss: 7074.9736\n",
      "Epoch 11, Training Loss: 29.7224, Validation Loss: 27.4959, Test Loss: 32.3495\n",
      "Epoch 21, Training Loss: 1.5733, Validation Loss: 1.0821, Test Loss: 2.0150\n",
      "Epoch 31, Training Loss: 0.6305, Validation Loss: 0.4079, Test Loss: 0.8219\n",
      "Epoch 41, Training Loss: 0.5841, Validation Loss: 0.3849, Test Loss: 0.7762\n",
      "Epoch 51, Training Loss: 0.5713, Validation Loss: 0.3704, Test Loss: 0.7134\n",
      "Epoch 61, Training Loss: 0.5328, Validation Loss: 0.3534, Test Loss: 0.6766\n",
      "Epoch 71, Training Loss: 0.4963, Validation Loss: 0.3441, Test Loss: 0.6777\n",
      "Epoch 81, Training Loss: 0.5003, Validation Loss: 0.3440, Test Loss: 0.6715\n",
      "Epoch 91, Training Loss: 0.5564, Validation Loss: 0.3885, Test Loss: 0.7450\n",
      "Epoch 101, Training Loss: 0.4797, Validation Loss: 0.3593, Test Loss: 0.5687\n",
      "Epoch 111, Training Loss: 0.5200, Validation Loss: 0.3890, Test Loss: 0.7082\n",
      "Epoch 121, Training Loss: 0.3735, Validation Loss: 0.2905, Test Loss: 0.4891\n",
      "Epoch 131, Training Loss: 0.3036, Validation Loss: 0.2363, Test Loss: 0.4379\n",
      "Epoch 141, Training Loss: 0.2681, Validation Loss: 0.1947, Test Loss: 0.3699\n",
      "Epoch 151, Training Loss: 0.3698, Validation Loss: 0.2914, Test Loss: 0.4321\n",
      "Epoch 161, Training Loss: 0.3523, Validation Loss: 0.2729, Test Loss: 0.4116\n",
      "Epoch 171, Training Loss: 0.3393, Validation Loss: 0.2497, Test Loss: 0.4005\n",
      "Epoch 181, Training Loss: 0.2824, Validation Loss: 0.2215, Test Loss: 0.3660\n",
      "Epoch 191, Training Loss: 0.2337, Validation Loss: 0.1689, Test Loss: 0.3055\n",
      "Epoch 201, Training Loss: 0.2407, Validation Loss: 0.2023, Test Loss: 0.3386\n",
      "Epoch 211, Training Loss: 0.2148, Validation Loss: 0.1613, Test Loss: 0.3106\n",
      "Epoch 221, Training Loss: 0.4070, Validation Loss: 0.3171, Test Loss: 0.4438\n",
      "Epoch 231, Training Loss: 0.3408, Validation Loss: 0.3168, Test Loss: 0.4171\n",
      "Epoch 241, Training Loss: 0.2072, Validation Loss: 0.1580, Test Loss: 0.2970\n",
      "Epoch 251, Training Loss: 0.3107, Validation Loss: 0.2668, Test Loss: 0.4353\n",
      "Epoch 261, Training Loss: 0.2264, Validation Loss: 0.1797, Test Loss: 0.3232\n",
      "Epoch 271, Training Loss: 0.3228, Validation Loss: 0.2949, Test Loss: 0.4225\n",
      "Epoch 281, Training Loss: 0.2345, Validation Loss: 0.1949, Test Loss: 0.3337\n",
      "Epoch 291, Training Loss: 0.2361, Validation Loss: 0.1897, Test Loss: 0.3071\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2361\n",
      "Validation loss: 0.1897\n",
      "Test loss: 0.3071\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 121/243 with parameters:\n",
      "Config indices: (1, 1, 1, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7147.3945, Validation Loss: 6537.4380, Test Loss: 7154.2471\n",
      "Epoch 11, Training Loss: 468.9598, Validation Loss: 454.3294, Test Loss: 503.2802\n",
      "Epoch 21, Training Loss: 114.3179, Validation Loss: 114.2838, Test Loss: 114.8052\n",
      "Epoch 31, Training Loss: 20.7913, Validation Loss: 19.6867, Test Loss: 20.3675\n",
      "Epoch 41, Training Loss: 3.8765, Validation Loss: 3.1523, Test Loss: 4.1150\n",
      "Epoch 51, Training Loss: 1.3696, Validation Loss: 0.9674, Test Loss: 1.6390\n",
      "Epoch 61, Training Loss: 0.8555, Validation Loss: 0.5764, Test Loss: 1.1025\n",
      "Epoch 71, Training Loss: 0.6491, Validation Loss: 0.4181, Test Loss: 0.8442\n",
      "Epoch 81, Training Loss: 0.5737, Validation Loss: 0.3652, Test Loss: 0.7542\n",
      "Epoch 91, Training Loss: 0.5381, Validation Loss: 0.3415, Test Loss: 0.6903\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.5381\n",
      "Validation loss: 0.3415\n",
      "Test loss: 0.6903\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 122/243 with parameters:\n",
      "Config indices: (1, 1, 1, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7146.2192, Validation Loss: 6536.0225, Test Loss: 7153.0312\n",
      "Epoch 11, Training Loss: 283.2182, Validation Loss: 279.9511, Test Loss: 291.2296\n",
      "Epoch 21, Training Loss: 22.4050, Validation Loss: 21.5458, Test Loss: 20.3576\n",
      "Epoch 31, Training Loss: 3.7746, Validation Loss: 3.3836, Test Loss: 3.9764\n",
      "Epoch 41, Training Loss: 1.6765, Validation Loss: 1.3738, Test Loss: 2.0155\n",
      "Epoch 51, Training Loss: 0.9593, Validation Loss: 0.6590, Test Loss: 1.2009\n",
      "Epoch 61, Training Loss: 0.6990, Validation Loss: 0.4753, Test Loss: 0.9082\n",
      "Epoch 71, Training Loss: 0.6010, Validation Loss: 0.3961, Test Loss: 0.7807\n",
      "Epoch 81, Training Loss: 0.5784, Validation Loss: 0.3943, Test Loss: 0.7550\n",
      "Epoch 91, Training Loss: 0.5893, Validation Loss: 0.4188, Test Loss: 0.7016\n",
      "Epoch 101, Training Loss: 0.4729, Validation Loss: 0.3181, Test Loss: 0.6148\n",
      "Epoch 111, Training Loss: 0.4486, Validation Loss: 0.2958, Test Loss: 0.5835\n",
      "Epoch 121, Training Loss: 0.4254, Validation Loss: 0.2884, Test Loss: 0.5453\n",
      "Epoch 131, Training Loss: 0.4311, Validation Loss: 0.2999, Test Loss: 0.5342\n",
      "Epoch 141, Training Loss: 0.4131, Validation Loss: 0.2951, Test Loss: 0.5573\n",
      "Epoch 151, Training Loss: 0.3604, Validation Loss: 0.2444, Test Loss: 0.4718\n",
      "Epoch 161, Training Loss: 0.3554, Validation Loss: 0.2631, Test Loss: 0.4686\n",
      "Epoch 171, Training Loss: 0.3586, Validation Loss: 0.2493, Test Loss: 0.4882\n",
      "Epoch 181, Training Loss: 0.3140, Validation Loss: 0.2307, Test Loss: 0.4305\n",
      "Epoch 191, Training Loss: 0.3552, Validation Loss: 0.2718, Test Loss: 0.4964\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.3552\n",
      "Validation loss: 0.2718\n",
      "Test loss: 0.4964\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 123/243 with parameters:\n",
      "Config indices: (1, 1, 1, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7141.0635, Validation Loss: 6531.4380, Test Loss: 7147.9854\n",
      "Epoch 11, Training Loss: 282.8896, Validation Loss: 281.9444, Test Loss: 298.0121\n",
      "Epoch 21, Training Loss: 44.4692, Validation Loss: 44.5179, Test Loss: 41.5957\n",
      "Epoch 31, Training Loss: 4.6844, Validation Loss: 4.0685, Test Loss: 4.9277\n",
      "Epoch 41, Training Loss: 1.5223, Validation Loss: 1.1577, Test Loss: 1.8718\n",
      "Epoch 51, Training Loss: 0.8776, Validation Loss: 0.5759, Test Loss: 1.1568\n",
      "Epoch 61, Training Loss: 0.6915, Validation Loss: 0.4459, Test Loss: 0.9470\n",
      "Epoch 71, Training Loss: 0.6073, Validation Loss: 0.3893, Test Loss: 0.8007\n",
      "Epoch 81, Training Loss: 0.5641, Validation Loss: 0.3582, Test Loss: 0.7288\n",
      "Epoch 91, Training Loss: 0.5258, Validation Loss: 0.3281, Test Loss: 0.6877\n",
      "Epoch 101, Training Loss: 0.5189, Validation Loss: 0.3238, Test Loss: 0.6851\n",
      "Epoch 111, Training Loss: 0.4955, Validation Loss: 0.3146, Test Loss: 0.6608\n",
      "Epoch 121, Training Loss: 0.4642, Validation Loss: 0.2915, Test Loss: 0.6072\n",
      "Epoch 131, Training Loss: 0.4471, Validation Loss: 0.2882, Test Loss: 0.5876\n",
      "Epoch 141, Training Loss: 0.4835, Validation Loss: 0.3362, Test Loss: 0.5883\n",
      "Epoch 151, Training Loss: 0.4074, Validation Loss: 0.2730, Test Loss: 0.5379\n",
      "Epoch 161, Training Loss: 0.4012, Validation Loss: 0.2721, Test Loss: 0.5266\n",
      "Epoch 171, Training Loss: 0.3654, Validation Loss: 0.2367, Test Loss: 0.4836\n",
      "Epoch 181, Training Loss: 0.3570, Validation Loss: 0.2306, Test Loss: 0.4771\n",
      "Epoch 191, Training Loss: 0.3519, Validation Loss: 0.2401, Test Loss: 0.4900\n",
      "Epoch 201, Training Loss: 0.3134, Validation Loss: 0.2134, Test Loss: 0.4299\n",
      "Epoch 211, Training Loss: 0.3745, Validation Loss: 0.2781, Test Loss: 0.4558\n",
      "Epoch 221, Training Loss: 0.2911, Validation Loss: 0.2023, Test Loss: 0.4119\n",
      "Epoch 231, Training Loss: 0.2776, Validation Loss: 0.1977, Test Loss: 0.3785\n",
      "Epoch 241, Training Loss: 0.3786, Validation Loss: 0.2869, Test Loss: 0.5202\n",
      "Epoch 251, Training Loss: 0.2908, Validation Loss: 0.2218, Test Loss: 0.4182\n",
      "Epoch 261, Training Loss: 0.3054, Validation Loss: 0.2272, Test Loss: 0.3736\n",
      "Epoch 271, Training Loss: 0.2576, Validation Loss: 0.1998, Test Loss: 0.3528\n",
      "Epoch 281, Training Loss: 0.3699, Validation Loss: 0.2838, Test Loss: 0.4217\n",
      "Epoch 291, Training Loss: 0.2913, Validation Loss: 0.2152, Test Loss: 0.3529\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.2913\n",
      "Validation loss: 0.2152\n",
      "Test loss: 0.3529\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 124/243 with parameters:\n",
      "Config indices: (1, 1, 1, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7161.9219, Validation Loss: 6551.1172, Test Loss: 7168.7358\n",
      "Epoch 11, Training Loss: 1704.9415, Validation Loss: 1490.2489, Test Loss: 1720.2524\n",
      "Epoch 21, Training Loss: 350.8846, Validation Loss: 350.7661, Test Loss: 372.8121\n",
      "Epoch 31, Training Loss: 142.0311, Validation Loss: 145.4068, Test Loss: 140.3478\n",
      "Epoch 41, Training Loss: 45.1393, Validation Loss: 45.2932, Test Loss: 42.9580\n",
      "Epoch 51, Training Loss: 14.1075, Validation Loss: 13.7287, Test Loss: 13.5908\n",
      "Epoch 61, Training Loss: 6.0972, Validation Loss: 5.6413, Test Loss: 5.9881\n",
      "Epoch 71, Training Loss: 3.2024, Validation Loss: 2.8210, Test Loss: 3.2583\n",
      "Epoch 81, Training Loss: 1.9034, Validation Loss: 1.5946, Test Loss: 2.0170\n",
      "Epoch 91, Training Loss: 1.2086, Validation Loss: 0.9357, Test Loss: 1.3489\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 1.2086\n",
      "Validation loss: 0.9357\n",
      "Test loss: 1.3489\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 125/243 with parameters:\n",
      "Config indices: (1, 1, 1, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7153.1445, Validation Loss: 6542.5889, Test Loss: 7160.0098\n",
      "Epoch 11, Training Loss: 814.9259, Validation Loss: 743.1809, Test Loss: 827.6433\n",
      "Epoch 21, Training Loss: 193.5904, Validation Loss: 194.2508, Test Loss: 199.9531\n",
      "Epoch 31, Training Loss: 75.4149, Validation Loss: 75.8675, Test Loss: 73.5871\n",
      "Epoch 41, Training Loss: 27.2329, Validation Loss: 27.1267, Test Loss: 26.0958\n",
      "Epoch 51, Training Loss: 9.0999, Validation Loss: 8.6925, Test Loss: 8.8209\n",
      "Epoch 61, Training Loss: 2.7499, Validation Loss: 2.3008, Test Loss: 3.0134\n",
      "Epoch 71, Training Loss: 1.2218, Validation Loss: 0.9097, Test Loss: 1.5533\n",
      "Epoch 81, Training Loss: 0.8462, Validation Loss: 0.6105, Test Loss: 1.1433\n",
      "Epoch 91, Training Loss: 0.6978, Validation Loss: 0.4847, Test Loss: 0.9247\n",
      "Epoch 101, Training Loss: 0.6102, Validation Loss: 0.4363, Test Loss: 0.8069\n",
      "Epoch 111, Training Loss: 0.5504, Validation Loss: 0.3852, Test Loss: 0.7234\n",
      "Epoch 121, Training Loss: 0.5082, Validation Loss: 0.3434, Test Loss: 0.6522\n",
      "Epoch 131, Training Loss: 0.4834, Validation Loss: 0.3280, Test Loss: 0.6058\n",
      "Epoch 141, Training Loss: 0.4569, Validation Loss: 0.3047, Test Loss: 0.5764\n",
      "Epoch 151, Training Loss: 0.4394, Validation Loss: 0.2933, Test Loss: 0.5606\n",
      "Epoch 161, Training Loss: 0.4304, Validation Loss: 0.2888, Test Loss: 0.5512\n",
      "Epoch 171, Training Loss: 0.4138, Validation Loss: 0.2735, Test Loss: 0.5127\n",
      "Epoch 181, Training Loss: 0.4094, Validation Loss: 0.2750, Test Loss: 0.5180\n",
      "Epoch 191, Training Loss: 0.3960, Validation Loss: 0.2613, Test Loss: 0.4986\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.3960\n",
      "Validation loss: 0.2613\n",
      "Test loss: 0.4986\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 126/243 with parameters:\n",
      "Config indices: (1, 1, 1, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7155.5210, Validation Loss: 6545.0835, Test Loss: 7162.3945\n",
      "Epoch 11, Training Loss: 875.1302, Validation Loss: 813.1185, Test Loss: 898.6240\n",
      "Epoch 21, Training Loss: 263.5508, Validation Loss: 266.9627, Test Loss: 266.2865\n",
      "Epoch 31, Training Loss: 75.4820, Validation Loss: 77.5506, Test Loss: 70.1230\n",
      "Epoch 41, Training Loss: 12.9892, Validation Loss: 12.1522, Test Loss: 12.5869\n",
      "Epoch 51, Training Loss: 4.0877, Validation Loss: 3.3703, Test Loss: 4.4183\n",
      "Epoch 61, Training Loss: 2.0572, Validation Loss: 1.4586, Test Loss: 2.3709\n",
      "Epoch 71, Training Loss: 1.2553, Validation Loss: 0.7862, Test Loss: 1.5416\n",
      "Epoch 81, Training Loss: 0.9159, Validation Loss: 0.5273, Test Loss: 1.1823\n",
      "Epoch 91, Training Loss: 0.7447, Validation Loss: 0.4261, Test Loss: 0.9881\n",
      "Epoch 101, Training Loss: 0.6500, Validation Loss: 0.3749, Test Loss: 0.8694\n",
      "Epoch 111, Training Loss: 0.5941, Validation Loss: 0.3471, Test Loss: 0.8079\n",
      "Epoch 121, Training Loss: 0.5547, Validation Loss: 0.3279, Test Loss: 0.7493\n",
      "Epoch 131, Training Loss: 0.5281, Validation Loss: 0.3167, Test Loss: 0.7184\n",
      "Epoch 141, Training Loss: 0.5142, Validation Loss: 0.3173, Test Loss: 0.6833\n",
      "Epoch 151, Training Loss: 0.4935, Validation Loss: 0.3032, Test Loss: 0.6754\n",
      "Epoch 161, Training Loss: 0.4807, Validation Loss: 0.2982, Test Loss: 0.6465\n",
      "Epoch 171, Training Loss: 0.4735, Validation Loss: 0.2955, Test Loss: 0.6179\n",
      "Epoch 181, Training Loss: 0.4588, Validation Loss: 0.2924, Test Loss: 0.6075\n",
      "Epoch 191, Training Loss: 0.4499, Validation Loss: 0.2901, Test Loss: 0.5922\n",
      "Epoch 201, Training Loss: 0.4387, Validation Loss: 0.2827, Test Loss: 0.5839\n",
      "Epoch 211, Training Loss: 0.4407, Validation Loss: 0.2869, Test Loss: 0.5670\n",
      "Epoch 221, Training Loss: 0.4274, Validation Loss: 0.2751, Test Loss: 0.5780\n",
      "Epoch 231, Training Loss: 0.4123, Validation Loss: 0.2679, Test Loss: 0.5401\n",
      "Epoch 241, Training Loss: 0.4060, Validation Loss: 0.2665, Test Loss: 0.5350\n",
      "Epoch 251, Training Loss: 0.4029, Validation Loss: 0.2692, Test Loss: 0.5300\n",
      "Epoch 261, Training Loss: 0.3819, Validation Loss: 0.2548, Test Loss: 0.5045\n",
      "Epoch 271, Training Loss: 0.3849, Validation Loss: 0.2556, Test Loss: 0.5278\n",
      "Epoch 281, Training Loss: 0.3640, Validation Loss: 0.2450, Test Loss: 0.5010\n",
      "Epoch 291, Training Loss: 0.3489, Validation Loss: 0.2290, Test Loss: 0.4682\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.3489\n",
      "Validation loss: 0.2290\n",
      "Test loss: 0.4682\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 127/243 with parameters:\n",
      "Config indices: (1, 1, 2, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7163.6621, Validation Loss: 6552.8857, Test Loss: 7170.4980\n",
      "Epoch 11, Training Loss: 6433.3789, Validation Loss: 5845.5273, Test Loss: 6448.1084\n",
      "Epoch 21, Training Loss: 2792.5620, Validation Loss: 2424.6311, Test Loss: 2817.6340\n",
      "Epoch 31, Training Loss: 712.3730, Validation Loss: 644.1082, Test Loss: 743.4496\n",
      "Epoch 41, Training Loss: 457.4418, Validation Loss: 437.7306, Test Loss: 491.0116\n",
      "Epoch 51, Training Loss: 307.0817, Validation Loss: 299.8938, Test Loss: 326.9957\n",
      "Epoch 61, Training Loss: 209.1273, Validation Loss: 206.8964, Test Loss: 216.2261\n",
      "Epoch 71, Training Loss: 138.4569, Validation Loss: 136.3073, Test Loss: 137.8468\n",
      "Epoch 81, Training Loss: 80.8269, Validation Loss: 78.1163, Test Loss: 77.5217\n",
      "Epoch 91, Training Loss: 38.3394, Validation Loss: 35.3075, Test Loss: 36.1672\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 38.3394\n",
      "Validation loss: 35.3075\n",
      "Test loss: 36.1672\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 128/243 with parameters:\n",
      "Config indices: (1, 1, 2, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7164.5322, Validation Loss: 6553.6616, Test Loss: 7171.3442\n",
      "Epoch 11, Training Loss: 6505.7905, Validation Loss: 5939.4434, Test Loss: 6519.9888\n",
      "Epoch 21, Training Loss: 2269.7034, Validation Loss: 2055.8020, Test Loss: 2334.2393\n",
      "Epoch 31, Training Loss: 735.9615, Validation Loss: 701.2628, Test Loss: 788.4144\n",
      "Epoch 41, Training Loss: 573.2847, Validation Loss: 552.5643, Test Loss: 602.4091\n",
      "Epoch 51, Training Loss: 395.8082, Validation Loss: 384.8849, Test Loss: 403.7529\n",
      "Epoch 61, Training Loss: 209.0029, Validation Loss: 204.8467, Test Loss: 206.1276\n",
      "Epoch 71, Training Loss: 91.8649, Validation Loss: 88.5357, Test Loss: 88.1392\n",
      "Epoch 81, Training Loss: 42.0025, Validation Loss: 38.8340, Test Loss: 39.7689\n",
      "Epoch 91, Training Loss: 18.4883, Validation Loss: 16.0236, Test Loss: 17.8157\n",
      "Epoch 101, Training Loss: 8.9714, Validation Loss: 7.1071, Test Loss: 8.9617\n",
      "Epoch 111, Training Loss: 4.7762, Validation Loss: 3.4811, Test Loss: 4.9945\n",
      "Epoch 121, Training Loss: 2.4962, Validation Loss: 1.5861, Test Loss: 2.8508\n",
      "Epoch 131, Training Loss: 1.4204, Validation Loss: 0.7486, Test Loss: 1.7988\n",
      "Epoch 141, Training Loss: 0.9449, Validation Loss: 0.4428, Test Loss: 1.3194\n",
      "Epoch 151, Training Loss: 0.7376, Validation Loss: 0.3599, Test Loss: 1.0781\n",
      "Epoch 161, Training Loss: 0.6332, Validation Loss: 0.3363, Test Loss: 0.8998\n",
      "Epoch 171, Training Loss: 0.5829, Validation Loss: 0.3314, Test Loss: 0.8023\n",
      "Epoch 181, Training Loss: 0.5639, Validation Loss: 0.3342, Test Loss: 0.7674\n",
      "Epoch 191, Training Loss: 0.5328, Validation Loss: 0.3183, Test Loss: 0.6932\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.5328\n",
      "Validation loss: 0.3183\n",
      "Test loss: 0.6932\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 129/243 with parameters:\n",
      "Config indices: (1, 1, 2, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7161.9868, Validation Loss: 6551.3066, Test Loss: 7168.8535\n",
      "Epoch 11, Training Loss: 6506.2705, Validation Loss: 5918.3511, Test Loss: 6520.6836\n",
      "Epoch 21, Training Loss: 2614.6426, Validation Loss: 2275.5132, Test Loss: 2652.4377\n",
      "Epoch 31, Training Loss: 738.1056, Validation Loss: 677.1099, Test Loss: 780.2050\n",
      "Epoch 41, Training Loss: 529.2612, Validation Loss: 505.7324, Test Loss: 566.7828\n",
      "Epoch 51, Training Loss: 346.9594, Validation Loss: 340.1923, Test Loss: 369.1530\n",
      "Epoch 61, Training Loss: 217.6080, Validation Loss: 215.8414, Test Loss: 225.2401\n",
      "Epoch 71, Training Loss: 132.7471, Validation Loss: 131.5227, Test Loss: 133.2850\n",
      "Epoch 81, Training Loss: 72.3565, Validation Loss: 70.6418, Test Loss: 71.1856\n",
      "Epoch 91, Training Loss: 37.5343, Validation Loss: 35.9618, Test Loss: 36.3205\n",
      "Epoch 101, Training Loss: 16.1508, Validation Loss: 14.5371, Test Loss: 15.6834\n",
      "Epoch 111, Training Loss: 6.5300, Validation Loss: 5.3498, Test Loss: 6.6819\n",
      "Epoch 121, Training Loss: 2.7557, Validation Loss: 1.9584, Test Loss: 3.1395\n",
      "Epoch 131, Training Loss: 1.3704, Validation Loss: 0.7983, Test Loss: 1.7976\n",
      "Epoch 141, Training Loss: 0.8671, Validation Loss: 0.4415, Test Loss: 1.2823\n",
      "Epoch 151, Training Loss: 0.6932, Validation Loss: 0.3562, Test Loss: 1.0594\n",
      "Epoch 161, Training Loss: 0.5840, Validation Loss: 0.3002, Test Loss: 0.9330\n",
      "Epoch 171, Training Loss: 0.5384, Validation Loss: 0.3043, Test Loss: 0.8398\n",
      "Epoch 181, Training Loss: 0.5403, Validation Loss: 0.3367, Test Loss: 0.7694\n",
      "Epoch 191, Training Loss: 0.4740, Validation Loss: 0.2856, Test Loss: 0.7047\n",
      "Epoch 201, Training Loss: 0.4891, Validation Loss: 0.3220, Test Loss: 0.6788\n",
      "Epoch 211, Training Loss: 0.4466, Validation Loss: 0.2823, Test Loss: 0.6467\n",
      "Epoch 221, Training Loss: 0.4419, Validation Loss: 0.2889, Test Loss: 0.6202\n",
      "Epoch 231, Training Loss: 0.4301, Validation Loss: 0.2758, Test Loss: 0.6041\n",
      "Epoch 241, Training Loss: 0.4291, Validation Loss: 0.2735, Test Loss: 0.6100\n",
      "Epoch 251, Training Loss: 0.4388, Validation Loss: 0.2930, Test Loss: 0.5832\n",
      "Epoch 261, Training Loss: 0.4082, Validation Loss: 0.2653, Test Loss: 0.5705\n",
      "Epoch 271, Training Loss: 0.4056, Validation Loss: 0.2681, Test Loss: 0.5526\n",
      "Epoch 281, Training Loss: 0.3945, Validation Loss: 0.2577, Test Loss: 0.5500\n",
      "Epoch 291, Training Loss: 0.3902, Validation Loss: 0.2586, Test Loss: 0.5419\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.3902\n",
      "Validation loss: 0.2586\n",
      "Test loss: 0.5419\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 130/243 with parameters:\n",
      "Config indices: (1, 1, 2, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.0244, Validation Loss: 6555.1333, Test Loss: 7172.8423\n",
      "Epoch 11, Training Loss: 7104.1240, Validation Loss: 6495.9609, Test Loss: 7111.2798\n",
      "Epoch 21, Training Loss: 6497.8140, Validation Loss: 5909.8926, Test Loss: 6511.5107\n",
      "Epoch 31, Training Loss: 4763.0117, Validation Loss: 4248.5967, Test Loss: 4791.0298\n",
      "Epoch 41, Training Loss: 2585.7041, Validation Loss: 2250.5703, Test Loss: 2609.6724\n",
      "Epoch 51, Training Loss: 1152.2576, Validation Loss: 1017.0283, Test Loss: 1176.2463\n",
      "Epoch 61, Training Loss: 641.4995, Validation Loss: 595.8676, Test Loss: 673.0620\n",
      "Epoch 71, Training Loss: 503.2956, Validation Loss: 482.6406, Test Loss: 535.0835\n",
      "Epoch 81, Training Loss: 398.3592, Validation Loss: 388.9274, Test Loss: 425.1911\n",
      "Epoch 91, Training Loss: 310.3260, Validation Loss: 306.0791, Test Loss: 329.9511\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 310.3260\n",
      "Validation loss: 306.0791\n",
      "Test loss: 329.9511\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 131/243 with parameters:\n",
      "Config indices: (1, 1, 2, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.0112, Validation Loss: 6555.1221, Test Loss: 7172.8164\n",
      "Epoch 11, Training Loss: 7109.0845, Validation Loss: 6500.0493, Test Loss: 7116.2817\n",
      "Epoch 21, Training Loss: 6703.7705, Validation Loss: 6102.4321, Test Loss: 6715.4683\n",
      "Epoch 31, Training Loss: 5538.3887, Validation Loss: 4965.2827, Test Loss: 5559.9272\n",
      "Epoch 41, Training Loss: 3839.8965, Validation Loss: 3362.3198, Test Loss: 3857.5496\n",
      "Epoch 51, Training Loss: 2303.1423, Validation Loss: 2003.9518, Test Loss: 2309.4224\n",
      "Epoch 61, Training Loss: 1177.6868, Validation Loss: 1042.2607, Test Loss: 1191.6116\n",
      "Epoch 71, Training Loss: 653.6870, Validation Loss: 606.8216, Test Loss: 676.9882\n",
      "Epoch 81, Training Loss: 471.0378, Validation Loss: 458.3755, Test Loss: 497.5763\n",
      "Epoch 91, Training Loss: 369.8843, Validation Loss: 369.0601, Test Loss: 393.4620\n",
      "Epoch 101, Training Loss: 301.5067, Validation Loss: 304.3695, Test Loss: 318.5043\n",
      "Epoch 111, Training Loss: 252.2857, Validation Loss: 257.2539, Test Loss: 263.7628\n",
      "Epoch 121, Training Loss: 211.4569, Validation Loss: 217.2461, Test Loss: 218.1902\n",
      "Epoch 131, Training Loss: 173.7567, Validation Loss: 179.5825, Test Loss: 176.5815\n",
      "Epoch 141, Training Loss: 137.7902, Validation Loss: 143.3285, Test Loss: 137.5657\n",
      "Epoch 151, Training Loss: 106.4803, Validation Loss: 111.0788, Test Loss: 103.8953\n",
      "Epoch 161, Training Loss: 79.7452, Validation Loss: 83.3699, Test Loss: 75.8483\n",
      "Epoch 171, Training Loss: 58.0025, Validation Loss: 60.3483, Test Loss: 53.7088\n",
      "Epoch 181, Training Loss: 40.8619, Validation Loss: 42.0225, Test Loss: 36.8712\n",
      "Epoch 191, Training Loss: 27.8444, Validation Loss: 28.0749, Test Loss: 24.7169\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 27.8444\n",
      "Validation loss: 28.0749\n",
      "Test loss: 24.7169\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 132/243 with parameters:\n",
      "Config indices: (1, 1, 2, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7165.5845, Validation Loss: 6554.6709, Test Loss: 7172.4043\n",
      "Epoch 11, Training Loss: 7087.1816, Validation Loss: 6478.8057, Test Loss: 7094.6167\n",
      "Epoch 21, Training Loss: 6421.3213, Validation Loss: 5839.5732, Test Loss: 6436.3228\n",
      "Epoch 31, Training Loss: 4401.2285, Validation Loss: 3931.6172, Test Loss: 4433.5742\n",
      "Epoch 41, Training Loss: 2045.0142, Validation Loss: 1791.1744, Test Loss: 2075.2065\n",
      "Epoch 51, Training Loss: 901.7910, Validation Loss: 819.4646, Test Loss: 930.7241\n",
      "Epoch 61, Training Loss: 633.9974, Validation Loss: 605.4498, Test Loss: 665.8959\n",
      "Epoch 71, Training Loss: 527.2106, Validation Loss: 512.8737, Test Loss: 556.3080\n",
      "Epoch 81, Training Loss: 420.3524, Validation Loss: 414.5692, Test Loss: 442.6398\n",
      "Epoch 91, Training Loss: 323.8218, Validation Loss: 322.7070, Test Loss: 338.1385\n",
      "Epoch 101, Training Loss: 249.9435, Validation Loss: 251.1945, Test Loss: 257.0187\n",
      "Epoch 111, Training Loss: 194.5456, Validation Loss: 196.6518, Test Loss: 195.9952\n",
      "Epoch 121, Training Loss: 146.8450, Validation Loss: 149.1393, Test Loss: 145.1405\n",
      "Epoch 131, Training Loss: 106.1181, Validation Loss: 107.7864, Test Loss: 102.6250\n",
      "Epoch 141, Training Loss: 72.5242, Validation Loss: 72.9061, Test Loss: 69.0191\n",
      "Epoch 151, Training Loss: 47.4536, Validation Loss: 46.8139, Test Loss: 44.6295\n",
      "Epoch 161, Training Loss: 30.2207, Validation Loss: 29.2273, Test Loss: 28.3544\n",
      "Epoch 171, Training Loss: 18.8804, Validation Loss: 17.8000, Test Loss: 18.0910\n",
      "Epoch 181, Training Loss: 11.6736, Validation Loss: 10.4863, Test Loss: 11.4768\n",
      "Epoch 191, Training Loss: 7.0309, Validation Loss: 5.8479, Test Loss: 7.1775\n",
      "Epoch 201, Training Loss: 4.3497, Validation Loss: 3.2498, Test Loss: 4.6560\n",
      "Epoch 211, Training Loss: 2.9168, Validation Loss: 2.0039, Test Loss: 3.2888\n",
      "Epoch 221, Training Loss: 2.1139, Validation Loss: 1.3779, Test Loss: 2.5462\n",
      "Epoch 231, Training Loss: 1.6263, Validation Loss: 1.0032, Test Loss: 2.0623\n",
      "Epoch 241, Training Loss: 1.2937, Validation Loss: 0.7535, Test Loss: 1.7291\n",
      "Epoch 251, Training Loss: 1.0787, Validation Loss: 0.6060, Test Loss: 1.4935\n",
      "Epoch 261, Training Loss: 0.9369, Validation Loss: 0.5102, Test Loss: 1.3332\n",
      "Epoch 271, Training Loss: 0.8362, Validation Loss: 0.4514, Test Loss: 1.2084\n",
      "Epoch 281, Training Loss: 0.7642, Validation Loss: 0.4213, Test Loss: 1.1187\n",
      "Epoch 291, Training Loss: 0.7254, Validation Loss: 0.4108, Test Loss: 1.0640\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.7254\n",
      "Validation loss: 0.4108\n",
      "Test loss: 1.0640\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 133/243 with parameters:\n",
      "Config indices: (1, 1, 2, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.2856, Validation Loss: 6555.3530, Test Loss: 7173.0972\n",
      "Epoch 11, Training Loss: 7153.2427, Validation Loss: 6542.8716, Test Loss: 7160.0342\n",
      "Epoch 21, Training Loss: 7086.2231, Validation Loss: 6479.7847, Test Loss: 7093.6035\n",
      "Epoch 31, Training Loss: 6840.9360, Validation Loss: 6250.7329, Test Loss: 6851.2832\n",
      "Epoch 41, Training Loss: 6253.2275, Validation Loss: 5704.6978, Test Loss: 6271.5425\n",
      "Epoch 51, Training Loss: 5210.2124, Validation Loss: 4739.8101, Test Loss: 5243.1626\n",
      "Epoch 61, Training Loss: 3810.8408, Validation Loss: 3451.4836, Test Loss: 3862.9583\n",
      "Epoch 71, Training Loss: 2401.1794, Validation Loss: 2162.5928, Test Loss: 2470.7334\n",
      "Epoch 81, Training Loss: 1391.8822, Validation Loss: 1252.4910, Test Loss: 1470.3059\n",
      "Epoch 91, Training Loss: 933.2283, Validation Loss: 851.0521, Test Loss: 1010.4178\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 933.2283\n",
      "Validation loss: 851.0521\n",
      "Test loss: 1010.4178\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 134/243 with parameters:\n",
      "Config indices: (1, 1, 2, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.0293, Validation Loss: 6555.1299, Test Loss: 7172.8574\n",
      "Epoch 11, Training Loss: 7153.5205, Validation Loss: 6542.9243, Test Loss: 7160.3789\n",
      "Epoch 21, Training Loss: 7104.1963, Validation Loss: 6494.2407, Test Loss: 7111.4634\n",
      "Epoch 31, Training Loss: 6953.1040, Validation Loss: 6345.3970, Test Loss: 6961.9360\n",
      "Epoch 41, Training Loss: 6606.0444, Validation Loss: 6004.9971, Test Loss: 6618.3740\n",
      "Epoch 51, Training Loss: 5999.8965, Validation Loss: 5414.3354, Test Loss: 6017.2407\n",
      "Epoch 61, Training Loss: 5139.1812, Validation Loss: 4586.7910, Test Loss: 5160.3887\n",
      "Epoch 71, Training Loss: 4147.8813, Validation Loss: 3654.6399, Test Loss: 4167.1514\n",
      "Epoch 81, Training Loss: 3176.1243, Validation Loss: 2772.4517, Test Loss: 3187.1377\n",
      "Epoch 91, Training Loss: 2318.1472, Validation Loss: 2021.5360, Test Loss: 2322.9822\n",
      "Epoch 101, Training Loss: 1605.4811, Validation Loss: 1410.0459, Test Loss: 1611.7212\n",
      "Epoch 111, Training Loss: 1088.9149, Validation Loss: 972.3821, Test Loss: 1100.6322\n",
      "Epoch 121, Training Loss: 777.3943, Validation Loss: 713.1721, Test Loss: 793.6853\n",
      "Epoch 131, Training Loss: 614.2599, Validation Loss: 580.2675, Test Loss: 634.7631\n",
      "Epoch 141, Training Loss: 522.9986, Validation Loss: 505.6083, Test Loss: 544.9296\n",
      "Epoch 151, Training Loss: 457.6957, Validation Loss: 449.3485, Test Loss: 479.0973\n",
      "Epoch 161, Training Loss: 404.3543, Validation Loss: 401.0214, Test Loss: 424.1897\n",
      "Epoch 171, Training Loss: 357.5254, Validation Loss: 357.3070, Test Loss: 375.0271\n",
      "Epoch 181, Training Loss: 318.0179, Validation Loss: 319.1133, Test Loss: 332.5411\n",
      "Epoch 191, Training Loss: 285.8875, Validation Loss: 287.4897, Test Loss: 297.1221\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 285.8875\n",
      "Validation loss: 287.4897\n",
      "Test loss: 297.1221\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 135/243 with parameters:\n",
      "Config indices: (1, 1, 2, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7165.1997, Validation Loss: 6554.3008, Test Loss: 7172.0347\n",
      "Epoch 11, Training Loss: 7141.1606, Validation Loss: 6531.2891, Test Loss: 7148.0674\n",
      "Epoch 21, Training Loss: 7018.1348, Validation Loss: 6414.8403, Test Loss: 7026.1982\n",
      "Epoch 31, Training Loss: 6572.3105, Validation Loss: 5996.9033, Test Loss: 6585.7422\n",
      "Epoch 41, Training Loss: 5600.6484, Validation Loss: 5091.1143, Test Loss: 5626.7617\n",
      "Epoch 51, Training Loss: 4111.3667, Validation Loss: 3712.1423, Test Loss: 4155.5718\n",
      "Epoch 61, Training Loss: 2505.1345, Validation Loss: 2240.5686, Test Loss: 2564.4180\n",
      "Epoch 71, Training Loss: 1367.7452, Validation Loss: 1221.4332, Test Loss: 1430.7715\n",
      "Epoch 81, Training Loss: 893.3523, Validation Loss: 817.4255, Test Loss: 952.3890\n",
      "Epoch 91, Training Loss: 762.5067, Validation Loss: 715.1774, Test Loss: 817.1469\n",
      "Epoch 101, Training Loss: 703.4478, Validation Loss: 666.9389, Test Loss: 754.3632\n",
      "Epoch 111, Training Loss: 649.8484, Validation Loss: 619.2790, Test Loss: 695.7710\n",
      "Epoch 121, Training Loss: 592.5804, Validation Loss: 567.3328, Test Loss: 632.9595\n",
      "Epoch 131, Training Loss: 531.4187, Validation Loss: 511.3229, Test Loss: 564.9643\n",
      "Epoch 141, Training Loss: 467.4562, Validation Loss: 451.6652, Test Loss: 494.4426\n",
      "Epoch 151, Training Loss: 401.5594, Validation Loss: 389.3422, Test Loss: 421.8217\n",
      "Epoch 161, Training Loss: 336.3894, Validation Loss: 327.8811, Test Loss: 350.9254\n",
      "Epoch 171, Training Loss: 277.4611, Validation Loss: 271.1686, Test Loss: 286.5812\n",
      "Epoch 181, Training Loss: 226.1417, Validation Loss: 221.7774, Test Loss: 230.3400\n",
      "Epoch 191, Training Loss: 182.7387, Validation Loss: 179.2605, Test Loss: 183.3433\n",
      "Epoch 201, Training Loss: 145.7920, Validation Loss: 142.7391, Test Loss: 144.5869\n",
      "Epoch 211, Training Loss: 113.5820, Validation Loss: 110.0886, Test Loss: 111.2746\n",
      "Epoch 221, Training Loss: 86.2501, Validation Loss: 82.6153, Test Loss: 83.7507\n",
      "Epoch 231, Training Loss: 63.8646, Validation Loss: 60.0739, Test Loss: 61.7532\n",
      "Epoch 241, Training Loss: 47.0891, Validation Loss: 43.5925, Test Loss: 45.6246\n",
      "Epoch 251, Training Loss: 35.0522, Validation Loss: 31.7014, Test Loss: 34.0397\n",
      "Epoch 261, Training Loss: 26.1468, Validation Loss: 23.0825, Test Loss: 25.5271\n",
      "Epoch 271, Training Loss: 19.8479, Validation Loss: 17.1644, Test Loss: 19.5337\n",
      "Epoch 281, Training Loss: 15.3769, Validation Loss: 13.0158, Test Loss: 15.3667\n",
      "Epoch 291, Training Loss: 12.0199, Validation Loss: 9.9104, Test Loss: 12.1907\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 12.0199\n",
      "Validation loss: 9.9104\n",
      "Test loss: 12.1907\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 136/243 with parameters:\n",
      "Config indices: (1, 2, 0, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 4321.5488, Validation Loss: 3806.6794, Test Loss: 4344.3525\n",
      "Epoch 11, Training Loss: 203.8048, Validation Loss: 203.9688, Test Loss: 211.0565\n",
      "Epoch 21, Training Loss: 84.4043, Validation Loss: 84.5660, Test Loss: 83.6209\n",
      "Epoch 31, Training Loss: 34.1117, Validation Loss: 33.0930, Test Loss: 33.4101\n",
      "Epoch 41, Training Loss: 15.3033, Validation Loss: 14.1881, Test Loss: 15.2879\n",
      "Epoch 51, Training Loss: 7.9112, Validation Loss: 6.9342, Test Loss: 8.2797\n",
      "Epoch 61, Training Loss: 4.6707, Validation Loss: 3.8497, Test Loss: 5.1611\n",
      "Epoch 71, Training Loss: 3.1763, Validation Loss: 2.4864, Test Loss: 3.6699\n",
      "Epoch 81, Training Loss: 2.3997, Validation Loss: 1.7955, Test Loss: 2.8713\n",
      "Epoch 91, Training Loss: 1.9675, Validation Loss: 1.4240, Test Loss: 2.4145\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 1.9675\n",
      "Validation loss: 1.4240\n",
      "Test loss: 2.4145\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 137/243 with parameters:\n",
      "Config indices: (1, 2, 0, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 4346.1538, Validation Loss: 3943.0925, Test Loss: 4384.1880\n",
      "Epoch 11, Training Loss: 189.2866, Validation Loss: 196.5745, Test Loss: 180.5221\n",
      "Epoch 21, Training Loss: 40.2068, Validation Loss: 39.4765, Test Loss: 35.3918\n",
      "Epoch 31, Training Loss: 9.2650, Validation Loss: 8.2096, Test Loss: 8.4127\n",
      "Epoch 41, Training Loss: 3.6129, Validation Loss: 2.8297, Test Loss: 3.5352\n",
      "Epoch 51, Training Loss: 2.0217, Validation Loss: 1.4012, Test Loss: 2.1476\n",
      "Epoch 61, Training Loss: 1.4016, Validation Loss: 0.8827, Test Loss: 1.6052\n",
      "Epoch 71, Training Loss: 1.1168, Validation Loss: 0.6730, Test Loss: 1.3414\n",
      "Epoch 81, Training Loss: 0.9615, Validation Loss: 0.5606, Test Loss: 1.1789\n",
      "Epoch 91, Training Loss: 0.8672, Validation Loss: 0.4941, Test Loss: 1.0767\n",
      "Epoch 101, Training Loss: 0.8040, Validation Loss: 0.4545, Test Loss: 1.0080\n",
      "Epoch 111, Training Loss: 0.7578, Validation Loss: 0.4291, Test Loss: 0.9478\n",
      "Epoch 121, Training Loss: 0.7229, Validation Loss: 0.4115, Test Loss: 0.9096\n",
      "Epoch 131, Training Loss: 0.6962, Validation Loss: 0.3986, Test Loss: 0.8763\n",
      "Epoch 141, Training Loss: 0.6768, Validation Loss: 0.3900, Test Loss: 0.8528\n",
      "Epoch 151, Training Loss: 0.6611, Validation Loss: 0.3829, Test Loss: 0.8286\n",
      "Epoch 161, Training Loss: 0.6488, Validation Loss: 0.3780, Test Loss: 0.8115\n",
      "Epoch 171, Training Loss: 0.6386, Validation Loss: 0.3745, Test Loss: 0.7929\n",
      "Epoch 181, Training Loss: 0.6300, Validation Loss: 0.3711, Test Loss: 0.7792\n",
      "Epoch 191, Training Loss: 0.6221, Validation Loss: 0.3681, Test Loss: 0.7696\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.6221\n",
      "Validation loss: 0.3681\n",
      "Test loss: 0.7696\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 138/243 with parameters:\n",
      "Config indices: (1, 2, 0, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 3547.9260, Validation Loss: 3123.7102, Test Loss: 3578.0227\n",
      "Epoch 11, Training Loss: 156.2729, Validation Loss: 154.0379, Test Loss: 158.5275\n",
      "Epoch 21, Training Loss: 47.9259, Validation Loss: 46.1395, Test Loss: 47.4286\n",
      "Epoch 31, Training Loss: 18.3569, Validation Loss: 16.7060, Test Loss: 18.2929\n",
      "Epoch 41, Training Loss: 9.1252, Validation Loss: 7.9472, Test Loss: 9.4049\n",
      "Epoch 51, Training Loss: 3.8765, Validation Loss: 3.0574, Test Loss: 4.2733\n",
      "Epoch 61, Training Loss: 2.0573, Validation Loss: 1.4793, Test Loss: 2.5227\n",
      "Epoch 71, Training Loss: 1.3886, Validation Loss: 0.9336, Test Loss: 1.8778\n",
      "Epoch 81, Training Loss: 1.0965, Validation Loss: 0.7229, Test Loss: 1.5676\n",
      "Epoch 91, Training Loss: 0.9450, Validation Loss: 0.6306, Test Loss: 1.3858\n",
      "Epoch 101, Training Loss: 0.8518, Validation Loss: 0.5725, Test Loss: 1.2616\n",
      "Epoch 111, Training Loss: 0.7905, Validation Loss: 0.5401, Test Loss: 1.1759\n",
      "Epoch 121, Training Loss: 0.7492, Validation Loss: 0.5162, Test Loss: 1.1048\n",
      "Epoch 131, Training Loss: 0.7179, Validation Loss: 0.4953, Test Loss: 1.0524\n",
      "Epoch 141, Training Loss: 0.6917, Validation Loss: 0.4786, Test Loss: 1.0120\n",
      "Epoch 151, Training Loss: 0.6754, Validation Loss: 0.4686, Test Loss: 0.9901\n",
      "Epoch 161, Training Loss: 0.6547, Validation Loss: 0.4524, Test Loss: 0.9415\n",
      "Epoch 171, Training Loss: 0.6397, Validation Loss: 0.4409, Test Loss: 0.9198\n",
      "Epoch 181, Training Loss: 0.6262, Validation Loss: 0.4338, Test Loss: 0.8928\n",
      "Epoch 191, Training Loss: 0.6150, Validation Loss: 0.4251, Test Loss: 0.8738\n",
      "Epoch 201, Training Loss: 0.6044, Validation Loss: 0.4189, Test Loss: 0.8500\n",
      "Epoch 211, Training Loss: 0.5978, Validation Loss: 0.4134, Test Loss: 0.8448\n",
      "Epoch 221, Training Loss: 0.5865, Validation Loss: 0.4048, Test Loss: 0.8181\n",
      "Epoch 231, Training Loss: 0.5791, Validation Loss: 0.3988, Test Loss: 0.8057\n",
      "Epoch 241, Training Loss: 0.5721, Validation Loss: 0.3939, Test Loss: 0.7942\n",
      "Epoch 251, Training Loss: 0.5665, Validation Loss: 0.3905, Test Loss: 0.7863\n",
      "Epoch 261, Training Loss: 0.5603, Validation Loss: 0.3854, Test Loss: 0.7662\n",
      "Epoch 271, Training Loss: 0.5541, Validation Loss: 0.3818, Test Loss: 0.7579\n",
      "Epoch 281, Training Loss: 0.5499, Validation Loss: 0.3796, Test Loss: 0.7481\n",
      "Epoch 291, Training Loss: 0.5457, Validation Loss: 0.3760, Test Loss: 0.7388\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.5457\n",
      "Validation loss: 0.3760\n",
      "Test loss: 0.7388\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 139/243 with parameters:\n",
      "Config indices: (1, 2, 0, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 5448.8457, Validation Loss: 4919.8359, Test Loss: 5474.9297\n",
      "Epoch 11, Training Loss: 274.2343, Validation Loss: 272.3781, Test Loss: 287.6502\n",
      "Epoch 21, Training Loss: 111.2822, Validation Loss: 110.9264, Test Loss: 110.0401\n",
      "Epoch 31, Training Loss: 43.6398, Validation Loss: 42.4039, Test Loss: 42.1807\n",
      "Epoch 41, Training Loss: 19.7698, Validation Loss: 18.5183, Test Loss: 19.1078\n",
      "Epoch 51, Training Loss: 10.5374, Validation Loss: 9.5532, Test Loss: 10.4622\n",
      "Epoch 61, Training Loss: 6.2185, Validation Loss: 5.3390, Test Loss: 6.4489\n",
      "Epoch 71, Training Loss: 3.8892, Validation Loss: 3.1104, Test Loss: 4.2726\n",
      "Epoch 81, Training Loss: 2.6915, Validation Loss: 2.0231, Test Loss: 3.1743\n",
      "Epoch 91, Training Loss: 2.0533, Validation Loss: 1.4429, Test Loss: 2.5633\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 2.0533\n",
      "Validation loss: 1.4429\n",
      "Test loss: 2.5633\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 140/243 with parameters:\n",
      "Config indices: (1, 2, 0, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6293.0811, Validation Loss: 5720.1855, Test Loss: 6308.2456\n",
      "Epoch 11, Training Loss: 307.9646, Validation Loss: 303.4767, Test Loss: 324.2327\n",
      "Epoch 21, Training Loss: 139.7615, Validation Loss: 140.2672, Test Loss: 138.3842\n",
      "Epoch 31, Training Loss: 72.3315, Validation Loss: 72.3791, Test Loss: 68.4983\n",
      "Epoch 41, Training Loss: 36.6726, Validation Loss: 35.1190, Test Loss: 34.2417\n",
      "Epoch 51, Training Loss: 18.9494, Validation Loss: 17.1457, Test Loss: 17.9028\n",
      "Epoch 61, Training Loss: 11.4184, Validation Loss: 9.9194, Test Loss: 11.1248\n",
      "Epoch 71, Training Loss: 7.7741, Validation Loss: 6.4835, Test Loss: 7.8721\n",
      "Epoch 81, Training Loss: 5.6714, Validation Loss: 4.5386, Test Loss: 5.9509\n",
      "Epoch 91, Training Loss: 2.8596, Validation Loss: 2.2645, Test Loss: 3.2782\n",
      "Epoch 101, Training Loss: 1.8104, Validation Loss: 1.5082, Test Loss: 2.1697\n",
      "Epoch 111, Training Loss: 1.5150, Validation Loss: 1.3149, Test Loss: 1.8439\n",
      "Epoch 121, Training Loss: 1.3516, Validation Loss: 1.1814, Test Loss: 1.6709\n",
      "Epoch 131, Training Loss: 1.2257, Validation Loss: 1.0570, Test Loss: 1.5329\n",
      "Epoch 141, Training Loss: 1.1213, Validation Loss: 0.9399, Test Loss: 1.4335\n",
      "Epoch 151, Training Loss: 1.0353, Validation Loss: 0.8445, Test Loss: 1.3396\n",
      "Epoch 161, Training Loss: 0.9605, Validation Loss: 0.7689, Test Loss: 1.2691\n",
      "Epoch 171, Training Loss: 0.9019, Validation Loss: 0.7083, Test Loss: 1.2036\n",
      "Epoch 181, Training Loss: 0.8538, Validation Loss: 0.6612, Test Loss: 1.1523\n",
      "Epoch 191, Training Loss: 0.8118, Validation Loss: 0.6114, Test Loss: 1.1076\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.8118\n",
      "Validation loss: 0.6114\n",
      "Test loss: 1.1076\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 141/243 with parameters:\n",
      "Config indices: (1, 2, 0, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6109.4854, Validation Loss: 5597.1904, Test Loss: 6130.6099\n",
      "Epoch 11, Training Loss: 529.3024, Validation Loss: 514.2823, Test Loss: 548.9790\n",
      "Epoch 21, Training Loss: 168.5488, Validation Loss: 160.6774, Test Loss: 162.5441\n",
      "Epoch 31, Training Loss: 47.7638, Validation Loss: 43.0325, Test Loss: 45.0196\n",
      "Epoch 41, Training Loss: 19.9090, Validation Loss: 16.9784, Test Loss: 19.1119\n",
      "Epoch 51, Training Loss: 10.1963, Validation Loss: 8.1202, Test Loss: 10.1873\n",
      "Epoch 61, Training Loss: 6.0574, Validation Loss: 4.4750, Test Loss: 6.3707\n",
      "Epoch 71, Training Loss: 4.1058, Validation Loss: 2.8287, Test Loss: 4.5711\n",
      "Epoch 81, Training Loss: 3.0678, Validation Loss: 1.9947, Test Loss: 3.5994\n",
      "Epoch 91, Training Loss: 2.4601, Validation Loss: 1.5292, Test Loss: 3.0024\n",
      "Epoch 101, Training Loss: 2.0530, Validation Loss: 1.2367, Test Loss: 2.5943\n",
      "Epoch 111, Training Loss: 1.7656, Validation Loss: 1.0366, Test Loss: 2.2979\n",
      "Epoch 121, Training Loss: 1.5515, Validation Loss: 0.8876, Test Loss: 2.0755\n",
      "Epoch 131, Training Loss: 1.3867, Validation Loss: 0.7751, Test Loss: 1.9011\n",
      "Epoch 141, Training Loss: 1.2593, Validation Loss: 0.6927, Test Loss: 1.7622\n",
      "Epoch 151, Training Loss: 1.1594, Validation Loss: 0.6268, Test Loss: 1.6494\n",
      "Epoch 161, Training Loss: 1.0793, Validation Loss: 0.5780, Test Loss: 1.5592\n",
      "Epoch 171, Training Loss: 1.0148, Validation Loss: 0.5398, Test Loss: 1.4870\n",
      "Epoch 181, Training Loss: 0.9579, Validation Loss: 0.5082, Test Loss: 1.4160\n",
      "Epoch 191, Training Loss: 0.9115, Validation Loss: 0.4820, Test Loss: 1.3647\n",
      "Epoch 201, Training Loss: 0.8697, Validation Loss: 0.4602, Test Loss: 1.3081\n",
      "Epoch 211, Training Loss: 0.8346, Validation Loss: 0.4432, Test Loss: 1.2613\n",
      "Epoch 221, Training Loss: 0.8038, Validation Loss: 0.4278, Test Loss: 1.2240\n",
      "Epoch 231, Training Loss: 0.7769, Validation Loss: 0.4144, Test Loss: 1.1918\n",
      "Epoch 241, Training Loss: 0.7531, Validation Loss: 0.4038, Test Loss: 1.1592\n",
      "Epoch 251, Training Loss: 0.7324, Validation Loss: 0.3947, Test Loss: 1.1322\n",
      "Epoch 261, Training Loss: 0.7133, Validation Loss: 0.3862, Test Loss: 1.1034\n",
      "Epoch 271, Training Loss: 0.6964, Validation Loss: 0.3797, Test Loss: 1.0737\n",
      "Epoch 281, Training Loss: 0.6811, Validation Loss: 0.3733, Test Loss: 1.0510\n",
      "Epoch 291, Training Loss: 0.6667, Validation Loss: 0.3653, Test Loss: 1.0354\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.6667\n",
      "Validation loss: 0.3653\n",
      "Test loss: 1.0354\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 142/243 with parameters:\n",
      "Config indices: (1, 2, 0, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6911.8491, Validation Loss: 6315.1235, Test Loss: 6921.1489\n",
      "Epoch 11, Training Loss: 629.7333, Validation Loss: 598.0494, Test Loss: 675.6045\n",
      "Epoch 21, Training Loss: 356.4705, Validation Loss: 350.8723, Test Loss: 376.7013\n",
      "Epoch 31, Training Loss: 209.0543, Validation Loss: 209.6147, Test Loss: 212.8605\n",
      "Epoch 41, Training Loss: 132.5383, Validation Loss: 133.2544, Test Loss: 130.4444\n",
      "Epoch 51, Training Loss: 84.2512, Validation Loss: 83.8139, Test Loss: 80.6942\n",
      "Epoch 61, Training Loss: 51.9076, Validation Loss: 50.8242, Test Loss: 49.3150\n",
      "Epoch 71, Training Loss: 31.5734, Validation Loss: 30.1937, Test Loss: 30.0937\n",
      "Epoch 81, Training Loss: 19.5820, Validation Loss: 18.1528, Test Loss: 18.9646\n",
      "Epoch 91, Training Loss: 12.7725, Validation Loss: 11.4062, Test Loss: 12.6844\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 12.7725\n",
      "Validation loss: 11.4062\n",
      "Test loss: 12.6844\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 143/243 with parameters:\n",
      "Config indices: (1, 2, 0, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6885.8848, Validation Loss: 6293.3022, Test Loss: 6895.1084\n",
      "Epoch 11, Training Loss: 597.4174, Validation Loss: 567.9386, Test Loss: 640.5492\n",
      "Epoch 21, Training Loss: 328.8840, Validation Loss: 318.1182, Test Loss: 343.6483\n",
      "Epoch 31, Training Loss: 168.8645, Validation Loss: 165.5537, Test Loss: 169.4792\n",
      "Epoch 41, Training Loss: 97.2953, Validation Loss: 94.1420, Test Loss: 93.9266\n",
      "Epoch 51, Training Loss: 58.1641, Validation Loss: 54.7990, Test Loss: 55.5601\n",
      "Epoch 61, Training Loss: 34.7584, Validation Loss: 31.8586, Test Loss: 33.3312\n",
      "Epoch 71, Training Loss: 21.4785, Validation Loss: 18.8747, Test Loss: 20.7142\n",
      "Epoch 81, Training Loss: 14.5774, Validation Loss: 12.3062, Test Loss: 14.4033\n",
      "Epoch 91, Training Loss: 10.6333, Validation Loss: 8.4995, Test Loss: 10.7295\n",
      "Epoch 101, Training Loss: 8.2604, Validation Loss: 6.4113, Test Loss: 8.5509\n",
      "Epoch 111, Training Loss: 6.7341, Validation Loss: 5.0886, Test Loss: 7.1313\n",
      "Epoch 121, Training Loss: 5.6628, Validation Loss: 4.1716, Test Loss: 6.0946\n",
      "Epoch 131, Training Loss: 4.8431, Validation Loss: 3.4913, Test Loss: 5.3050\n",
      "Epoch 141, Training Loss: 4.2303, Validation Loss: 2.9606, Test Loss: 4.6822\n",
      "Epoch 151, Training Loss: 3.7456, Validation Loss: 2.5808, Test Loss: 4.2256\n",
      "Epoch 161, Training Loss: 3.3596, Validation Loss: 2.2605, Test Loss: 3.8319\n",
      "Epoch 171, Training Loss: 3.0336, Validation Loss: 2.0047, Test Loss: 3.5119\n",
      "Epoch 181, Training Loss: 2.7599, Validation Loss: 1.7947, Test Loss: 3.2348\n",
      "Epoch 191, Training Loss: 2.5328, Validation Loss: 1.6227, Test Loss: 2.9980\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 2.5328\n",
      "Validation loss: 1.6227\n",
      "Test loss: 2.9980\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 144/243 with parameters:\n",
      "Config indices: (1, 2, 0, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6996.9141, Validation Loss: 6390.6626, Test Loss: 7005.3203\n",
      "Epoch 11, Training Loss: 747.5905, Validation Loss: 694.2659, Test Loss: 782.8926\n",
      "Epoch 21, Training Loss: 398.9425, Validation Loss: 397.9569, Test Loss: 425.8721\n",
      "Epoch 31, Training Loss: 262.5387, Validation Loss: 268.1105, Test Loss: 273.8946\n",
      "Epoch 41, Training Loss: 199.0968, Validation Loss: 205.0461, Test Loss: 202.0115\n",
      "Epoch 51, Training Loss: 151.8327, Validation Loss: 157.1358, Test Loss: 149.9894\n",
      "Epoch 61, Training Loss: 114.9914, Validation Loss: 119.5721, Test Loss: 111.1365\n",
      "Epoch 71, Training Loss: 86.6966, Validation Loss: 90.1550, Test Loss: 81.8635\n",
      "Epoch 81, Training Loss: 64.8024, Validation Loss: 67.0544, Test Loss: 60.2301\n",
      "Epoch 91, Training Loss: 47.9297, Validation Loss: 49.4522, Test Loss: 44.1808\n",
      "Epoch 101, Training Loss: 35.0614, Validation Loss: 35.7226, Test Loss: 32.0115\n",
      "Epoch 111, Training Loss: 25.5842, Validation Loss: 25.7626, Test Loss: 23.2439\n",
      "Epoch 121, Training Loss: 18.8389, Validation Loss: 18.7799, Test Loss: 17.0963\n",
      "Epoch 131, Training Loss: 14.2760, Validation Loss: 14.1410, Test Loss: 13.0185\n",
      "Epoch 141, Training Loss: 11.3039, Validation Loss: 11.1816, Test Loss: 10.4280\n",
      "Epoch 151, Training Loss: 9.1303, Validation Loss: 8.9154, Test Loss: 8.4748\n",
      "Epoch 161, Training Loss: 7.4440, Validation Loss: 7.1723, Test Loss: 6.9702\n",
      "Epoch 171, Training Loss: 6.1070, Validation Loss: 5.8019, Test Loss: 5.7636\n",
      "Epoch 181, Training Loss: 5.0323, Validation Loss: 4.7004, Test Loss: 4.7824\n",
      "Epoch 191, Training Loss: 4.1684, Validation Loss: 3.8170, Test Loss: 4.0082\n",
      "Epoch 201, Training Loss: 3.4785, Validation Loss: 3.1005, Test Loss: 3.3781\n",
      "Epoch 211, Training Loss: 2.9268, Validation Loss: 2.5459, Test Loss: 2.8866\n",
      "Epoch 221, Training Loss: 2.4879, Validation Loss: 2.1142, Test Loss: 2.5036\n",
      "Epoch 231, Training Loss: 2.1434, Validation Loss: 1.7707, Test Loss: 2.1816\n",
      "Epoch 241, Training Loss: 1.8702, Validation Loss: 1.5117, Test Loss: 1.9465\n",
      "Epoch 251, Training Loss: 1.6593, Validation Loss: 1.3188, Test Loss: 1.7565\n",
      "Epoch 261, Training Loss: 1.4974, Validation Loss: 1.1789, Test Loss: 1.6085\n",
      "Epoch 271, Training Loss: 1.3755, Validation Loss: 1.0713, Test Loss: 1.5005\n",
      "Epoch 281, Training Loss: 1.2784, Validation Loss: 0.9881, Test Loss: 1.4200\n",
      "Epoch 291, Training Loss: 1.1994, Validation Loss: 0.9208, Test Loss: 1.3502\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 1.1994\n",
      "Validation loss: 0.9208\n",
      "Test loss: 1.3502\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 145/243 with parameters:\n",
      "Config indices: (1, 2, 1, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7160.4409, Validation Loss: 6549.7432, Test Loss: 7167.2363\n",
      "Epoch 11, Training Loss: 7074.7466, Validation Loss: 6465.0659, Test Loss: 7082.0815\n",
      "Epoch 21, Training Loss: 6892.3667, Validation Loss: 6282.9868, Test Loss: 6901.4092\n",
      "Epoch 31, Training Loss: 6617.7280, Validation Loss: 6009.3765, Test Loss: 6629.4102\n",
      "Epoch 41, Training Loss: 6270.4146, Validation Loss: 5664.8521, Test Loss: 6284.9873\n",
      "Epoch 51, Training Loss: 5871.7710, Validation Loss: 5272.1255, Test Loss: 5888.7466\n",
      "Epoch 61, Training Loss: 5432.9707, Validation Loss: 4844.5449, Test Loss: 5451.3755\n",
      "Epoch 71, Training Loss: 4961.5132, Validation Loss: 4392.3872, Test Loss: 4979.9985\n",
      "Epoch 81, Training Loss: 4495.5356, Validation Loss: 3952.6787, Test Loss: 4512.4385\n",
      "Epoch 91, Training Loss: 4051.4043, Validation Loss: 3541.2793, Test Loss: 4065.3862\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 4051.4043\n",
      "Validation loss: 3541.2793\n",
      "Test loss: 4065.3862\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 146/243 with parameters:\n",
      "Config indices: (1, 2, 1, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7160.5972, Validation Loss: 6549.8911, Test Loss: 7167.4077\n",
      "Epoch 11, Training Loss: 7075.4717, Validation Loss: 6466.4316, Test Loss: 7082.9639\n",
      "Epoch 21, Training Loss: 6851.2305, Validation Loss: 6246.9712, Test Loss: 6861.2144\n",
      "Epoch 31, Training Loss: 6501.5259, Validation Loss: 5906.1411, Test Loss: 6515.4121\n",
      "Epoch 41, Training Loss: 6052.8198, Validation Loss: 5471.1934, Test Loss: 6071.2905\n",
      "Epoch 51, Training Loss: 5536.1479, Validation Loss: 4973.7905, Test Loss: 5559.0225\n",
      "Epoch 61, Training Loss: 4983.1758, Validation Loss: 4446.0898, Test Loss: 5009.5176\n",
      "Epoch 71, Training Loss: 4425.3457, Validation Loss: 3919.6353, Test Loss: 4453.6274\n",
      "Epoch 81, Training Loss: 3888.5583, Validation Loss: 3419.8645, Test Loss: 3917.0935\n",
      "Epoch 91, Training Loss: 3391.1641, Validation Loss: 2964.0208, Test Loss: 3418.5396\n",
      "Epoch 101, Training Loss: 2942.5881, Validation Loss: 2559.8467, Test Loss: 2968.0103\n",
      "Epoch 111, Training Loss: 2544.9363, Validation Loss: 2207.3723, Test Loss: 2568.4177\n",
      "Epoch 121, Training Loss: 2196.3096, Validation Loss: 1902.7073, Test Loss: 2218.4436\n",
      "Epoch 131, Training Loss: 1893.8945, Validation Loss: 1641.4862, Test Loss: 1915.5262\n",
      "Epoch 141, Training Loss: 1635.0411, Validation Loss: 1420.0066, Test Loss: 1656.9767\n",
      "Epoch 151, Training Loss: 1417.1025, Validation Loss: 1234.9736, Test Loss: 1439.8643\n",
      "Epoch 161, Training Loss: 1236.9667, Validation Loss: 1083.3441, Test Loss: 1260.9160\n",
      "Epoch 171, Training Loss: 1090.6370, Validation Loss: 961.3076, Test Loss: 1116.0315\n",
      "Epoch 181, Training Loss: 973.8561, Validation Loss: 864.8201, Test Loss: 1000.6814\n",
      "Epoch 191, Training Loss: 881.9112, Validation Loss: 789.5981, Test Loss: 909.9906\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 881.9112\n",
      "Validation loss: 789.5981\n",
      "Test loss: 909.9906\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 147/243 with parameters:\n",
      "Config indices: (1, 2, 1, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7160.2871, Validation Loss: 6549.6709, Test Loss: 7167.0957\n",
      "Epoch 11, Training Loss: 7056.6704, Validation Loss: 6448.6787, Test Loss: 7064.3457\n",
      "Epoch 21, Training Loss: 6841.0356, Validation Loss: 6236.7104, Test Loss: 6851.0791\n",
      "Epoch 31, Training Loss: 6522.3281, Validation Loss: 5922.8447, Test Loss: 6535.8179\n",
      "Epoch 41, Training Loss: 6116.4829, Validation Loss: 5524.2373, Test Loss: 6133.8110\n",
      "Epoch 51, Training Loss: 5652.9531, Validation Loss: 5071.7002, Test Loss: 5673.6675\n",
      "Epoch 61, Training Loss: 5161.4092, Validation Loss: 4596.1548, Test Loss: 5184.2515\n",
      "Epoch 71, Training Loss: 4668.5552, Validation Loss: 4125.2573, Test Loss: 4691.7852\n",
      "Epoch 81, Training Loss: 4195.4912, Validation Loss: 3680.4004, Test Loss: 4217.3364\n",
      "Epoch 91, Training Loss: 3755.5312, Validation Loss: 3274.2751, Test Loss: 3774.6768\n",
      "Epoch 101, Training Loss: 3353.2529, Validation Loss: 2910.1362, Test Loss: 3369.1909\n",
      "Epoch 111, Training Loss: 2986.2871, Validation Loss: 2583.8279, Test Loss: 2999.4692\n",
      "Epoch 121, Training Loss: 2650.4009, Validation Loss: 2289.2034, Test Loss: 2661.7183\n",
      "Epoch 131, Training Loss: 2342.4539, Validation Loss: 2021.6713, Test Loss: 2352.8733\n",
      "Epoch 141, Training Loss: 2061.5398, Validation Loss: 1779.1400, Test Loss: 2072.1694\n",
      "Epoch 151, Training Loss: 1808.0754, Validation Loss: 1561.2837, Test Loss: 1819.7888\n",
      "Epoch 161, Training Loss: 1582.3973, Validation Loss: 1368.1857, Test Loss: 1595.8369\n",
      "Epoch 171, Training Loss: 1384.5436, Validation Loss: 1199.5865, Test Loss: 1399.9833\n",
      "Epoch 181, Training Loss: 1213.8785, Validation Loss: 1054.6954, Test Loss: 1231.3125\n",
      "Epoch 191, Training Loss: 1068.6428, Validation Loss: 932.0457, Test Loss: 1088.1022\n",
      "Epoch 201, Training Loss: 946.7936, Validation Loss: 829.6645, Test Loss: 968.1581\n",
      "Epoch 211, Training Loss: 845.5781, Validation Loss: 745.1663, Test Loss: 868.7767\n",
      "Epoch 221, Training Loss: 762.1733, Validation Loss: 676.0688, Test Loss: 787.0447\n",
      "Epoch 231, Training Loss: 693.8809, Validation Loss: 619.8207, Test Loss: 720.2117\n",
      "Epoch 241, Training Loss: 638.0698, Validation Loss: 574.2353, Test Loss: 665.6707\n",
      "Epoch 251, Training Loss: 592.2342, Validation Loss: 537.1387, Test Loss: 621.0084\n",
      "Epoch 261, Training Loss: 554.3712, Validation Loss: 506.7473, Test Loss: 584.0665\n",
      "Epoch 271, Training Loss: 522.7672, Validation Loss: 481.3528, Test Loss: 553.0928\n",
      "Epoch 281, Training Loss: 496.0688, Validation Loss: 459.8437, Test Loss: 526.8254\n",
      "Epoch 291, Training Loss: 473.0573, Validation Loss: 441.2220, Test Loss: 504.0900\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 473.0573\n",
      "Validation loss: 441.2220\n",
      "Test loss: 504.0900\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 148/243 with parameters:\n",
      "Config indices: (1, 2, 1, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7162.9839, Validation Loss: 6552.2070, Test Loss: 7169.7998\n",
      "Epoch 11, Training Loss: 7131.4302, Validation Loss: 6522.4497, Test Loss: 7138.4258\n",
      "Epoch 21, Training Loss: 7075.9839, Validation Loss: 6470.5205, Test Loss: 7083.5254\n",
      "Epoch 31, Training Loss: 6991.3569, Validation Loss: 6391.2900, Test Loss: 6999.8633\n",
      "Epoch 41, Training Loss: 6878.3037, Validation Loss: 6285.4561, Test Loss: 6888.1899\n",
      "Epoch 51, Training Loss: 6736.0420, Validation Loss: 6152.5020, Test Loss: 6747.7515\n",
      "Epoch 61, Training Loss: 6566.2754, Validation Loss: 5994.0234, Test Loss: 6580.2061\n",
      "Epoch 71, Training Loss: 6372.2744, Validation Loss: 5813.0400, Test Loss: 6388.7705\n",
      "Epoch 81, Training Loss: 6156.3159, Validation Loss: 5611.7686, Test Loss: 6175.6885\n",
      "Epoch 91, Training Loss: 5921.3389, Validation Loss: 5392.9648, Test Loss: 5943.8228\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 5921.3389\n",
      "Validation loss: 5392.9648\n",
      "Test loss: 5943.8228\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 149/243 with parameters:\n",
      "Config indices: (1, 2, 1, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7160.9619, Validation Loss: 6550.4604, Test Loss: 7167.8018\n",
      "Epoch 11, Training Loss: 7114.4595, Validation Loss: 6507.7705, Test Loss: 7121.6528\n",
      "Epoch 21, Training Loss: 7038.7563, Validation Loss: 6438.6343, Test Loss: 7046.7354\n",
      "Epoch 31, Training Loss: 6930.8408, Validation Loss: 6340.0098, Test Loss: 6940.1011\n",
      "Epoch 41, Training Loss: 6789.1094, Validation Loss: 6210.3735, Test Loss: 6800.1914\n",
      "Epoch 51, Training Loss: 6617.3931, Validation Loss: 6053.3672, Test Loss: 6630.7896\n",
      "Epoch 61, Training Loss: 6418.5015, Validation Loss: 5871.5557, Test Loss: 6434.6479\n",
      "Epoch 71, Training Loss: 6195.6528, Validation Loss: 5667.8945, Test Loss: 6214.9512\n",
      "Epoch 81, Training Loss: 5951.3701, Validation Loss: 5444.7300, Test Loss: 5974.1528\n",
      "Epoch 91, Training Loss: 5689.0796, Validation Loss: 5205.1826, Test Loss: 5715.6152\n",
      "Epoch 101, Training Loss: 5411.8833, Validation Loss: 4952.1167, Test Loss: 5442.4077\n",
      "Epoch 111, Training Loss: 5123.4800, Validation Loss: 4688.9204, Test Loss: 5158.1616\n",
      "Epoch 121, Training Loss: 4827.7466, Validation Loss: 4419.0620, Test Loss: 4866.6641\n",
      "Epoch 131, Training Loss: 4527.8091, Validation Loss: 4145.4263, Test Loss: 4570.9966\n",
      "Epoch 141, Training Loss: 4228.2432, Validation Loss: 3872.1948, Test Loss: 4275.6484\n",
      "Epoch 151, Training Loss: 3931.0366, Validation Loss: 3601.2566, Test Loss: 3982.5730\n",
      "Epoch 161, Training Loss: 3640.2983, Validation Loss: 3336.3093, Test Loss: 3695.8037\n",
      "Epoch 171, Training Loss: 3357.9409, Validation Loss: 3079.1663, Test Loss: 3417.2297\n",
      "Epoch 181, Training Loss: 3087.5999, Validation Loss: 2833.0466, Test Loss: 3150.3916\n",
      "Epoch 191, Training Loss: 2831.3694, Validation Loss: 2599.9116, Test Loss: 2897.3613\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 2831.3694\n",
      "Validation loss: 2599.9116\n",
      "Test loss: 2897.3613\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 150/243 with parameters:\n",
      "Config indices: (1, 2, 1, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7162.0220, Validation Loss: 6551.3071, Test Loss: 7168.8589\n",
      "Epoch 11, Training Loss: 7125.9609, Validation Loss: 6516.1250, Test Loss: 7132.9609\n",
      "Epoch 21, Training Loss: 7068.0156, Validation Loss: 6458.8765, Test Loss: 7075.5098\n",
      "Epoch 31, Training Loss: 6984.3955, Validation Loss: 6375.7314, Test Loss: 6992.6997\n",
      "Epoch 41, Training Loss: 6873.2637, Validation Loss: 6264.8989, Test Loss: 6882.6665\n",
      "Epoch 51, Training Loss: 6734.3774, Validation Loss: 6126.0903, Test Loss: 6745.0933\n",
      "Epoch 61, Training Loss: 6569.5122, Validation Loss: 5961.0723, Test Loss: 6581.6245\n",
      "Epoch 71, Training Loss: 6380.3345, Validation Loss: 5772.3271, Test Loss: 6393.9502\n",
      "Epoch 81, Training Loss: 6162.5371, Validation Loss: 5556.8535, Test Loss: 6177.6382\n",
      "Epoch 91, Training Loss: 5929.3491, Validation Loss: 5327.0264, Test Loss: 5945.6387\n",
      "Epoch 101, Training Loss: 5685.2900, Validation Loss: 5087.8745, Test Loss: 5702.3159\n",
      "Epoch 111, Training Loss: 5435.4097, Validation Loss: 4844.7104, Test Loss: 5452.6416\n",
      "Epoch 121, Training Loss: 5183.8613, Validation Loss: 4601.9756, Test Loss: 5200.6938\n",
      "Epoch 131, Training Loss: 4934.5972, Validation Loss: 4363.7368, Test Loss: 4950.3984\n",
      "Epoch 141, Training Loss: 4690.7656, Validation Loss: 4133.2163, Test Loss: 4704.9331\n",
      "Epoch 151, Training Loss: 4454.6201, Validation Loss: 3912.5808, Test Loss: 4466.6416\n",
      "Epoch 161, Training Loss: 4227.1987, Validation Loss: 3702.7292, Test Loss: 4236.7407\n",
      "Epoch 171, Training Loss: 4009.0938, Validation Loss: 3503.9746, Test Loss: 4016.0078\n",
      "Epoch 181, Training Loss: 3799.8154, Validation Loss: 3315.5166, Test Loss: 3804.1572\n",
      "Epoch 191, Training Loss: 3598.4717, Validation Loss: 3136.0957, Test Loss: 3600.4993\n",
      "Epoch 201, Training Loss: 3403.5681, Validation Loss: 2963.9092, Test Loss: 3403.6731\n",
      "Epoch 211, Training Loss: 3214.9705, Validation Loss: 2798.4736, Test Loss: 3213.6116\n",
      "Epoch 221, Training Loss: 3031.4062, Validation Loss: 2638.2732, Test Loss: 3029.0693\n",
      "Epoch 231, Training Loss: 2853.5791, Validation Loss: 2483.6316, Test Loss: 2850.6802\n",
      "Epoch 241, Training Loss: 2680.7627, Validation Loss: 2333.7993, Test Loss: 2677.7341\n",
      "Epoch 251, Training Loss: 2513.4817, Validation Loss: 2189.0554, Test Loss: 2510.6750\n",
      "Epoch 261, Training Loss: 2351.7583, Validation Loss: 2049.2971, Test Loss: 2349.4292\n",
      "Epoch 271, Training Loss: 2196.5076, Validation Loss: 1915.2412, Test Loss: 2194.8940\n",
      "Epoch 281, Training Loss: 2047.8401, Validation Loss: 1787.0017, Test Loss: 2047.2122\n",
      "Epoch 291, Training Loss: 1905.8300, Validation Loss: 1664.6631, Test Loss: 1906.2291\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 1905.8300\n",
      "Validation loss: 1664.6631\n",
      "Test loss: 1906.2291\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 151/243 with parameters:\n",
      "Config indices: (1, 2, 1, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7164.0083, Validation Loss: 6553.1328, Test Loss: 7170.8350\n",
      "Epoch 11, Training Loss: 7145.2744, Validation Loss: 6534.6841, Test Loss: 7152.1606\n",
      "Epoch 21, Training Loss: 7116.1313, Validation Loss: 6505.3809, Test Loss: 7123.1772\n",
      "Epoch 31, Training Loss: 7073.9756, Validation Loss: 6462.8628, Test Loss: 7081.3574\n",
      "Epoch 41, Training Loss: 7019.2969, Validation Loss: 6407.7261, Test Loss: 7027.1543\n",
      "Epoch 51, Training Loss: 6951.0908, Validation Loss: 6339.1787, Test Loss: 6959.5806\n",
      "Epoch 61, Training Loss: 6868.1445, Validation Loss: 6256.0005, Test Loss: 6877.3999\n",
      "Epoch 71, Training Loss: 6772.2515, Validation Loss: 6160.0742, Test Loss: 6782.3882\n",
      "Epoch 81, Training Loss: 6662.9658, Validation Loss: 6050.9321, Test Loss: 6674.0723\n",
      "Epoch 91, Training Loss: 6541.7729, Validation Loss: 5930.1504, Test Loss: 6553.8770\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 6541.7729\n",
      "Validation loss: 5930.1504\n",
      "Test loss: 6553.8770\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 152/243 with parameters:\n",
      "Config indices: (1, 2, 1, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7164.3994, Validation Loss: 6553.5508, Test Loss: 7171.2300\n",
      "Epoch 11, Training Loss: 7150.3931, Validation Loss: 6540.2764, Test Loss: 7157.2529\n",
      "Epoch 21, Training Loss: 7132.1016, Validation Loss: 6523.0938, Test Loss: 7139.1006\n",
      "Epoch 31, Training Loss: 7106.1382, Validation Loss: 6498.8120, Test Loss: 7113.3535\n",
      "Epoch 41, Training Loss: 7071.2197, Validation Loss: 6466.3223, Test Loss: 7078.7944\n",
      "Epoch 51, Training Loss: 7028.3149, Validation Loss: 6426.4902, Test Loss: 7036.3872\n",
      "Epoch 61, Training Loss: 6975.9756, Validation Loss: 6377.9917, Test Loss: 6984.6777\n",
      "Epoch 71, Training Loss: 6914.6841, Validation Loss: 6321.3130, Test Loss: 6924.1670\n",
      "Epoch 81, Training Loss: 6845.9546, Validation Loss: 6257.8232, Test Loss: 6856.3408\n",
      "Epoch 91, Training Loss: 6769.2329, Validation Loss: 6187.0269, Test Loss: 6780.6479\n",
      "Epoch 101, Training Loss: 6685.2378, Validation Loss: 6109.5791, Test Loss: 6697.8022\n",
      "Epoch 111, Training Loss: 6593.8911, Validation Loss: 6025.4126, Test Loss: 6607.7222\n",
      "Epoch 121, Training Loss: 6496.0962, Validation Loss: 5935.3604, Test Loss: 6511.3018\n",
      "Epoch 131, Training Loss: 6391.4004, Validation Loss: 5839.0107, Test Loss: 6408.0820\n",
      "Epoch 141, Training Loss: 6280.7012, Validation Loss: 5737.1855, Test Loss: 6298.9448\n",
      "Epoch 151, Training Loss: 6164.3931, Validation Loss: 5630.2344, Test Loss: 6184.2788\n",
      "Epoch 161, Training Loss: 6043.0010, Validation Loss: 5518.6675, Test Loss: 6064.6089\n",
      "Epoch 171, Training Loss: 5916.3872, Validation Loss: 5402.3672, Test Loss: 5939.8022\n",
      "Epoch 181, Training Loss: 5785.5420, Validation Loss: 5282.2173, Test Loss: 5810.8213\n",
      "Epoch 191, Training Loss: 5650.3574, Validation Loss: 5158.1392, Test Loss: 5677.5601\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 5650.3574\n",
      "Validation loss: 5158.1392\n",
      "Test loss: 5677.5601\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 153/243 with parameters:\n",
      "Config indices: (1, 2, 1, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7163.2871, Validation Loss: 6552.5430, Test Loss: 7170.1064\n",
      "Epoch 11, Training Loss: 7141.7256, Validation Loss: 6532.3091, Test Loss: 7148.6787\n",
      "Epoch 21, Training Loss: 7114.7222, Validation Loss: 6506.9194, Test Loss: 7121.9165\n",
      "Epoch 31, Training Loss: 7080.7046, Validation Loss: 6474.8652, Test Loss: 7088.2549\n",
      "Epoch 41, Training Loss: 7038.8911, Validation Loss: 6435.5176, Test Loss: 7046.9180\n",
      "Epoch 51, Training Loss: 6989.3931, Validation Loss: 6388.9844, Test Loss: 6998.0098\n",
      "Epoch 61, Training Loss: 6932.4497, Validation Loss: 6335.4370, Test Loss: 6941.7715\n",
      "Epoch 71, Training Loss: 6868.0229, Validation Loss: 6274.7974, Test Loss: 6878.1602\n",
      "Epoch 81, Training Loss: 6795.8960, Validation Loss: 6206.9243, Test Loss: 6806.9858\n",
      "Epoch 91, Training Loss: 6717.0996, Validation Loss: 6132.7612, Test Loss: 6729.2417\n",
      "Epoch 101, Training Loss: 6631.1719, Validation Loss: 6051.9019, Test Loss: 6644.4561\n",
      "Epoch 111, Training Loss: 6539.5044, Validation Loss: 5965.6309, Test Loss: 6554.0146\n",
      "Epoch 121, Training Loss: 6441.4355, Validation Loss: 5873.3945, Test Loss: 6457.2622\n",
      "Epoch 131, Training Loss: 6337.8379, Validation Loss: 5776.0054, Test Loss: 6355.0508\n",
      "Epoch 141, Training Loss: 6228.6880, Validation Loss: 5673.4160, Test Loss: 6247.3491\n",
      "Epoch 151, Training Loss: 6114.1841, Validation Loss: 5565.8784, Test Loss: 6134.3574\n",
      "Epoch 161, Training Loss: 5995.3931, Validation Loss: 5454.3389, Test Loss: 6017.1074\n",
      "Epoch 171, Training Loss: 5872.8394, Validation Loss: 5339.3130, Test Loss: 5896.1260\n",
      "Epoch 181, Training Loss: 5745.9814, Validation Loss: 5220.3359, Test Loss: 5770.8740\n",
      "Epoch 191, Training Loss: 5615.5796, Validation Loss: 5098.1245, Test Loss: 5642.1045\n",
      "Epoch 201, Training Loss: 5482.0332, Validation Loss: 4973.0532, Test Loss: 5510.2095\n",
      "Epoch 211, Training Loss: 5345.6621, Validation Loss: 4845.4531, Test Loss: 5375.4937\n",
      "Epoch 221, Training Loss: 5207.3442, Validation Loss: 4716.1348, Test Loss: 5238.8203\n",
      "Epoch 231, Training Loss: 5067.0034, Validation Loss: 4585.0186, Test Loss: 5100.1108\n",
      "Epoch 241, Training Loss: 4925.6309, Validation Loss: 4453.0366, Test Loss: 4960.3384\n",
      "Epoch 251, Training Loss: 4784.0322, Validation Loss: 4320.9214, Test Loss: 4820.2930\n",
      "Epoch 261, Training Loss: 4641.1001, Validation Loss: 4187.7354, Test Loss: 4678.8901\n",
      "Epoch 271, Training Loss: 4497.0303, Validation Loss: 4053.6873, Test Loss: 4536.3198\n",
      "Epoch 281, Training Loss: 4353.8135, Validation Loss: 3920.5457, Test Loss: 4394.5366\n",
      "Epoch 291, Training Loss: 4211.5952, Validation Loss: 3788.4543, Test Loss: 4253.6802\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 4211.5952\n",
      "Validation loss: 3788.4543\n",
      "Test loss: 4253.6802\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 154/243 with parameters:\n",
      "Config indices: (1, 2, 2, 0, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.2622, Validation Loss: 6555.3896, Test Loss: 7173.0815\n",
      "Epoch 11, Training Loss: 7164.8545, Validation Loss: 6554.0591, Test Loss: 7171.6709\n",
      "Epoch 21, Training Loss: 7164.0181, Validation Loss: 6553.2690, Test Loss: 7170.8320\n",
      "Epoch 31, Training Loss: 7163.3125, Validation Loss: 6552.6040, Test Loss: 7170.1309\n",
      "Epoch 41, Training Loss: 7162.6641, Validation Loss: 6551.9873, Test Loss: 7169.4829\n",
      "Epoch 51, Training Loss: 7162.0454, Validation Loss: 6551.3975, Test Loss: 7168.8682\n",
      "Epoch 61, Training Loss: 7161.4604, Validation Loss: 6550.8403, Test Loss: 7168.2847\n",
      "Epoch 71, Training Loss: 7160.9009, Validation Loss: 6550.3086, Test Loss: 7167.7300\n",
      "Epoch 81, Training Loss: 7160.3604, Validation Loss: 6549.7959, Test Loss: 7167.1919\n",
      "Epoch 91, Training Loss: 7159.8330, Validation Loss: 6549.2944, Test Loss: 7166.6685\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 100\n",
      "Training loss: 7159.8330\n",
      "Validation loss: 6549.2944\n",
      "Test loss: 7166.6685\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 155/243 with parameters:\n",
      "Config indices: (1, 2, 2, 0, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.9771, Validation Loss: 6556.0620, Test Loss: 7173.7920\n",
      "Epoch 11, Training Loss: 7165.2539, Validation Loss: 6554.4229, Test Loss: 7172.0723\n",
      "Epoch 21, Training Loss: 7164.3862, Validation Loss: 6553.5923, Test Loss: 7171.2031\n",
      "Epoch 31, Training Loss: 7163.6792, Validation Loss: 6552.9180, Test Loss: 7170.4966\n",
      "Epoch 41, Training Loss: 7163.0430, Validation Loss: 6552.3086, Test Loss: 7169.8628\n",
      "Epoch 51, Training Loss: 7162.4482, Validation Loss: 6551.7402, Test Loss: 7169.2686\n",
      "Epoch 61, Training Loss: 7161.8813, Validation Loss: 6551.1992, Test Loss: 7168.7046\n",
      "Epoch 71, Training Loss: 7161.3408, Validation Loss: 6550.6807, Test Loss: 7168.1641\n",
      "Epoch 81, Training Loss: 7160.8164, Validation Loss: 6550.1792, Test Loss: 7167.6396\n",
      "Epoch 91, Training Loss: 7160.3037, Validation Loss: 6549.6895, Test Loss: 7167.1274\n",
      "Epoch 101, Training Loss: 7159.7988, Validation Loss: 6549.2056, Test Loss: 7166.6240\n",
      "Epoch 111, Training Loss: 7159.2954, Validation Loss: 6548.7266, Test Loss: 7166.1230\n",
      "Epoch 121, Training Loss: 7158.7969, Validation Loss: 6548.2485, Test Loss: 7165.6255\n",
      "Epoch 131, Training Loss: 7158.2979, Validation Loss: 6547.7686, Test Loss: 7165.1255\n",
      "Epoch 141, Training Loss: 7157.7930, Validation Loss: 6547.2876, Test Loss: 7164.6211\n",
      "Epoch 151, Training Loss: 7157.2842, Validation Loss: 6546.8008, Test Loss: 7164.1152\n",
      "Epoch 161, Training Loss: 7156.7729, Validation Loss: 6546.3105, Test Loss: 7163.6050\n",
      "Epoch 171, Training Loss: 7156.2622, Validation Loss: 6545.8198, Test Loss: 7163.0933\n",
      "Epoch 181, Training Loss: 7155.7451, Validation Loss: 6545.3271, Test Loss: 7162.5815\n",
      "Epoch 191, Training Loss: 7155.2290, Validation Loss: 6544.8320, Test Loss: 7162.0674\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 200\n",
      "Training loss: 7155.2290\n",
      "Validation loss: 6544.8320\n",
      "Test loss: 7162.0674\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 156/243 with parameters:\n",
      "Config indices: (1, 2, 2, 0, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.1411, Validation Loss: 6555.2925, Test Loss: 7172.9570\n",
      "Epoch 11, Training Loss: 7164.6216, Validation Loss: 6553.8745, Test Loss: 7171.4438\n",
      "Epoch 21, Training Loss: 7163.6646, Validation Loss: 6552.9702, Test Loss: 7170.4937\n",
      "Epoch 31, Training Loss: 7162.8687, Validation Loss: 6552.2188, Test Loss: 7169.7012\n",
      "Epoch 41, Training Loss: 7162.1621, Validation Loss: 6551.5518, Test Loss: 7168.9951\n",
      "Epoch 51, Training Loss: 7161.5244, Validation Loss: 6550.9482, Test Loss: 7168.3589\n",
      "Epoch 61, Training Loss: 7160.9316, Validation Loss: 6550.3862, Test Loss: 7167.7627\n",
      "Epoch 71, Training Loss: 7160.3604, Validation Loss: 6549.8467, Test Loss: 7167.1934\n",
      "Epoch 81, Training Loss: 7159.7993, Validation Loss: 6549.3223, Test Loss: 7166.6357\n",
      "Epoch 91, Training Loss: 7159.2515, Validation Loss: 6548.8037, Test Loss: 7166.0869\n",
      "Epoch 101, Training Loss: 7158.7031, Validation Loss: 6548.2876, Test Loss: 7165.5410\n",
      "Epoch 111, Training Loss: 7158.1592, Validation Loss: 6547.7734, Test Loss: 7164.9966\n",
      "Epoch 121, Training Loss: 7157.6104, Validation Loss: 6547.2559, Test Loss: 7164.4497\n",
      "Epoch 131, Training Loss: 7157.0615, Validation Loss: 6546.7373, Test Loss: 7163.9009\n",
      "Epoch 141, Training Loss: 7156.5088, Validation Loss: 6546.2139, Test Loss: 7163.3491\n",
      "Epoch 151, Training Loss: 7155.9487, Validation Loss: 6545.6851, Test Loss: 7162.7920\n",
      "Epoch 161, Training Loss: 7155.3833, Validation Loss: 6545.1484, Test Loss: 7162.2280\n",
      "Epoch 171, Training Loss: 7154.8135, Validation Loss: 6544.6079, Test Loss: 7161.6587\n",
      "Epoch 181, Training Loss: 7154.2368, Validation Loss: 6544.0645, Test Loss: 7161.0835\n",
      "Epoch 191, Training Loss: 7153.6558, Validation Loss: 6543.5176, Test Loss: 7160.5044\n",
      "Epoch 201, Training Loss: 7153.0649, Validation Loss: 6542.9648, Test Loss: 7159.9185\n",
      "Epoch 211, Training Loss: 7152.4668, Validation Loss: 6542.4014, Test Loss: 7159.3228\n",
      "Epoch 221, Training Loss: 7151.8555, Validation Loss: 6541.8276, Test Loss: 7158.7183\n",
      "Epoch 231, Training Loss: 7151.2388, Validation Loss: 6541.2466, Test Loss: 7158.1021\n",
      "Epoch 241, Training Loss: 7150.6118, Validation Loss: 6540.6562, Test Loss: 7157.4800\n",
      "Epoch 251, Training Loss: 7149.9829, Validation Loss: 6540.0630, Test Loss: 7156.8525\n",
      "Epoch 261, Training Loss: 7149.3472, Validation Loss: 6539.4648, Test Loss: 7156.2212\n",
      "Epoch 271, Training Loss: 7148.7070, Validation Loss: 6538.8608, Test Loss: 7155.5845\n",
      "Epoch 281, Training Loss: 7148.0615, Validation Loss: 6538.2515, Test Loss: 7154.9404\n",
      "Epoch 291, Training Loss: 7147.4082, Validation Loss: 6537.6377, Test Loss: 7154.2910\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 16, Epochs: 300\n",
      "Training loss: 7147.4082\n",
      "Validation loss: 6537.6377\n",
      "Test loss: 7154.2910\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 157/243 with parameters:\n",
      "Config indices: (1, 2, 2, 1, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.9043, Validation Loss: 6555.9546, Test Loss: 7173.7217\n",
      "Epoch 11, Training Loss: 7165.8213, Validation Loss: 6554.9253, Test Loss: 7172.6489\n",
      "Epoch 21, Training Loss: 7165.2085, Validation Loss: 6554.3374, Test Loss: 7172.0361\n",
      "Epoch 31, Training Loss: 7164.7129, Validation Loss: 6553.8628, Test Loss: 7171.5391\n",
      "Epoch 41, Training Loss: 7164.2832, Validation Loss: 6553.4526, Test Loss: 7171.1089\n",
      "Epoch 51, Training Loss: 7163.8960, Validation Loss: 6553.0825, Test Loss: 7170.7222\n",
      "Epoch 61, Training Loss: 7163.5405, Validation Loss: 6552.7417, Test Loss: 7170.3643\n",
      "Epoch 71, Training Loss: 7163.2046, Validation Loss: 6552.4175, Test Loss: 7170.0264\n",
      "Epoch 81, Training Loss: 7162.8833, Validation Loss: 6552.1104, Test Loss: 7169.7070\n",
      "Epoch 91, Training Loss: 7162.5781, Validation Loss: 6551.8159, Test Loss: 7169.3989\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 100\n",
      "Training loss: 7162.5781\n",
      "Validation loss: 6551.8159\n",
      "Test loss: 7169.3989\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 158/243 with parameters:\n",
      "Config indices: (1, 2, 2, 1, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7167.1226, Validation Loss: 6556.1865, Test Loss: 7173.9341\n",
      "Epoch 11, Training Loss: 7166.2842, Validation Loss: 6555.3818, Test Loss: 7173.0928\n",
      "Epoch 21, Training Loss: 7165.8506, Validation Loss: 6554.9673, Test Loss: 7172.6592\n",
      "Epoch 31, Training Loss: 7165.5181, Validation Loss: 6554.6484, Test Loss: 7172.3242\n",
      "Epoch 41, Training Loss: 7165.2290, Validation Loss: 6554.3716, Test Loss: 7172.0366\n",
      "Epoch 51, Training Loss: 7164.9692, Validation Loss: 6554.1235, Test Loss: 7171.7749\n",
      "Epoch 61, Training Loss: 7164.7290, Validation Loss: 6553.8945, Test Loss: 7171.5347\n",
      "Epoch 71, Training Loss: 7164.5034, Validation Loss: 6553.6777, Test Loss: 7171.3086\n",
      "Epoch 81, Training Loss: 7164.2842, Validation Loss: 6553.4717, Test Loss: 7171.0908\n",
      "Epoch 91, Training Loss: 7164.0708, Validation Loss: 6553.2686, Test Loss: 7170.8774\n",
      "Epoch 101, Training Loss: 7163.8618, Validation Loss: 6553.0713, Test Loss: 7170.6680\n",
      "Epoch 111, Training Loss: 7163.6606, Validation Loss: 6552.8779, Test Loss: 7170.4653\n",
      "Epoch 121, Training Loss: 7163.4595, Validation Loss: 6552.6885, Test Loss: 7170.2671\n",
      "Epoch 131, Training Loss: 7163.2612, Validation Loss: 6552.5000, Test Loss: 7170.0693\n",
      "Epoch 141, Training Loss: 7163.0615, Validation Loss: 6552.3120, Test Loss: 7169.8696\n",
      "Epoch 151, Training Loss: 7162.8604, Validation Loss: 6552.1201, Test Loss: 7169.6680\n",
      "Epoch 161, Training Loss: 7162.6606, Validation Loss: 6551.9302, Test Loss: 7169.4673\n",
      "Epoch 171, Training Loss: 7162.4609, Validation Loss: 6551.7417, Test Loss: 7169.2690\n",
      "Epoch 181, Training Loss: 7162.2656, Validation Loss: 6551.5542, Test Loss: 7169.0752\n",
      "Epoch 191, Training Loss: 7162.0698, Validation Loss: 6551.3682, Test Loss: 7168.8794\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 200\n",
      "Training loss: 7162.0698\n",
      "Validation loss: 6551.3682\n",
      "Test loss: 7168.8794\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 159/243 with parameters:\n",
      "Config indices: (1, 2, 2, 1, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.2476, Validation Loss: 6555.3066, Test Loss: 7173.0674\n",
      "Epoch 11, Training Loss: 7165.1719, Validation Loss: 6554.2710, Test Loss: 7171.9922\n",
      "Epoch 21, Training Loss: 7164.5107, Validation Loss: 6553.6313, Test Loss: 7171.3315\n",
      "Epoch 31, Training Loss: 7163.9619, Validation Loss: 6553.1001, Test Loss: 7170.7817\n",
      "Epoch 41, Training Loss: 7163.4722, Validation Loss: 6552.6279, Test Loss: 7170.2969\n",
      "Epoch 51, Training Loss: 7163.0278, Validation Loss: 6552.1992, Test Loss: 7169.8511\n",
      "Epoch 61, Training Loss: 7162.6138, Validation Loss: 6551.7993, Test Loss: 7169.4385\n",
      "Epoch 71, Training Loss: 7162.2192, Validation Loss: 6551.4180, Test Loss: 7169.0454\n",
      "Epoch 81, Training Loss: 7161.8408, Validation Loss: 6551.0537, Test Loss: 7168.6680\n",
      "Epoch 91, Training Loss: 7161.4741, Validation Loss: 6550.7007, Test Loss: 7168.3008\n",
      "Epoch 101, Training Loss: 7161.1187, Validation Loss: 6550.3555, Test Loss: 7167.9468\n",
      "Epoch 111, Training Loss: 7160.7695, Validation Loss: 6550.0176, Test Loss: 7167.5972\n",
      "Epoch 121, Training Loss: 7160.4268, Validation Loss: 6549.6865, Test Loss: 7167.2559\n",
      "Epoch 131, Training Loss: 7160.0879, Validation Loss: 6549.3608, Test Loss: 7166.9185\n",
      "Epoch 141, Training Loss: 7159.7510, Validation Loss: 6549.0381, Test Loss: 7166.5850\n",
      "Epoch 151, Training Loss: 7159.4214, Validation Loss: 6548.7188, Test Loss: 7166.2544\n",
      "Epoch 161, Training Loss: 7159.0928, Validation Loss: 6548.4014, Test Loss: 7165.9248\n",
      "Epoch 171, Training Loss: 7158.7622, Validation Loss: 6548.0825, Test Loss: 7165.5986\n",
      "Epoch 181, Training Loss: 7158.4326, Validation Loss: 6547.7656, Test Loss: 7165.2700\n",
      "Epoch 191, Training Loss: 7158.1045, Validation Loss: 6547.4492, Test Loss: 7164.9438\n",
      "Epoch 201, Training Loss: 7157.7793, Validation Loss: 6547.1343, Test Loss: 7164.6182\n",
      "Epoch 211, Training Loss: 7157.4536, Validation Loss: 6546.8208, Test Loss: 7164.2944\n",
      "Epoch 221, Training Loss: 7157.1309, Validation Loss: 6546.5068, Test Loss: 7163.9717\n",
      "Epoch 231, Training Loss: 7156.8066, Validation Loss: 6546.1934, Test Loss: 7163.6489\n",
      "Epoch 241, Training Loss: 7156.4805, Validation Loss: 6545.8799, Test Loss: 7163.3252\n",
      "Epoch 251, Training Loss: 7156.1572, Validation Loss: 6545.5669, Test Loss: 7163.0015\n",
      "Epoch 261, Training Loss: 7155.8311, Validation Loss: 6545.2520, Test Loss: 7162.6777\n",
      "Epoch 271, Training Loss: 7155.5063, Validation Loss: 6544.9355, Test Loss: 7162.3540\n",
      "Epoch 281, Training Loss: 7155.1792, Validation Loss: 6544.6206, Test Loss: 7162.0288\n",
      "Epoch 291, Training Loss: 7154.8530, Validation Loss: 6544.3037, Test Loss: 7161.7031\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 32, Epochs: 300\n",
      "Training loss: 7154.8530\n",
      "Validation loss: 6544.3037\n",
      "Test loss: 7161.7031\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 160/243 with parameters:\n",
      "Config indices: (1, 2, 2, 2, 0)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.4009, Validation Loss: 6555.4824, Test Loss: 7173.2300\n",
      "Epoch 11, Training Loss: 7165.2339, Validation Loss: 6554.3896, Test Loss: 7172.0679\n",
      "Epoch 21, Training Loss: 7164.6509, Validation Loss: 6553.8433, Test Loss: 7171.4878\n",
      "Epoch 31, Training Loss: 7164.2134, Validation Loss: 6553.4316, Test Loss: 7171.0498\n",
      "Epoch 41, Training Loss: 7163.8472, Validation Loss: 6553.0854, Test Loss: 7170.6841\n",
      "Epoch 51, Training Loss: 7163.5229, Validation Loss: 6552.7798, Test Loss: 7170.3604\n",
      "Epoch 61, Training Loss: 7163.2256, Validation Loss: 6552.4995, Test Loss: 7170.0640\n",
      "Epoch 71, Training Loss: 7162.9497, Validation Loss: 6552.2402, Test Loss: 7169.7896\n",
      "Epoch 81, Training Loss: 7162.6934, Validation Loss: 6551.9966, Test Loss: 7169.5298\n",
      "Epoch 91, Training Loss: 7162.4463, Validation Loss: 6551.7646, Test Loss: 7169.2861\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 100\n",
      "Training loss: 7162.4463\n",
      "Validation loss: 6551.7646\n",
      "Test loss: 7169.2861\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 161/243 with parameters:\n",
      "Config indices: (1, 2, 2, 2, 1)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.6333, Validation Loss: 6555.6704, Test Loss: 7173.4399\n",
      "Epoch 11, Training Loss: 7165.8247, Validation Loss: 6554.9077, Test Loss: 7172.6309\n",
      "Epoch 21, Training Loss: 7165.3716, Validation Loss: 6554.4824, Test Loss: 7172.1807\n",
      "Epoch 31, Training Loss: 7165.0059, Validation Loss: 6554.1396, Test Loss: 7171.8203\n",
      "Epoch 41, Training Loss: 7164.6934, Validation Loss: 6553.8413, Test Loss: 7171.5049\n",
      "Epoch 51, Training Loss: 7164.3945, Validation Loss: 6553.5615, Test Loss: 7171.2095\n",
      "Epoch 61, Training Loss: 7164.1055, Validation Loss: 6553.2876, Test Loss: 7170.9185\n",
      "Epoch 71, Training Loss: 7163.8379, Validation Loss: 6553.0347, Test Loss: 7170.6499\n",
      "Epoch 81, Training Loss: 7163.5879, Validation Loss: 6552.7993, Test Loss: 7170.4014\n",
      "Epoch 91, Training Loss: 7163.3506, Validation Loss: 6552.5757, Test Loss: 7170.1655\n",
      "Epoch 101, Training Loss: 7163.1250, Validation Loss: 6552.3628, Test Loss: 7169.9414\n",
      "Epoch 111, Training Loss: 7162.9072, Validation Loss: 6552.1567, Test Loss: 7169.7222\n",
      "Epoch 121, Training Loss: 7162.6963, Validation Loss: 6551.9575, Test Loss: 7169.5142\n",
      "Epoch 131, Training Loss: 7162.4912, Validation Loss: 6551.7632, Test Loss: 7169.3071\n",
      "Epoch 141, Training Loss: 7162.2842, Validation Loss: 6551.5693, Test Loss: 7169.1035\n",
      "Epoch 151, Training Loss: 7162.0781, Validation Loss: 6551.3750, Test Loss: 7168.8965\n",
      "Epoch 161, Training Loss: 7161.8667, Validation Loss: 6551.1787, Test Loss: 7168.6899\n",
      "Epoch 171, Training Loss: 7161.6597, Validation Loss: 6550.9829, Test Loss: 7168.4844\n",
      "Epoch 181, Training Loss: 7161.4580, Validation Loss: 6550.7915, Test Loss: 7168.2827\n",
      "Epoch 191, Training Loss: 7161.2607, Validation Loss: 6550.6025, Test Loss: 7168.0859\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 200\n",
      "Training loss: 7161.2607\n",
      "Validation loss: 6550.6025\n",
      "Test loss: 7168.0859\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 162/243 with parameters:\n",
      "Config indices: (1, 2, 2, 2, 2)\n",
      "{'hidden_sizes': [16, 16, 16], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7167.3379, Validation Loss: 6556.3687, Test Loss: 7174.1387\n",
      "Epoch 11, Training Loss: 7166.6655, Validation Loss: 6555.7373, Test Loss: 7173.4746\n",
      "Epoch 21, Training Loss: 7166.3716, Validation Loss: 6555.4600, Test Loss: 7173.1826\n",
      "Epoch 31, Training Loss: 7166.1558, Validation Loss: 6555.2520, Test Loss: 7172.9658\n",
      "Epoch 41, Training Loss: 7165.9697, Validation Loss: 6555.0742, Test Loss: 7172.7783\n",
      "Epoch 51, Training Loss: 7165.8018, Validation Loss: 6554.9180, Test Loss: 7172.6133\n",
      "Epoch 61, Training Loss: 7165.6504, Validation Loss: 6554.7725, Test Loss: 7172.4595\n",
      "Epoch 71, Training Loss: 7165.5034, Validation Loss: 6554.6362, Test Loss: 7172.3130\n",
      "Epoch 81, Training Loss: 7165.3677, Validation Loss: 6554.5059, Test Loss: 7172.1758\n",
      "Epoch 91, Training Loss: 7165.2319, Validation Loss: 6554.3770, Test Loss: 7172.0376\n",
      "Epoch 101, Training Loss: 7165.0991, Validation Loss: 6554.2520, Test Loss: 7171.9053\n",
      "Epoch 111, Training Loss: 7164.9692, Validation Loss: 6554.1255, Test Loss: 7171.7720\n",
      "Epoch 121, Training Loss: 7164.8330, Validation Loss: 6553.9995, Test Loss: 7171.6382\n",
      "Epoch 131, Training Loss: 7164.6963, Validation Loss: 6553.8721, Test Loss: 7171.5005\n",
      "Epoch 141, Training Loss: 7164.5518, Validation Loss: 6553.7393, Test Loss: 7171.3569\n",
      "Epoch 151, Training Loss: 7164.4092, Validation Loss: 6553.6060, Test Loss: 7171.2134\n",
      "Epoch 161, Training Loss: 7164.2744, Validation Loss: 6553.4810, Test Loss: 7171.0786\n",
      "Epoch 171, Training Loss: 7164.1445, Validation Loss: 6553.3608, Test Loss: 7170.9497\n",
      "Epoch 181, Training Loss: 7164.0220, Validation Loss: 6553.2466, Test Loss: 7170.8267\n",
      "Epoch 191, Training Loss: 7163.9043, Validation Loss: 6553.1343, Test Loss: 7170.7070\n",
      "Epoch 201, Training Loss: 7163.7891, Validation Loss: 6553.0273, Test Loss: 7170.5894\n",
      "Epoch 211, Training Loss: 7163.6768, Validation Loss: 6552.9214, Test Loss: 7170.4775\n",
      "Epoch 221, Training Loss: 7163.5684, Validation Loss: 6552.8179, Test Loss: 7170.3677\n",
      "Epoch 231, Training Loss: 7163.4585, Validation Loss: 6552.7173, Test Loss: 7170.2612\n",
      "Epoch 241, Training Loss: 7163.3521, Validation Loss: 6552.6191, Test Loss: 7170.1553\n",
      "Epoch 251, Training Loss: 7163.2485, Validation Loss: 6552.5210, Test Loss: 7170.0518\n",
      "Epoch 261, Training Loss: 7163.1460, Validation Loss: 6552.4253, Test Loss: 7169.9468\n",
      "Epoch 271, Training Loss: 7163.0439, Validation Loss: 6552.3306, Test Loss: 7169.8462\n",
      "Epoch 281, Training Loss: 7162.9463, Validation Loss: 6552.2373, Test Loss: 7169.7471\n",
      "Epoch 291, Training Loss: 7162.8491, Validation Loss: 6552.1440, Test Loss: 7169.6489\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [16, 16, 16], Batch size: 64, Epochs: 300\n",
      "Training loss: 7162.8491\n",
      "Validation loss: 6552.1440\n",
      "Test loss: 7169.6489\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 163/243 with parameters:\n",
      "Config indices: (2, 0, 0, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 277567733760.0000, Validation Loss: 277571862528.0000, Test Loss: 277567471616.0000\n",
      "Epoch 11, Training Loss: 808.9113, Validation Loss: 821.5337, Test Loss: 769.6666\n",
      "Epoch 21, Training Loss: 808.9454, Validation Loss: 819.8008, Test Loss: 769.8314\n",
      "Epoch 31, Training Loss: 808.9315, Validation Loss: 822.4070, Test Loss: 769.6238\n",
      "Epoch 41, Training Loss: 808.9196, Validation Loss: 820.4722, Test Loss: 769.7543\n",
      "Epoch 51, Training Loss: 808.9459, Validation Loss: 819.7925, Test Loss: 769.8323\n",
      "Epoch 61, Training Loss: 808.9742, Validation Loss: 819.3226, Test Loss: 769.8976\n",
      "Epoch 71, Training Loss: 808.9891, Validation Loss: 823.5200, Test Loss: 769.6036\n",
      "Epoch 81, Training Loss: 809.1000, Validation Loss: 818.0213, Test Loss: 770.1288\n",
      "Epoch 91, Training Loss: 808.9454, Validation Loss: 822.7482, Test Loss: 769.6136\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 808.9454\n",
      "Validation loss: 822.7482\n",
      "Test loss: 769.6136\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 164/243 with parameters:\n",
      "Config indices: (2, 0, 0, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 165/243 with parameters:\n",
      "Config indices: (2, 0, 0, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 201, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 211, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 221, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 231, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 241, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 251, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 261, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 271, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 281, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 291, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 166/243 with parameters:\n",
      "Config indices: (2, 0, 0, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 167/243 with parameters:\n",
      "Config indices: (2, 0, 0, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 168/243 with parameters:\n",
      "Config indices: (2, 0, 0, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 2353.3474, Validation Loss: 2058.4902, Test Loss: 2336.8167\n",
      "Epoch 11, Training Loss: 808.9133, Validation Loss: 821.6990, Test Loss: 769.6566\n",
      "Epoch 21, Training Loss: 808.9131, Validation Loss: 821.6951, Test Loss: 769.6568\n",
      "Epoch 31, Training Loss: 808.9386, Validation Loss: 819.9401, Test Loss: 769.8139\n",
      "Epoch 41, Training Loss: 808.9110, Validation Loss: 821.5257, Test Loss: 769.6671\n",
      "Epoch 51, Training Loss: 808.9158, Validation Loss: 820.6323, Test Loss: 769.7384\n",
      "Epoch 61, Training Loss: 808.9148, Validation Loss: 821.7969, Test Loss: 769.6511\n",
      "Epoch 71, Training Loss: 808.9432, Validation Loss: 822.7021, Test Loss: 769.6146\n",
      "Epoch 81, Training Loss: 808.9450, Validation Loss: 819.8054, Test Loss: 769.8308\n",
      "Epoch 91, Training Loss: 808.9121, Validation Loss: 820.8604, Test Loss: 769.7174\n",
      "Epoch 101, Training Loss: 808.9448, Validation Loss: 819.8175, Test Loss: 769.8292\n",
      "Epoch 111, Training Loss: 808.9261, Validation Loss: 822.2605, Test Loss: 769.6293\n",
      "Epoch 121, Training Loss: 808.9359, Validation Loss: 822.5281, Test Loss: 769.6198\n",
      "Epoch 131, Training Loss: 808.9102, Validation Loss: 821.4252, Test Loss: 769.6737\n",
      "Epoch 141, Training Loss: 808.9821, Validation Loss: 823.4175, Test Loss: 769.6038\n",
      "Epoch 151, Training Loss: 808.9528, Validation Loss: 819.6576, Test Loss: 769.8502\n",
      "Epoch 161, Training Loss: 808.9544, Validation Loss: 819.6365, Test Loss: 769.8530\n",
      "Epoch 171, Training Loss: 808.9280, Validation Loss: 822.3154, Test Loss: 769.6270\n",
      "Epoch 181, Training Loss: 809.0555, Validation Loss: 824.3682, Test Loss: 769.6121\n",
      "Epoch 191, Training Loss: 808.9573, Validation Loss: 819.5845, Test Loss: 769.8602\n",
      "Epoch 201, Training Loss: 808.9450, Validation Loss: 822.7430, Test Loss: 769.6136\n",
      "Epoch 211, Training Loss: 808.9401, Validation Loss: 819.9116, Test Loss: 769.8174\n",
      "Epoch 221, Training Loss: 808.9193, Validation Loss: 820.4897, Test Loss: 769.7525\n",
      "Epoch 231, Training Loss: 808.9238, Validation Loss: 822.1851, Test Loss: 769.6323\n",
      "Epoch 241, Training Loss: 808.9365, Validation Loss: 819.9877, Test Loss: 769.8080\n",
      "Epoch 251, Training Loss: 808.9271, Validation Loss: 820.2354, Test Loss: 769.7795\n",
      "Epoch 261, Training Loss: 808.9116, Validation Loss: 820.8923, Test Loss: 769.7148\n",
      "Epoch 271, Training Loss: 808.9142, Validation Loss: 821.7578, Test Loss: 769.6533\n",
      "Epoch 281, Training Loss: 808.9159, Validation Loss: 821.8544, Test Loss: 769.6480\n",
      "Epoch 291, Training Loss: 808.9415, Validation Loss: 822.6681, Test Loss: 769.6156\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 808.9415\n",
      "Validation loss: 822.6681\n",
      "Test loss: 769.6156\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 169/243 with parameters:\n",
      "Config indices: (2, 0, 0, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 170/243 with parameters:\n",
      "Config indices: (2, 0, 0, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 812.2028, Validation Loss: 838.7162, Test Loss: 771.9333\n",
      "Epoch 11, Training Loss: 842.3250, Validation Loss: 809.4705, Test Loss: 806.4399\n",
      "Epoch 21, Training Loss: 808.9764, Validation Loss: 823.3232, Test Loss: 769.6045\n",
      "Epoch 31, Training Loss: 808.9103, Validation Loss: 821.0823, Test Loss: 769.6989\n",
      "Epoch 41, Training Loss: 808.9886, Validation Loss: 819.1240, Test Loss: 769.9277\n",
      "Epoch 51, Training Loss: 808.9103, Validation Loss: 821.3780, Test Loss: 769.6771\n",
      "Epoch 61, Training Loss: 808.9211, Validation Loss: 822.0884, Test Loss: 769.6365\n",
      "Epoch 71, Training Loss: 808.9100, Validation Loss: 821.2638, Test Loss: 769.6850\n",
      "Epoch 81, Training Loss: 808.9656, Validation Loss: 819.4495, Test Loss: 769.8790\n",
      "Epoch 91, Training Loss: 808.9175, Validation Loss: 820.5647, Test Loss: 769.7449\n",
      "Epoch 101, Training Loss: 808.9127, Validation Loss: 821.6661, Test Loss: 769.6586\n",
      "Epoch 111, Training Loss: 808.9536, Validation Loss: 819.6446, Test Loss: 769.8520\n",
      "Epoch 121, Training Loss: 808.9664, Validation Loss: 819.4357, Test Loss: 769.8810\n",
      "Epoch 131, Training Loss: 808.9100, Validation Loss: 821.1426, Test Loss: 769.6942\n",
      "Epoch 141, Training Loss: 808.9131, Validation Loss: 820.7781, Test Loss: 769.7248\n",
      "Epoch 151, Training Loss: 808.9335, Validation Loss: 822.4666, Test Loss: 769.6216\n",
      "Epoch 161, Training Loss: 809.0014, Validation Loss: 823.6988, Test Loss: 769.6036\n",
      "Epoch 171, Training Loss: 808.9100, Validation Loss: 821.3048, Test Loss: 769.6820\n",
      "Epoch 181, Training Loss: 808.9158, Validation Loss: 821.8452, Test Loss: 769.6484\n",
      "Epoch 191, Training Loss: 808.9187, Validation Loss: 820.5079, Test Loss: 769.7506\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 808.9187\n",
      "Validation loss: 820.5079\n",
      "Test loss: 769.7506\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 171/243 with parameters:\n",
      "Config indices: (2, 0, 0, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 11, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 21, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 31, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 41, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 51, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 61, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 71, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 81, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 91, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 101, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 111, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 121, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 131, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 141, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 151, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 161, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 171, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 181, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 191, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 201, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 211, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 221, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 231, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 241, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 251, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 261, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 271, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 281, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Epoch 291, Training Loss: nan, Validation Loss: nan, Test Loss: nan\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 172/243 with parameters:\n",
      "Config indices: (2, 0, 1, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 175.4443, Validation Loss: 196.4230, Test Loss: 158.1195\n",
      "Epoch 11, Training Loss: 50.5894, Validation Loss: 44.0601, Test Loss: 48.6520\n",
      "Epoch 21, Training Loss: 10.5540, Validation Loss: 8.7097, Test Loss: 10.1150\n",
      "Epoch 31, Training Loss: 27.8119, Validation Loss: 26.8469, Test Loss: 29.2497\n",
      "Epoch 41, Training Loss: 14.0492, Validation Loss: 12.1299, Test Loss: 14.7327\n",
      "Epoch 51, Training Loss: 13.6100, Validation Loss: 11.0229, Test Loss: 13.4784\n",
      "Epoch 61, Training Loss: 2.9828, Validation Loss: 2.0258, Test Loss: 3.0778\n",
      "Epoch 71, Training Loss: 2.1899, Validation Loss: 1.4339, Test Loss: 2.1986\n",
      "Epoch 81, Training Loss: 37.3835, Validation Loss: 32.5489, Test Loss: 37.8225\n",
      "Epoch 91, Training Loss: 2.0333, Validation Loss: 1.2936, Test Loss: 2.0990\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 2.0333\n",
      "Validation loss: 1.2936\n",
      "Test loss: 2.0990\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 173/243 with parameters:\n",
      "Config indices: (2, 0, 1, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 124.0460, Validation Loss: 100.0565, Test Loss: 114.0312\n",
      "Epoch 11, Training Loss: 2.7201, Validation Loss: 2.2998, Test Loss: 2.9777\n",
      "Epoch 21, Training Loss: 1.2568, Validation Loss: 0.9717, Test Loss: 1.5520\n",
      "Epoch 31, Training Loss: 1.4564, Validation Loss: 1.1315, Test Loss: 1.6374\n",
      "Epoch 41, Training Loss: 1.2738, Validation Loss: 0.8413, Test Loss: 1.3950\n",
      "Epoch 51, Training Loss: 0.6887, Validation Loss: 0.3896, Test Loss: 0.8024\n",
      "Epoch 61, Training Loss: 4.9888, Validation Loss: 4.9869, Test Loss: 4.8945\n",
      "Epoch 71, Training Loss: 0.8600, Validation Loss: 0.6465, Test Loss: 0.9571\n",
      "Epoch 81, Training Loss: 0.7656, Validation Loss: 0.5359, Test Loss: 0.7478\n",
      "Epoch 91, Training Loss: 1.6345, Validation Loss: 1.6221, Test Loss: 1.6642\n",
      "Epoch 101, Training Loss: 0.3881, Validation Loss: 0.2759, Test Loss: 0.4633\n",
      "Epoch 111, Training Loss: 1.0749, Validation Loss: 0.7957, Test Loss: 1.0694\n",
      "Epoch 121, Training Loss: 1.2133, Validation Loss: 1.2796, Test Loss: 1.2969\n",
      "Epoch 131, Training Loss: 1.7643, Validation Loss: 1.3885, Test Loss: 1.7701\n",
      "Epoch 141, Training Loss: 8.3364, Validation Loss: 9.8369, Test Loss: 8.4478\n",
      "Epoch 151, Training Loss: 0.5559, Validation Loss: 0.4285, Test Loss: 0.6852\n",
      "Epoch 161, Training Loss: 0.7527, Validation Loss: 0.6510, Test Loss: 0.8860\n",
      "Epoch 171, Training Loss: 0.4113, Validation Loss: 0.3473, Test Loss: 0.5234\n",
      "Epoch 181, Training Loss: 1.3308, Validation Loss: 1.1818, Test Loss: 1.3268\n",
      "Epoch 191, Training Loss: 1.2290, Validation Loss: 1.2359, Test Loss: 1.2123\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 1.2290\n",
      "Validation loss: 1.2359\n",
      "Test loss: 1.2123\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 174/243 with parameters:\n",
      "Config indices: (2, 0, 1, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 178.4091, Validation Loss: 183.6982, Test Loss: 164.6482\n",
      "Epoch 11, Training Loss: 38.5389, Validation Loss: 34.3543, Test Loss: 37.6789\n",
      "Epoch 21, Training Loss: 0.8846, Validation Loss: 0.6467, Test Loss: 1.1260\n",
      "Epoch 31, Training Loss: 4.0698, Validation Loss: 3.7443, Test Loss: 4.2767\n",
      "Epoch 41, Training Loss: 27.9894, Validation Loss: 25.2994, Test Loss: 25.8273\n",
      "Epoch 51, Training Loss: 186.7886, Validation Loss: 168.9726, Test Loss: 187.0513\n",
      "Epoch 61, Training Loss: 16.1470, Validation Loss: 14.4608, Test Loss: 15.5282\n",
      "Epoch 71, Training Loss: 22.8174, Validation Loss: 20.3636, Test Loss: 19.5063\n",
      "Epoch 81, Training Loss: 44.3493, Validation Loss: 38.6823, Test Loss: 43.4359\n",
      "Epoch 91, Training Loss: 6.6674, Validation Loss: 4.8848, Test Loss: 6.5280\n",
      "Epoch 101, Training Loss: 9.2282, Validation Loss: 8.6171, Test Loss: 8.7706\n",
      "Epoch 111, Training Loss: 45.1810, Validation Loss: 44.9166, Test Loss: 44.1813\n",
      "Epoch 121, Training Loss: 72.9373, Validation Loss: 68.0289, Test Loss: 74.6865\n",
      "Epoch 131, Training Loss: 7.4813, Validation Loss: 6.3248, Test Loss: 7.5806\n",
      "Epoch 141, Training Loss: 1.2613, Validation Loss: 0.8515, Test Loss: 1.3149\n",
      "Epoch 151, Training Loss: 0.6988, Validation Loss: 0.5325, Test Loss: 0.7724\n",
      "Epoch 161, Training Loss: 0.6877, Validation Loss: 0.5278, Test Loss: 0.8006\n",
      "Epoch 171, Training Loss: 2.2170, Validation Loss: 1.8484, Test Loss: 2.2880\n",
      "Epoch 181, Training Loss: 1.7847, Validation Loss: 1.4891, Test Loss: 1.8223\n",
      "Epoch 191, Training Loss: 0.6872, Validation Loss: 0.4673, Test Loss: 0.7811\n",
      "Epoch 201, Training Loss: 1.1670, Validation Loss: 0.9853, Test Loss: 1.2298\n",
      "Epoch 211, Training Loss: 1.0938, Validation Loss: 1.0260, Test Loss: 1.3169\n",
      "Epoch 221, Training Loss: 0.5714, Validation Loss: 0.4137, Test Loss: 0.6963\n",
      "Epoch 231, Training Loss: 0.6873, Validation Loss: 0.5188, Test Loss: 0.8707\n",
      "Epoch 241, Training Loss: 0.4582, Validation Loss: 0.3517, Test Loss: 0.5972\n",
      "Epoch 251, Training Loss: 0.5055, Validation Loss: 0.3850, Test Loss: 0.5444\n",
      "Epoch 261, Training Loss: 0.3886, Validation Loss: 0.3244, Test Loss: 0.5302\n",
      "Epoch 271, Training Loss: 1.1648, Validation Loss: 1.1194, Test Loss: 1.1954\n",
      "Epoch 281, Training Loss: 0.3416, Validation Loss: 0.2757, Test Loss: 0.4384\n",
      "Epoch 291, Training Loss: 1.3239, Validation Loss: 1.2228, Test Loss: 1.5543\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 1.3239\n",
      "Validation loss: 1.2228\n",
      "Test loss: 1.5543\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 175/243 with parameters:\n",
      "Config indices: (2, 0, 1, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 443.4605, Validation Loss: 476.8104, Test Loss: 442.4154\n",
      "Epoch 11, Training Loss: 7.0001, Validation Loss: 5.4681, Test Loss: 7.6085\n",
      "Epoch 21, Training Loss: 4.3937, Validation Loss: 3.6762, Test Loss: 4.3638\n",
      "Epoch 31, Training Loss: 16.5710, Validation Loss: 17.6715, Test Loss: 16.2093\n",
      "Epoch 41, Training Loss: 12.4387, Validation Loss: 12.3637, Test Loss: 11.8488\n",
      "Epoch 51, Training Loss: 6.3476, Validation Loss: 6.5931, Test Loss: 6.0186\n",
      "Epoch 61, Training Loss: 1.6819, Validation Loss: 1.3354, Test Loss: 1.5143\n",
      "Epoch 71, Training Loss: 9.9457, Validation Loss: 8.9232, Test Loss: 9.5667\n",
      "Epoch 81, Training Loss: 26.1292, Validation Loss: 24.9299, Test Loss: 26.2156\n",
      "Epoch 91, Training Loss: 11.9144, Validation Loss: 11.4108, Test Loss: 12.1830\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 11.9144\n",
      "Validation loss: 11.4108\n",
      "Test loss: 12.1830\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 176/243 with parameters:\n",
      "Config indices: (2, 0, 1, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 170.2526, Validation Loss: 157.6191, Test Loss: 171.4826\n",
      "Epoch 11, Training Loss: 8.4079, Validation Loss: 6.6519, Test Loss: 8.1975\n",
      "Epoch 21, Training Loss: 23.5507, Validation Loss: 22.2485, Test Loss: 23.0229\n",
      "Epoch 31, Training Loss: 2.0093, Validation Loss: 1.4238, Test Loss: 1.8224\n",
      "Epoch 41, Training Loss: 7.3347, Validation Loss: 6.5712, Test Loss: 7.1720\n",
      "Epoch 51, Training Loss: 2.1258, Validation Loss: 1.7534, Test Loss: 2.0452\n",
      "Epoch 61, Training Loss: 2.3485, Validation Loss: 1.8313, Test Loss: 2.3165\n",
      "Epoch 71, Training Loss: 2.1822, Validation Loss: 1.9014, Test Loss: 2.2970\n",
      "Epoch 81, Training Loss: 39.3492, Validation Loss: 36.9336, Test Loss: 38.3582\n",
      "Epoch 91, Training Loss: 1.4323, Validation Loss: 1.2683, Test Loss: 1.5531\n",
      "Epoch 101, Training Loss: 15.1127, Validation Loss: 13.2576, Test Loss: 15.7331\n",
      "Epoch 111, Training Loss: 8.3312, Validation Loss: 7.3654, Test Loss: 8.4125\n",
      "Epoch 121, Training Loss: 2.5200, Validation Loss: 2.3617, Test Loss: 2.5200\n",
      "Epoch 131, Training Loss: 2.2725, Validation Loss: 1.7958, Test Loss: 2.3826\n",
      "Epoch 141, Training Loss: 2.5290, Validation Loss: 2.2007, Test Loss: 2.3846\n",
      "Epoch 151, Training Loss: 4.5334, Validation Loss: 4.4135, Test Loss: 4.6216\n",
      "Epoch 161, Training Loss: 2.7902, Validation Loss: 2.3089, Test Loss: 2.7828\n",
      "Epoch 171, Training Loss: 51.3463, Validation Loss: 47.8475, Test Loss: 51.9882\n",
      "Epoch 181, Training Loss: 9.3365, Validation Loss: 8.2887, Test Loss: 9.4538\n",
      "Epoch 191, Training Loss: 1.1816, Validation Loss: 0.8694, Test Loss: 1.2658\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 1.1816\n",
      "Validation loss: 0.8694\n",
      "Test loss: 1.2658\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 177/243 with parameters:\n",
      "Config indices: (2, 0, 1, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 237.7194, Validation Loss: 205.7714, Test Loss: 240.8040\n",
      "Epoch 11, Training Loss: 219.7855, Validation Loss: 219.5797, Test Loss: 210.6821\n",
      "Epoch 21, Training Loss: 50.3231, Validation Loss: 48.2429, Test Loss: 50.8928\n",
      "Epoch 31, Training Loss: 1.7409, Validation Loss: 1.2441, Test Loss: 1.9670\n",
      "Epoch 41, Training Loss: 1.5869, Validation Loss: 1.2568, Test Loss: 1.9571\n",
      "Epoch 51, Training Loss: 7.9548, Validation Loss: 6.8489, Test Loss: 7.6704\n",
      "Epoch 61, Training Loss: 0.7758, Validation Loss: 0.4652, Test Loss: 1.0257\n",
      "Epoch 71, Training Loss: 0.9275, Validation Loss: 0.6229, Test Loss: 1.1271\n",
      "Epoch 81, Training Loss: 2.4795, Validation Loss: 2.1065, Test Loss: 2.9813\n",
      "Epoch 91, Training Loss: 2.2713, Validation Loss: 1.6874, Test Loss: 2.3938\n",
      "Epoch 101, Training Loss: 0.7144, Validation Loss: 0.5245, Test Loss: 0.8916\n",
      "Epoch 111, Training Loss: 1.1049, Validation Loss: 0.8356, Test Loss: 1.3590\n",
      "Epoch 121, Training Loss: 2.2957, Validation Loss: 1.9409, Test Loss: 2.6323\n",
      "Epoch 131, Training Loss: 0.6327, Validation Loss: 0.4528, Test Loss: 0.7172\n",
      "Epoch 141, Training Loss: 0.6424, Validation Loss: 0.5311, Test Loss: 0.8331\n",
      "Epoch 151, Training Loss: 0.4038, Validation Loss: 0.2786, Test Loss: 0.5344\n",
      "Epoch 161, Training Loss: 0.9391, Validation Loss: 0.8616, Test Loss: 1.0045\n",
      "Epoch 171, Training Loss: 0.5069, Validation Loss: 0.3892, Test Loss: 0.6864\n",
      "Epoch 181, Training Loss: 3.6588, Validation Loss: 3.0833, Test Loss: 3.9608\n",
      "Epoch 191, Training Loss: 0.5746, Validation Loss: 0.4151, Test Loss: 0.6524\n",
      "Epoch 201, Training Loss: 0.4643, Validation Loss: 0.3431, Test Loss: 0.5300\n",
      "Epoch 211, Training Loss: 0.9494, Validation Loss: 0.8792, Test Loss: 0.9798\n",
      "Epoch 221, Training Loss: 0.4324, Validation Loss: 0.3286, Test Loss: 0.6002\n",
      "Epoch 231, Training Loss: 1.8500, Validation Loss: 1.5215, Test Loss: 1.8580\n",
      "Epoch 241, Training Loss: 0.5406, Validation Loss: 0.4105, Test Loss: 0.5805\n",
      "Epoch 251, Training Loss: 0.5012, Validation Loss: 0.3630, Test Loss: 0.6432\n",
      "Epoch 261, Training Loss: 0.5911, Validation Loss: 0.4358, Test Loss: 0.6644\n",
      "Epoch 271, Training Loss: 0.5638, Validation Loss: 0.4770, Test Loss: 0.6545\n",
      "Epoch 281, Training Loss: 0.5423, Validation Loss: 0.4509, Test Loss: 0.7258\n",
      "Epoch 291, Training Loss: 1.9088, Validation Loss: 1.6536, Test Loss: 1.8209\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 1.9088\n",
      "Validation loss: 1.6536\n",
      "Test loss: 1.8209\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 178/243 with parameters:\n",
      "Config indices: (2, 0, 1, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7579.6592, Validation Loss: 6725.4814, Test Loss: 7492.7573\n",
      "Epoch 11, Training Loss: 22.8104, Validation Loss: 21.9890, Test Loss: 21.8051\n",
      "Epoch 21, Training Loss: 4.8390, Validation Loss: 4.1330, Test Loss: 4.4712\n",
      "Epoch 31, Training Loss: 1.5748, Validation Loss: 1.0967, Test Loss: 1.4084\n",
      "Epoch 41, Training Loss: 16.1256, Validation Loss: 14.7489, Test Loss: 15.9745\n",
      "Epoch 51, Training Loss: 19.2985, Validation Loss: 17.8736, Test Loss: 18.2327\n",
      "Epoch 61, Training Loss: 33.3211, Validation Loss: 30.2953, Test Loss: 33.0965\n",
      "Epoch 71, Training Loss: 9.6217, Validation Loss: 10.0477, Test Loss: 9.3520\n",
      "Epoch 81, Training Loss: 39.9639, Validation Loss: 35.7019, Test Loss: 38.5934\n",
      "Epoch 91, Training Loss: 9.4088, Validation Loss: 8.2219, Test Loss: 9.2217\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 9.4088\n",
      "Validation loss: 8.2219\n",
      "Test loss: 9.2217\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 179/243 with parameters:\n",
      "Config indices: (2, 0, 1, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 4449.6089, Validation Loss: 4018.3997, Test Loss: 4450.9453\n",
      "Epoch 11, Training Loss: 54.8137, Validation Loss: 44.2012, Test Loss: 51.0217\n",
      "Epoch 21, Training Loss: 133.8487, Validation Loss: 119.2838, Test Loss: 138.7180\n",
      "Epoch 31, Training Loss: 17.3218, Validation Loss: 15.3210, Test Loss: 17.4082\n",
      "Epoch 41, Training Loss: 8.7737, Validation Loss: 6.7197, Test Loss: 8.9810\n",
      "Epoch 51, Training Loss: 16.8496, Validation Loss: 15.4028, Test Loss: 17.3151\n",
      "Epoch 61, Training Loss: 15.3535, Validation Loss: 12.9670, Test Loss: 15.5526\n",
      "Epoch 71, Training Loss: 0.8735, Validation Loss: 0.6244, Test Loss: 1.0949\n",
      "Epoch 81, Training Loss: 4.4548, Validation Loss: 3.5698, Test Loss: 4.7858\n",
      "Epoch 91, Training Loss: 1.4127, Validation Loss: 1.2244, Test Loss: 1.6777\n",
      "Epoch 101, Training Loss: 2.2386, Validation Loss: 1.8501, Test Loss: 2.2308\n",
      "Epoch 111, Training Loss: 2.5093, Validation Loss: 2.0319, Test Loss: 2.5558\n",
      "Epoch 121, Training Loss: 1.4649, Validation Loss: 1.2887, Test Loss: 1.7336\n",
      "Epoch 131, Training Loss: 0.9925, Validation Loss: 0.8890, Test Loss: 1.1334\n",
      "Epoch 141, Training Loss: 0.5573, Validation Loss: 0.4394, Test Loss: 0.7560\n",
      "Epoch 151, Training Loss: 0.7312, Validation Loss: 0.4692, Test Loss: 0.9118\n",
      "Epoch 161, Training Loss: 0.5930, Validation Loss: 0.4299, Test Loss: 0.7034\n",
      "Epoch 171, Training Loss: 1.3066, Validation Loss: 1.0111, Test Loss: 1.4151\n",
      "Epoch 181, Training Loss: 1.7473, Validation Loss: 1.4645, Test Loss: 2.0780\n",
      "Epoch 191, Training Loss: 0.7801, Validation Loss: 0.6105, Test Loss: 1.0094\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.7801\n",
      "Validation loss: 0.6105\n",
      "Test loss: 1.0094\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 180/243 with parameters:\n",
      "Config indices: (2, 0, 1, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 4777.9858, Validation Loss: 4303.8438, Test Loss: 4778.5596\n",
      "Epoch 11, Training Loss: 7.5311, Validation Loss: 5.9353, Test Loss: 7.4593\n",
      "Epoch 21, Training Loss: 2.7635, Validation Loss: 2.0310, Test Loss: 2.8721\n",
      "Epoch 31, Training Loss: 13.4248, Validation Loss: 13.9348, Test Loss: 12.8491\n",
      "Epoch 41, Training Loss: 23.7644, Validation Loss: 26.3122, Test Loss: 22.3234\n",
      "Epoch 51, Training Loss: 1.8788, Validation Loss: 1.3375, Test Loss: 1.9003\n",
      "Epoch 61, Training Loss: 9.7409, Validation Loss: 10.4190, Test Loss: 9.2769\n",
      "Epoch 71, Training Loss: 8.0845, Validation Loss: 9.3383, Test Loss: 8.1411\n",
      "Epoch 81, Training Loss: 2.3729, Validation Loss: 2.1235, Test Loss: 2.5987\n",
      "Epoch 91, Training Loss: 43.9164, Validation Loss: 40.5825, Test Loss: 45.0000\n",
      "Epoch 101, Training Loss: 4.8753, Validation Loss: 4.0335, Test Loss: 4.8363\n",
      "Epoch 111, Training Loss: 29.6805, Validation Loss: 26.4370, Test Loss: 29.1476\n",
      "Epoch 121, Training Loss: 4.1775, Validation Loss: 3.6726, Test Loss: 4.5240\n",
      "Epoch 131, Training Loss: 1.0210, Validation Loss: 0.7363, Test Loss: 1.2454\n",
      "Epoch 141, Training Loss: 1.1236, Validation Loss: 0.7994, Test Loss: 1.3662\n",
      "Epoch 151, Training Loss: 6.2391, Validation Loss: 5.8718, Test Loss: 6.7389\n",
      "Epoch 161, Training Loss: 11.2039, Validation Loss: 9.6716, Test Loss: 11.9136\n",
      "Epoch 171, Training Loss: 0.8041, Validation Loss: 0.6448, Test Loss: 0.9926\n",
      "Epoch 181, Training Loss: 5.0679, Validation Loss: 4.2766, Test Loss: 5.5660\n",
      "Epoch 191, Training Loss: 3.2266, Validation Loss: 2.7685, Test Loss: 3.5653\n",
      "Epoch 201, Training Loss: 0.6009, Validation Loss: 0.3997, Test Loss: 0.7247\n",
      "Epoch 211, Training Loss: 5.3935, Validation Loss: 5.5149, Test Loss: 5.3867\n",
      "Epoch 221, Training Loss: 0.6918, Validation Loss: 0.4289, Test Loss: 0.8922\n",
      "Epoch 231, Training Loss: 3.1245, Validation Loss: 2.9169, Test Loss: 3.3365\n",
      "Epoch 241, Training Loss: 4.4886, Validation Loss: 3.9302, Test Loss: 4.8271\n",
      "Epoch 251, Training Loss: 2.5586, Validation Loss: 2.2798, Test Loss: 2.8528\n",
      "Epoch 261, Training Loss: 2.0430, Validation Loss: 1.6451, Test Loss: 2.0566\n",
      "Epoch 271, Training Loss: 0.9353, Validation Loss: 0.7019, Test Loss: 1.0139\n",
      "Epoch 281, Training Loss: 3.1714, Validation Loss: 3.1940, Test Loss: 3.1413\n",
      "Epoch 291, Training Loss: 0.5764, Validation Loss: 0.4108, Test Loss: 0.6561\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.5764\n",
      "Validation loss: 0.4108\n",
      "Test loss: 0.6561\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 181/243 with parameters:\n",
      "Config indices: (2, 0, 2, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6893.7524, Validation Loss: 6296.3755, Test Loss: 6899.5962\n",
      "Epoch 11, Training Loss: 1.4310, Validation Loss: 1.0692, Test Loss: 1.2682\n",
      "Epoch 21, Training Loss: 0.7054, Validation Loss: 0.4687, Test Loss: 0.8458\n",
      "Epoch 31, Training Loss: 0.6103, Validation Loss: 0.4207, Test Loss: 0.7116\n",
      "Epoch 41, Training Loss: 0.5778, Validation Loss: 0.3696, Test Loss: 0.7138\n",
      "Epoch 51, Training Loss: 0.5423, Validation Loss: 0.3674, Test Loss: 0.6705\n",
      "Epoch 61, Training Loss: 0.5006, Validation Loss: 0.3288, Test Loss: 0.6131\n",
      "Epoch 71, Training Loss: 0.4753, Validation Loss: 0.3196, Test Loss: 0.6233\n",
      "Epoch 81, Training Loss: 0.4948, Validation Loss: 0.3283, Test Loss: 0.6655\n",
      "Epoch 91, Training Loss: 0.4336, Validation Loss: 0.2845, Test Loss: 0.5681\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.4336\n",
      "Validation loss: 0.2845\n",
      "Test loss: 0.5681\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 182/243 with parameters:\n",
      "Config indices: (2, 0, 2, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6874.9204, Validation Loss: 6278.2837, Test Loss: 6880.7686\n",
      "Epoch 11, Training Loss: 0.8397, Validation Loss: 0.5423, Test Loss: 1.0082\n",
      "Epoch 21, Training Loss: 0.6551, Validation Loss: 0.4075, Test Loss: 0.7811\n",
      "Epoch 31, Training Loss: 0.5596, Validation Loss: 0.3350, Test Loss: 0.7274\n",
      "Epoch 41, Training Loss: 0.5097, Validation Loss: 0.3099, Test Loss: 0.6625\n",
      "Epoch 51, Training Loss: 0.5325, Validation Loss: 0.3285, Test Loss: 0.7114\n",
      "Epoch 61, Training Loss: 0.5360, Validation Loss: 0.3392, Test Loss: 0.7285\n",
      "Epoch 71, Training Loss: 0.4552, Validation Loss: 0.2851, Test Loss: 0.6193\n",
      "Epoch 81, Training Loss: 0.4403, Validation Loss: 0.2826, Test Loss: 0.5895\n",
      "Epoch 91, Training Loss: 0.4280, Validation Loss: 0.2733, Test Loss: 0.5837\n",
      "Epoch 101, Training Loss: 0.4069, Validation Loss: 0.2655, Test Loss: 0.5547\n",
      "Epoch 111, Training Loss: 0.4233, Validation Loss: 0.2792, Test Loss: 0.5881\n",
      "Epoch 121, Training Loss: 0.3871, Validation Loss: 0.2568, Test Loss: 0.5320\n",
      "Epoch 131, Training Loss: 0.4074, Validation Loss: 0.2766, Test Loss: 0.5146\n",
      "Epoch 141, Training Loss: 0.3871, Validation Loss: 0.2507, Test Loss: 0.5339\n",
      "Epoch 151, Training Loss: 0.3577, Validation Loss: 0.2359, Test Loss: 0.4873\n",
      "Epoch 161, Training Loss: 0.3462, Validation Loss: 0.2305, Test Loss: 0.4789\n",
      "Epoch 171, Training Loss: 0.3843, Validation Loss: 0.2667, Test Loss: 0.4773\n",
      "Epoch 181, Training Loss: 0.3397, Validation Loss: 0.2237, Test Loss: 0.4716\n",
      "Epoch 191, Training Loss: 0.3428, Validation Loss: 0.2346, Test Loss: 0.4381\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.3428\n",
      "Validation loss: 0.2346\n",
      "Test loss: 0.4381\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 183/243 with parameters:\n",
      "Config indices: (2, 0, 2, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6876.1387, Validation Loss: 6280.2139, Test Loss: 6881.8853\n",
      "Epoch 11, Training Loss: 0.8938, Validation Loss: 0.6042, Test Loss: 0.9753\n",
      "Epoch 21, Training Loss: 0.7146, Validation Loss: 0.4773, Test Loss: 0.8359\n",
      "Epoch 31, Training Loss: 0.6532, Validation Loss: 0.4214, Test Loss: 0.7810\n",
      "Epoch 41, Training Loss: 0.6011, Validation Loss: 0.4084, Test Loss: 0.7143\n",
      "Epoch 51, Training Loss: 0.5577, Validation Loss: 0.3510, Test Loss: 0.6759\n",
      "Epoch 61, Training Loss: 0.5369, Validation Loss: 0.3456, Test Loss: 0.6743\n",
      "Epoch 71, Training Loss: 0.5093, Validation Loss: 0.3286, Test Loss: 0.6433\n",
      "Epoch 81, Training Loss: 0.5331, Validation Loss: 0.3633, Test Loss: 0.6171\n",
      "Epoch 91, Training Loss: 0.5052, Validation Loss: 0.3425, Test Loss: 0.5937\n",
      "Epoch 101, Training Loss: 0.4587, Validation Loss: 0.2825, Test Loss: 0.5753\n",
      "Epoch 111, Training Loss: 0.4645, Validation Loss: 0.2914, Test Loss: 0.5974\n",
      "Epoch 121, Training Loss: 0.4595, Validation Loss: 0.2918, Test Loss: 0.5956\n",
      "Epoch 131, Training Loss: 0.4345, Validation Loss: 0.2853, Test Loss: 0.5212\n",
      "Epoch 141, Training Loss: 0.4149, Validation Loss: 0.2726, Test Loss: 0.5424\n",
      "Epoch 151, Training Loss: 0.3904, Validation Loss: 0.2539, Test Loss: 0.4894\n",
      "Epoch 161, Training Loss: 0.3926, Validation Loss: 0.2492, Test Loss: 0.5128\n",
      "Epoch 171, Training Loss: 0.3985, Validation Loss: 0.2598, Test Loss: 0.5240\n",
      "Epoch 181, Training Loss: 0.3685, Validation Loss: 0.2422, Test Loss: 0.4579\n",
      "Epoch 191, Training Loss: 0.3559, Validation Loss: 0.2384, Test Loss: 0.4694\n",
      "Epoch 201, Training Loss: 0.3435, Validation Loss: 0.2203, Test Loss: 0.4531\n",
      "Epoch 211, Training Loss: 0.3474, Validation Loss: 0.2460, Test Loss: 0.4512\n",
      "Epoch 221, Training Loss: 0.3579, Validation Loss: 0.2459, Test Loss: 0.4377\n",
      "Epoch 231, Training Loss: 0.3177, Validation Loss: 0.2110, Test Loss: 0.4193\n",
      "Epoch 241, Training Loss: 0.3182, Validation Loss: 0.2189, Test Loss: 0.4069\n",
      "Epoch 251, Training Loss: 0.3066, Validation Loss: 0.2094, Test Loss: 0.4112\n",
      "Epoch 261, Training Loss: 0.3018, Validation Loss: 0.2049, Test Loss: 0.4072\n",
      "Epoch 271, Training Loss: 0.2975, Validation Loss: 0.2067, Test Loss: 0.3848\n",
      "Epoch 281, Training Loss: 0.2927, Validation Loss: 0.2031, Test Loss: 0.3781\n",
      "Epoch 291, Training Loss: 0.2861, Validation Loss: 0.1935, Test Loss: 0.3816\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2861\n",
      "Validation loss: 0.1935\n",
      "Test loss: 0.3816\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 184/243 with parameters:\n",
      "Config indices: (2, 0, 2, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7042.1357, Validation Loss: 6437.3008, Test Loss: 7048.5049\n",
      "Epoch 11, Training Loss: 4.1830, Validation Loss: 3.7557, Test Loss: 3.1742\n",
      "Epoch 21, Training Loss: 1.1204, Validation Loss: 0.9024, Test Loss: 1.0402\n",
      "Epoch 31, Training Loss: 0.8340, Validation Loss: 0.6057, Test Loss: 0.8787\n",
      "Epoch 41, Training Loss: 0.6778, Validation Loss: 0.4485, Test Loss: 0.7726\n",
      "Epoch 51, Training Loss: 0.6366, Validation Loss: 0.3959, Test Loss: 0.7836\n",
      "Epoch 61, Training Loss: 0.5463, Validation Loss: 0.3406, Test Loss: 0.6577\n",
      "Epoch 71, Training Loss: 0.5284, Validation Loss: 0.3483, Test Loss: 0.6450\n",
      "Epoch 81, Training Loss: 0.5043, Validation Loss: 0.3060, Test Loss: 0.6436\n",
      "Epoch 91, Training Loss: 0.4726, Validation Loss: 0.3036, Test Loss: 0.5879\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.4726\n",
      "Validation loss: 0.3036\n",
      "Test loss: 0.5879\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 185/243 with parameters:\n",
      "Config indices: (2, 0, 2, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7047.7500, Validation Loss: 6442.6372, Test Loss: 7054.1387\n",
      "Epoch 11, Training Loss: 2.2316, Validation Loss: 1.6942, Test Loss: 1.8231\n",
      "Epoch 21, Training Loss: 1.0722, Validation Loss: 0.8129, Test Loss: 1.0754\n",
      "Epoch 31, Training Loss: 0.8595, Validation Loss: 0.6727, Test Loss: 0.9116\n",
      "Epoch 41, Training Loss: 0.7861, Validation Loss: 0.6087, Test Loss: 0.8375\n",
      "Epoch 51, Training Loss: 0.7349, Validation Loss: 0.5512, Test Loss: 0.8103\n",
      "Epoch 61, Training Loss: 0.7070, Validation Loss: 0.5102, Test Loss: 0.8059\n",
      "Epoch 71, Training Loss: 0.6541, Validation Loss: 0.4684, Test Loss: 0.7172\n",
      "Epoch 81, Training Loss: 0.6117, Validation Loss: 0.4208, Test Loss: 0.7083\n",
      "Epoch 91, Training Loss: 0.5847, Validation Loss: 0.4023, Test Loss: 0.6757\n",
      "Epoch 101, Training Loss: 0.5641, Validation Loss: 0.3847, Test Loss: 0.6554\n",
      "Epoch 111, Training Loss: 0.5569, Validation Loss: 0.3720, Test Loss: 0.6715\n",
      "Epoch 121, Training Loss: 0.5498, Validation Loss: 0.3716, Test Loss: 0.6661\n",
      "Epoch 131, Training Loss: 0.5253, Validation Loss: 0.3558, Test Loss: 0.6287\n",
      "Epoch 141, Training Loss: 0.5051, Validation Loss: 0.3394, Test Loss: 0.6127\n",
      "Epoch 151, Training Loss: 0.4989, Validation Loss: 0.3340, Test Loss: 0.5962\n",
      "Epoch 161, Training Loss: 0.4928, Validation Loss: 0.3293, Test Loss: 0.6157\n",
      "Epoch 171, Training Loss: 0.4957, Validation Loss: 0.3329, Test Loss: 0.6272\n",
      "Epoch 181, Training Loss: 0.4679, Validation Loss: 0.3108, Test Loss: 0.5858\n",
      "Epoch 191, Training Loss: 0.4687, Validation Loss: 0.3263, Test Loss: 0.5630\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.4687\n",
      "Validation loss: 0.3263\n",
      "Test loss: 0.5630\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 186/243 with parameters:\n",
      "Config indices: (2, 0, 2, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7052.4941, Validation Loss: 6447.1299, Test Loss: 7058.9053\n",
      "Epoch 11, Training Loss: 3.4646, Validation Loss: 2.9783, Test Loss: 2.8841\n",
      "Epoch 21, Training Loss: 1.3161, Validation Loss: 1.0645, Test Loss: 1.3297\n",
      "Epoch 31, Training Loss: 0.9883, Validation Loss: 0.7650, Test Loss: 1.0783\n",
      "Epoch 41, Training Loss: 0.8329, Validation Loss: 0.6003, Test Loss: 0.9327\n",
      "Epoch 51, Training Loss: 0.7629, Validation Loss: 0.5562, Test Loss: 0.8600\n",
      "Epoch 61, Training Loss: 0.6630, Validation Loss: 0.4480, Test Loss: 0.8197\n",
      "Epoch 71, Training Loss: 0.6254, Validation Loss: 0.4092, Test Loss: 0.7845\n",
      "Epoch 81, Training Loss: 0.6033, Validation Loss: 0.3969, Test Loss: 0.7608\n",
      "Epoch 91, Training Loss: 0.5690, Validation Loss: 0.3688, Test Loss: 0.7005\n",
      "Epoch 101, Training Loss: 0.5422, Validation Loss: 0.3429, Test Loss: 0.6902\n",
      "Epoch 111, Training Loss: 0.5379, Validation Loss: 0.3223, Test Loss: 0.6945\n",
      "Epoch 121, Training Loss: 0.5100, Validation Loss: 0.3122, Test Loss: 0.6550\n",
      "Epoch 131, Training Loss: 0.5166, Validation Loss: 0.3207, Test Loss: 0.6897\n",
      "Epoch 141, Training Loss: 0.4972, Validation Loss: 0.3042, Test Loss: 0.6644\n",
      "Epoch 151, Training Loss: 0.4733, Validation Loss: 0.2973, Test Loss: 0.6249\n",
      "Epoch 161, Training Loss: 0.4803, Validation Loss: 0.2962, Test Loss: 0.6504\n",
      "Epoch 171, Training Loss: 0.4695, Validation Loss: 0.2955, Test Loss: 0.6161\n",
      "Epoch 181, Training Loss: 0.4520, Validation Loss: 0.2804, Test Loss: 0.6153\n",
      "Epoch 191, Training Loss: 0.4992, Validation Loss: 0.3150, Test Loss: 0.6875\n",
      "Epoch 201, Training Loss: 0.4438, Validation Loss: 0.2857, Test Loss: 0.5825\n",
      "Epoch 211, Training Loss: 0.4320, Validation Loss: 0.2740, Test Loss: 0.5947\n",
      "Epoch 221, Training Loss: 0.4251, Validation Loss: 0.2665, Test Loss: 0.5725\n",
      "Epoch 231, Training Loss: 0.4951, Validation Loss: 0.3628, Test Loss: 0.6076\n",
      "Epoch 241, Training Loss: 0.4232, Validation Loss: 0.2812, Test Loss: 0.5599\n",
      "Epoch 251, Training Loss: 0.4120, Validation Loss: 0.2576, Test Loss: 0.5649\n",
      "Epoch 261, Training Loss: 0.4056, Validation Loss: 0.2537, Test Loss: 0.5625\n",
      "Epoch 271, Training Loss: 0.3948, Validation Loss: 0.2508, Test Loss: 0.5435\n",
      "Epoch 281, Training Loss: 0.3931, Validation Loss: 0.2506, Test Loss: 0.5288\n",
      "Epoch 291, Training Loss: 0.4060, Validation Loss: 0.2590, Test Loss: 0.5699\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.4060\n",
      "Validation loss: 0.2590\n",
      "Test loss: 0.5699\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 187/243 with parameters:\n",
      "Config indices: (2, 0, 2, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7107.1382, Validation Loss: 6499.0947, Test Loss: 7113.7427\n",
      "Epoch 11, Training Loss: 43.4294, Validation Loss: 43.2727, Test Loss: 37.6756\n",
      "Epoch 21, Training Loss: 2.7448, Validation Loss: 2.7279, Test Loss: 2.3189\n",
      "Epoch 31, Training Loss: 2.2526, Validation Loss: 2.0655, Test Loss: 1.9316\n",
      "Epoch 41, Training Loss: 1.9722, Validation Loss: 1.6773, Test Loss: 1.6703\n",
      "Epoch 51, Training Loss: 1.6905, Validation Loss: 1.4038, Test Loss: 1.4651\n",
      "Epoch 61, Training Loss: 1.3816, Validation Loss: 1.0917, Test Loss: 1.2251\n",
      "Epoch 71, Training Loss: 1.1286, Validation Loss: 0.8475, Test Loss: 1.0247\n",
      "Epoch 81, Training Loss: 0.9635, Validation Loss: 0.7019, Test Loss: 0.9224\n",
      "Epoch 91, Training Loss: 0.8506, Validation Loss: 0.5938, Test Loss: 0.8628\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.8506\n",
      "Validation loss: 0.5938\n",
      "Test loss: 0.8628\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 188/243 with parameters:\n",
      "Config indices: (2, 0, 2, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7112.2754, Validation Loss: 6504.0566, Test Loss: 7118.8745\n",
      "Epoch 11, Training Loss: 71.2242, Validation Loss: 72.1252, Test Loss: 66.0511\n",
      "Epoch 21, Training Loss: 3.8618, Validation Loss: 3.4413, Test Loss: 3.0519\n",
      "Epoch 31, Training Loss: 1.7179, Validation Loss: 1.5419, Test Loss: 1.5772\n",
      "Epoch 41, Training Loss: 1.4280, Validation Loss: 1.2790, Test Loss: 1.4102\n",
      "Epoch 51, Training Loss: 1.3473, Validation Loss: 1.1657, Test Loss: 1.3937\n",
      "Epoch 61, Training Loss: 1.2804, Validation Loss: 1.0948, Test Loss: 1.2565\n",
      "Epoch 71, Training Loss: 1.1576, Validation Loss: 0.9675, Test Loss: 1.1728\n",
      "Epoch 81, Training Loss: 1.1412, Validation Loss: 0.9336, Test Loss: 1.2050\n",
      "Epoch 91, Training Loss: 1.0464, Validation Loss: 0.8414, Test Loss: 1.1009\n",
      "Epoch 101, Training Loss: 1.0132, Validation Loss: 0.8132, Test Loss: 1.0824\n",
      "Epoch 111, Training Loss: 0.9417, Validation Loss: 0.7409, Test Loss: 1.0057\n",
      "Epoch 121, Training Loss: 0.8526, Validation Loss: 0.6742, Test Loss: 0.8744\n",
      "Epoch 131, Training Loss: 0.8132, Validation Loss: 0.6301, Test Loss: 0.8325\n",
      "Epoch 141, Training Loss: 0.7771, Validation Loss: 0.5843, Test Loss: 0.7976\n",
      "Epoch 151, Training Loss: 0.7719, Validation Loss: 0.5810, Test Loss: 0.8217\n",
      "Epoch 161, Training Loss: 0.7269, Validation Loss: 0.5526, Test Loss: 0.7461\n",
      "Epoch 171, Training Loss: 0.6840, Validation Loss: 0.5009, Test Loss: 0.7204\n",
      "Epoch 181, Training Loss: 0.6748, Validation Loss: 0.5020, Test Loss: 0.6940\n",
      "Epoch 191, Training Loss: 0.7261, Validation Loss: 0.5507, Test Loss: 0.7134\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.7261\n",
      "Validation loss: 0.5507\n",
      "Test loss: 0.7134\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 189/243 with parameters:\n",
      "Config indices: (2, 0, 2, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.gradient_descent.GradientDescentOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7111.6118, Validation Loss: 6503.3755, Test Loss: 7118.2393\n",
      "Epoch 11, Training Loss: 79.3316, Validation Loss: 80.5743, Test Loss: 72.2207\n",
      "Epoch 21, Training Loss: 4.3417, Validation Loss: 3.7116, Test Loss: 3.4088\n",
      "Epoch 31, Training Loss: 1.8934, Validation Loss: 1.4213, Test Loss: 1.5518\n",
      "Epoch 41, Training Loss: 1.2133, Validation Loss: 0.8726, Test Loss: 1.0851\n",
      "Epoch 51, Training Loss: 0.9742, Validation Loss: 0.7034, Test Loss: 0.9273\n",
      "Epoch 61, Training Loss: 0.8074, Validation Loss: 0.5703, Test Loss: 0.8509\n",
      "Epoch 71, Training Loss: 0.7402, Validation Loss: 0.5226, Test Loss: 0.8287\n",
      "Epoch 81, Training Loss: 0.6776, Validation Loss: 0.4806, Test Loss: 0.7653\n",
      "Epoch 91, Training Loss: 0.6503, Validation Loss: 0.4655, Test Loss: 0.7255\n",
      "Epoch 101, Training Loss: 0.6364, Validation Loss: 0.4581, Test Loss: 0.7117\n",
      "Epoch 111, Training Loss: 0.6133, Validation Loss: 0.4455, Test Loss: 0.6977\n",
      "Epoch 121, Training Loss: 0.6302, Validation Loss: 0.4628, Test Loss: 0.7018\n",
      "Epoch 131, Training Loss: 0.5819, Validation Loss: 0.4180, Test Loss: 0.6811\n",
      "Epoch 141, Training Loss: 0.5665, Validation Loss: 0.4004, Test Loss: 0.7103\n",
      "Epoch 151, Training Loss: 0.5496, Validation Loss: 0.3868, Test Loss: 0.6871\n",
      "Epoch 161, Training Loss: 0.5561, Validation Loss: 0.3917, Test Loss: 0.7143\n",
      "Epoch 171, Training Loss: 0.5629, Validation Loss: 0.3909, Test Loss: 0.7254\n",
      "Epoch 181, Training Loss: 0.5193, Validation Loss: 0.3714, Test Loss: 0.6519\n",
      "Epoch 191, Training Loss: 0.5119, Validation Loss: 0.3602, Test Loss: 0.6456\n",
      "Epoch 201, Training Loss: 0.5300, Validation Loss: 0.3709, Test Loss: 0.6990\n",
      "Epoch 211, Training Loss: 0.6113, Validation Loss: 0.4311, Test Loss: 0.8102\n",
      "Epoch 221, Training Loss: 0.5010, Validation Loss: 0.3511, Test Loss: 0.6609\n",
      "Epoch 231, Training Loss: 0.5121, Validation Loss: 0.3596, Test Loss: 0.6857\n",
      "Epoch 241, Training Loss: 0.5383, Validation Loss: 0.4083, Test Loss: 0.6436\n",
      "Epoch 251, Training Loss: 0.4786, Validation Loss: 0.3344, Test Loss: 0.6354\n",
      "Epoch 261, Training Loss: 0.4691, Validation Loss: 0.3321, Test Loss: 0.6205\n",
      "Epoch 271, Training Loss: 0.4978, Validation Loss: 0.3454, Test Loss: 0.6745\n",
      "Epoch 281, Training Loss: 0.4595, Validation Loss: 0.3193, Test Loss: 0.6041\n",
      "Epoch 291, Training Loss: 0.4536, Validation Loss: 0.3148, Test Loss: 0.5993\n",
      "Optimizer: GradientDescentOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.4536\n",
      "Validation loss: 0.3148\n",
      "Test loss: 0.5993\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 190/243 with parameters:\n",
      "Config indices: (2, 1, 0, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 196.9075, Validation Loss: 202.6235, Test Loss: 192.6044\n",
      "Epoch 11, Training Loss: 0.5680, Validation Loss: 0.3645, Test Loss: 0.7623\n",
      "Epoch 21, Training Loss: 0.4950, Validation Loss: 0.3217, Test Loss: 0.6718\n",
      "Epoch 31, Training Loss: 1.4302, Validation Loss: 1.2249, Test Loss: 1.4110\n",
      "Epoch 41, Training Loss: 0.7637, Validation Loss: 0.6107, Test Loss: 0.7902\n",
      "Epoch 51, Training Loss: 0.4830, Validation Loss: 0.3404, Test Loss: 0.5037\n",
      "Epoch 61, Training Loss: 1.0204, Validation Loss: 0.9089, Test Loss: 1.2013\n",
      "Epoch 71, Training Loss: 0.5764, Validation Loss: 0.4974, Test Loss: 0.5905\n",
      "Epoch 81, Training Loss: 0.5949, Validation Loss: 0.5021, Test Loss: 0.6152\n",
      "Epoch 91, Training Loss: 0.2546, Validation Loss: 0.1932, Test Loss: 0.3130\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.2546\n",
      "Validation loss: 0.1932\n",
      "Test loss: 0.3130\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 191/243 with parameters:\n",
      "Config indices: (2, 1, 0, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 234.8949, Validation Loss: 239.0935, Test Loss: 245.8531\n",
      "Epoch 11, Training Loss: 0.4467, Validation Loss: 0.2857, Test Loss: 0.6098\n",
      "Epoch 21, Training Loss: 0.4182, Validation Loss: 0.2876, Test Loss: 0.5809\n",
      "Epoch 31, Training Loss: 0.5045, Validation Loss: 0.3838, Test Loss: 0.6875\n",
      "Epoch 41, Training Loss: 0.3376, Validation Loss: 0.2438, Test Loss: 0.4695\n",
      "Epoch 51, Training Loss: 0.4091, Validation Loss: 0.3211, Test Loss: 0.4595\n",
      "Epoch 61, Training Loss: 2.2708, Validation Loss: 2.1671, Test Loss: 2.3656\n",
      "Epoch 71, Training Loss: 0.5749, Validation Loss: 0.4439, Test Loss: 0.5718\n",
      "Epoch 81, Training Loss: 1.2177, Validation Loss: 1.1337, Test Loss: 1.3842\n",
      "Epoch 91, Training Loss: 0.7378, Validation Loss: 0.6263, Test Loss: 0.7487\n",
      "Epoch 101, Training Loss: 0.3456, Validation Loss: 0.2634, Test Loss: 0.3783\n",
      "Epoch 111, Training Loss: 0.4110, Validation Loss: 0.3453, Test Loss: 0.5086\n",
      "Epoch 121, Training Loss: 0.2679, Validation Loss: 0.2261, Test Loss: 0.3633\n",
      "Epoch 131, Training Loss: 0.2460, Validation Loss: 0.2046, Test Loss: 0.3323\n",
      "Epoch 141, Training Loss: 0.8773, Validation Loss: 0.8902, Test Loss: 0.9817\n",
      "Epoch 151, Training Loss: 0.8951, Validation Loss: 0.8755, Test Loss: 1.0160\n",
      "Epoch 161, Training Loss: 0.3700, Validation Loss: 0.3208, Test Loss: 0.4830\n",
      "Epoch 171, Training Loss: 0.4649, Validation Loss: 0.3507, Test Loss: 0.4722\n",
      "Epoch 181, Training Loss: 0.6092, Validation Loss: 0.6063, Test Loss: 0.6220\n",
      "Epoch 191, Training Loss: 0.2169, Validation Loss: 0.1665, Test Loss: 0.2868\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.2169\n",
      "Validation loss: 0.1665\n",
      "Test loss: 0.2868\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 192/243 with parameters:\n",
      "Config indices: (2, 1, 0, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 221.6276, Validation Loss: 223.2292, Test Loss: 225.6935\n",
      "Epoch 11, Training Loss: 0.5862, Validation Loss: 0.3714, Test Loss: 0.6353\n",
      "Epoch 21, Training Loss: 0.5487, Validation Loss: 0.4186, Test Loss: 0.6719\n",
      "Epoch 31, Training Loss: 0.4424, Validation Loss: 0.3426, Test Loss: 0.5057\n",
      "Epoch 41, Training Loss: 0.3660, Validation Loss: 0.2776, Test Loss: 0.4202\n",
      "Epoch 51, Training Loss: 0.5228, Validation Loss: 0.3882, Test Loss: 0.5319\n",
      "Epoch 61, Training Loss: 0.5710, Validation Loss: 0.5424, Test Loss: 0.6637\n",
      "Epoch 71, Training Loss: 0.4708, Validation Loss: 0.3723, Test Loss: 0.5704\n",
      "Epoch 81, Training Loss: 0.6329, Validation Loss: 0.5952, Test Loss: 0.6726\n",
      "Epoch 91, Training Loss: 0.6932, Validation Loss: 0.5366, Test Loss: 0.6780\n",
      "Epoch 101, Training Loss: 1.0279, Validation Loss: 0.8753, Test Loss: 0.9933\n",
      "Epoch 111, Training Loss: 0.5333, Validation Loss: 0.4649, Test Loss: 0.5490\n",
      "Epoch 121, Training Loss: 2.5138, Validation Loss: 2.4716, Test Loss: 2.7022\n",
      "Epoch 131, Training Loss: 0.2233, Validation Loss: 0.1690, Test Loss: 0.2821\n",
      "Epoch 141, Training Loss: 0.3160, Validation Loss: 0.2474, Test Loss: 0.3592\n",
      "Epoch 151, Training Loss: 0.2222, Validation Loss: 0.1675, Test Loss: 0.2910\n",
      "Epoch 161, Training Loss: 0.5694, Validation Loss: 0.4864, Test Loss: 0.5632\n",
      "Epoch 171, Training Loss: 0.6643, Validation Loss: 0.5298, Test Loss: 0.6556\n",
      "Epoch 181, Training Loss: 0.3199, Validation Loss: 0.2815, Test Loss: 0.3985\n",
      "Epoch 191, Training Loss: 0.5894, Validation Loss: 0.5705, Test Loss: 0.6951\n",
      "Epoch 201, Training Loss: 0.3370, Validation Loss: 0.2849, Test Loss: 0.3888\n",
      "Epoch 211, Training Loss: 0.2577, Validation Loss: 0.2086, Test Loss: 0.3347\n",
      "Epoch 221, Training Loss: 0.7085, Validation Loss: 0.6839, Test Loss: 0.7418\n",
      "Epoch 231, Training Loss: 1.2209, Validation Loss: 1.0742, Test Loss: 1.4017\n",
      "Epoch 241, Training Loss: 0.2420, Validation Loss: 0.2088, Test Loss: 0.3318\n",
      "Epoch 251, Training Loss: 0.2127, Validation Loss: 0.1568, Test Loss: 0.2898\n",
      "Epoch 261, Training Loss: 0.3023, Validation Loss: 0.2628, Test Loss: 0.3858\n",
      "Epoch 271, Training Loss: 0.5699, Validation Loss: 0.4703, Test Loss: 0.5802\n",
      "Epoch 281, Training Loss: 0.2389, Validation Loss: 0.1754, Test Loss: 0.2876\n",
      "Epoch 291, Training Loss: 0.2162, Validation Loss: 0.1778, Test Loss: 0.2851\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2162\n",
      "Validation loss: 0.1778\n",
      "Test loss: 0.2851\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 193/243 with parameters:\n",
      "Config indices: (2, 1, 0, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 879.9292, Validation Loss: 883.8896, Test Loss: 852.4285\n",
      "Epoch 11, Training Loss: 0.6131, Validation Loss: 0.3566, Test Loss: 0.8211\n",
      "Epoch 21, Training Loss: 0.5661, Validation Loss: 0.3632, Test Loss: 0.7436\n",
      "Epoch 31, Training Loss: 0.5596, Validation Loss: 0.3806, Test Loss: 0.6538\n",
      "Epoch 41, Training Loss: 0.4714, Validation Loss: 0.3073, Test Loss: 0.6172\n",
      "Epoch 51, Training Loss: 0.4233, Validation Loss: 0.2925, Test Loss: 0.5304\n",
      "Epoch 61, Training Loss: 0.3645, Validation Loss: 0.2590, Test Loss: 0.4743\n",
      "Epoch 71, Training Loss: 0.3339, Validation Loss: 0.2192, Test Loss: 0.4354\n",
      "Epoch 81, Training Loss: 0.5564, Validation Loss: 0.4648, Test Loss: 0.7170\n",
      "Epoch 91, Training Loss: 0.5168, Validation Loss: 0.4125, Test Loss: 0.6584\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.5168\n",
      "Validation loss: 0.4125\n",
      "Test loss: 0.6584\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 194/243 with parameters:\n",
      "Config indices: (2, 1, 0, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 958.4579, Validation Loss: 1000.1387, Test Loss: 904.5017\n",
      "Epoch 11, Training Loss: 0.4883, Validation Loss: 0.3022, Test Loss: 0.6270\n",
      "Epoch 21, Training Loss: 0.4531, Validation Loss: 0.2968, Test Loss: 0.5650\n",
      "Epoch 31, Training Loss: 0.4406, Validation Loss: 0.3121, Test Loss: 0.5467\n",
      "Epoch 41, Training Loss: 0.4298, Validation Loss: 0.2973, Test Loss: 0.5676\n",
      "Epoch 51, Training Loss: 0.4074, Validation Loss: 0.3001, Test Loss: 0.5023\n",
      "Epoch 61, Training Loss: 0.4000, Validation Loss: 0.2962, Test Loss: 0.5364\n",
      "Epoch 71, Training Loss: 0.7229, Validation Loss: 0.6687, Test Loss: 0.8743\n",
      "Epoch 81, Training Loss: 0.3626, Validation Loss: 0.3022, Test Loss: 0.4725\n",
      "Epoch 91, Training Loss: 0.6894, Validation Loss: 0.5562, Test Loss: 0.6953\n",
      "Epoch 101, Training Loss: 0.5875, Validation Loss: 0.5227, Test Loss: 0.6347\n",
      "Epoch 111, Training Loss: 0.3558, Validation Loss: 0.2914, Test Loss: 0.4066\n",
      "Epoch 121, Training Loss: 0.6241, Validation Loss: 0.5375, Test Loss: 0.7555\n",
      "Epoch 131, Training Loss: 0.4127, Validation Loss: 0.3476, Test Loss: 0.5261\n",
      "Epoch 141, Training Loss: 0.3546, Validation Loss: 0.3269, Test Loss: 0.4417\n",
      "Epoch 151, Training Loss: 0.2989, Validation Loss: 0.2264, Test Loss: 0.3423\n",
      "Epoch 161, Training Loss: 0.2320, Validation Loss: 0.1840, Test Loss: 0.3082\n",
      "Epoch 171, Training Loss: 0.2080, Validation Loss: 0.1571, Test Loss: 0.2779\n",
      "Epoch 181, Training Loss: 0.2339, Validation Loss: 0.1712, Test Loss: 0.2789\n",
      "Epoch 191, Training Loss: 0.2869, Validation Loss: 0.2571, Test Loss: 0.3444\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.2869\n",
      "Validation loss: 0.2571\n",
      "Test loss: 0.3444\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 195/243 with parameters:\n",
      "Config indices: (2, 1, 0, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 1183.2776, Validation Loss: 1172.4603, Test Loss: 1190.4918\n",
      "Epoch 11, Training Loss: 0.9949, Validation Loss: 0.7885, Test Loss: 1.1882\n",
      "Epoch 21, Training Loss: 0.6682, Validation Loss: 0.4779, Test Loss: 0.7829\n",
      "Epoch 31, Training Loss: 0.6799, Validation Loss: 0.5040, Test Loss: 0.7858\n",
      "Epoch 41, Training Loss: 0.5277, Validation Loss: 0.3467, Test Loss: 0.6883\n",
      "Epoch 51, Training Loss: 0.6735, Validation Loss: 0.4845, Test Loss: 0.8581\n",
      "Epoch 61, Training Loss: 0.5148, Validation Loss: 0.3816, Test Loss: 0.5694\n",
      "Epoch 71, Training Loss: 0.7475, Validation Loss: 0.6147, Test Loss: 0.7621\n",
      "Epoch 81, Training Loss: 0.5480, Validation Loss: 0.4447, Test Loss: 0.5918\n",
      "Epoch 91, Training Loss: 0.4117, Validation Loss: 0.3553, Test Loss: 0.5083\n",
      "Epoch 101, Training Loss: 0.3715, Validation Loss: 0.3158, Test Loss: 0.4237\n",
      "Epoch 111, Training Loss: 0.3608, Validation Loss: 0.3090, Test Loss: 0.4091\n",
      "Epoch 121, Training Loss: 0.8644, Validation Loss: 0.6738, Test Loss: 0.8277\n",
      "Epoch 131, Training Loss: 0.2752, Validation Loss: 0.2287, Test Loss: 0.3239\n",
      "Epoch 141, Training Loss: 0.8221, Validation Loss: 0.7434, Test Loss: 0.9689\n",
      "Epoch 151, Training Loss: 0.8312, Validation Loss: 0.7323, Test Loss: 0.9644\n",
      "Epoch 161, Training Loss: 0.3267, Validation Loss: 0.2498, Test Loss: 0.3584\n",
      "Epoch 171, Training Loss: 0.4249, Validation Loss: 0.3373, Test Loss: 0.4471\n",
      "Epoch 181, Training Loss: 0.2791, Validation Loss: 0.2467, Test Loss: 0.3589\n",
      "Epoch 191, Training Loss: 0.8003, Validation Loss: 0.7111, Test Loss: 0.9407\n",
      "Epoch 201, Training Loss: 0.4615, Validation Loss: 0.3996, Test Loss: 0.5782\n",
      "Epoch 211, Training Loss: 0.4042, Validation Loss: 0.3754, Test Loss: 0.4437\n",
      "Epoch 221, Training Loss: 0.3974, Validation Loss: 0.3421, Test Loss: 0.4788\n",
      "Epoch 231, Training Loss: 1.4204, Validation Loss: 1.2714, Test Loss: 1.4140\n",
      "Epoch 241, Training Loss: 0.2563, Validation Loss: 0.2139, Test Loss: 0.3427\n",
      "Epoch 251, Training Loss: 0.2370, Validation Loss: 0.1955, Test Loss: 0.3231\n",
      "Epoch 261, Training Loss: 0.2649, Validation Loss: 0.2200, Test Loss: 0.3548\n",
      "Epoch 271, Training Loss: 0.3017, Validation Loss: 0.2772, Test Loss: 0.3637\n",
      "Epoch 281, Training Loss: 0.3291, Validation Loss: 0.2804, Test Loss: 0.4289\n",
      "Epoch 291, Training Loss: 0.2880, Validation Loss: 0.2486, Test Loss: 0.3786\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.2880\n",
      "Validation loss: 0.2486\n",
      "Test loss: 0.3786\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 196/243 with parameters:\n",
      "Config indices: (2, 1, 0, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6355.3691, Validation Loss: 5807.3271, Test Loss: 6368.8457\n",
      "Epoch 11, Training Loss: 1.0790, Validation Loss: 0.7583, Test Loss: 1.2468\n",
      "Epoch 21, Training Loss: 0.6032, Validation Loss: 0.3696, Test Loss: 0.7750\n",
      "Epoch 31, Training Loss: 0.5400, Validation Loss: 0.3330, Test Loss: 0.6783\n",
      "Epoch 41, Training Loss: 0.5129, Validation Loss: 0.3288, Test Loss: 0.6545\n",
      "Epoch 51, Training Loss: 0.4929, Validation Loss: 0.3122, Test Loss: 0.6395\n",
      "Epoch 61, Training Loss: 0.4836, Validation Loss: 0.3152, Test Loss: 0.6357\n",
      "Epoch 71, Training Loss: 0.4414, Validation Loss: 0.2776, Test Loss: 0.5637\n",
      "Epoch 81, Training Loss: 0.4895, Validation Loss: 0.3563, Test Loss: 0.5777\n",
      "Epoch 91, Training Loss: 0.5635, Validation Loss: 0.3956, Test Loss: 0.7348\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.5635\n",
      "Validation loss: 0.3956\n",
      "Test loss: 0.7348\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 197/243 with parameters:\n",
      "Config indices: (2, 1, 0, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6026.6265, Validation Loss: 5453.3501, Test Loss: 6042.4116\n",
      "Epoch 11, Training Loss: 1.6909, Validation Loss: 1.2157, Test Loss: 1.8292\n",
      "Epoch 21, Training Loss: 0.6583, Validation Loss: 0.3988, Test Loss: 0.8409\n",
      "Epoch 31, Training Loss: 0.6266, Validation Loss: 0.3870, Test Loss: 0.7783\n",
      "Epoch 41, Training Loss: 0.6111, Validation Loss: 0.3839, Test Loss: 0.7745\n",
      "Epoch 51, Training Loss: 0.5938, Validation Loss: 0.3655, Test Loss: 0.7483\n",
      "Epoch 61, Training Loss: 0.5833, Validation Loss: 0.3751, Test Loss: 0.7449\n",
      "Epoch 71, Training Loss: 0.6743, Validation Loss: 0.4795, Test Loss: 0.7529\n",
      "Epoch 81, Training Loss: 0.6383, Validation Loss: 0.4171, Test Loss: 0.7056\n",
      "Epoch 91, Training Loss: 0.5200, Validation Loss: 0.3673, Test Loss: 0.6387\n",
      "Epoch 101, Training Loss: 0.5907, Validation Loss: 0.4094, Test Loss: 0.7748\n",
      "Epoch 111, Training Loss: 0.4520, Validation Loss: 0.3087, Test Loss: 0.5632\n",
      "Epoch 121, Training Loss: 0.6145, Validation Loss: 0.4411, Test Loss: 0.7961\n",
      "Epoch 131, Training Loss: 0.3974, Validation Loss: 0.2555, Test Loss: 0.4911\n",
      "Epoch 141, Training Loss: 0.4943, Validation Loss: 0.3518, Test Loss: 0.6543\n",
      "Epoch 151, Training Loss: 0.3723, Validation Loss: 0.2378, Test Loss: 0.4510\n",
      "Epoch 161, Training Loss: 0.3696, Validation Loss: 0.2535, Test Loss: 0.4420\n",
      "Epoch 171, Training Loss: 0.3933, Validation Loss: 0.2878, Test Loss: 0.4831\n",
      "Epoch 181, Training Loss: 0.2808, Validation Loss: 0.1989, Test Loss: 0.3778\n",
      "Epoch 191, Training Loss: 0.2850, Validation Loss: 0.2243, Test Loss: 0.3738\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.2850\n",
      "Validation loss: 0.2243\n",
      "Test loss: 0.3738\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 198/243 with parameters:\n",
      "Config indices: (2, 1, 0, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 5447.2817, Validation Loss: 4912.2959, Test Loss: 5467.1602\n",
      "Epoch 11, Training Loss: 1.5654, Validation Loss: 1.0381, Test Loss: 1.8480\n",
      "Epoch 21, Training Loss: 0.7354, Validation Loss: 0.4081, Test Loss: 0.9745\n",
      "Epoch 31, Training Loss: 0.6781, Validation Loss: 0.3951, Test Loss: 0.9326\n",
      "Epoch 41, Training Loss: 0.6073, Validation Loss: 0.3681, Test Loss: 0.8256\n",
      "Epoch 51, Training Loss: 0.5866, Validation Loss: 0.3584, Test Loss: 0.7856\n",
      "Epoch 61, Training Loss: 0.5692, Validation Loss: 0.3829, Test Loss: 0.7202\n",
      "Epoch 71, Training Loss: 0.5483, Validation Loss: 0.3549, Test Loss: 0.7384\n",
      "Epoch 81, Training Loss: 0.4986, Validation Loss: 0.3245, Test Loss: 0.6771\n",
      "Epoch 91, Training Loss: 0.4692, Validation Loss: 0.3026, Test Loss: 0.6199\n",
      "Epoch 101, Training Loss: 0.4979, Validation Loss: 0.3556, Test Loss: 0.6184\n",
      "Epoch 111, Training Loss: 0.7025, Validation Loss: 0.5557, Test Loss: 0.9202\n",
      "Epoch 121, Training Loss: 0.3914, Validation Loss: 0.2580, Test Loss: 0.5168\n",
      "Epoch 131, Training Loss: 0.3924, Validation Loss: 0.2638, Test Loss: 0.4961\n",
      "Epoch 141, Training Loss: 0.3980, Validation Loss: 0.2779, Test Loss: 0.5586\n",
      "Epoch 151, Training Loss: 1.1279, Validation Loss: 0.9315, Test Loss: 1.3704\n",
      "Epoch 161, Training Loss: 0.3526, Validation Loss: 0.2508, Test Loss: 0.4920\n",
      "Epoch 171, Training Loss: 0.3232, Validation Loss: 0.2366, Test Loss: 0.4532\n",
      "Epoch 181, Training Loss: 0.3134, Validation Loss: 0.2365, Test Loss: 0.4253\n",
      "Epoch 191, Training Loss: 0.2589, Validation Loss: 0.1898, Test Loss: 0.3523\n",
      "Epoch 201, Training Loss: 0.2960, Validation Loss: 0.2308, Test Loss: 0.3844\n",
      "Epoch 211, Training Loss: 0.3068, Validation Loss: 0.2504, Test Loss: 0.3925\n",
      "Epoch 221, Training Loss: 0.2936, Validation Loss: 0.2344, Test Loss: 0.3979\n",
      "Epoch 231, Training Loss: 0.3537, Validation Loss: 0.2873, Test Loss: 0.4689\n",
      "Epoch 241, Training Loss: 0.2271, Validation Loss: 0.1659, Test Loss: 0.3039\n",
      "Epoch 251, Training Loss: 0.4820, Validation Loss: 0.3657, Test Loss: 0.4915\n",
      "Epoch 261, Training Loss: 0.7476, Validation Loss: 0.6121, Test Loss: 0.7435\n",
      "Epoch 271, Training Loss: 0.2702, Validation Loss: 0.2141, Test Loss: 0.3582\n",
      "Epoch 281, Training Loss: 2.6546, Validation Loss: 2.2581, Test Loss: 2.5581\n",
      "Epoch 291, Training Loss: 0.3953, Validation Loss: 0.3781, Test Loss: 0.4832\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.3953\n",
      "Validation loss: 0.3781\n",
      "Test loss: 0.4832\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 199/243 with parameters:\n",
      "Config indices: (2, 1, 1, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6867.9951, Validation Loss: 6279.8374, Test Loss: 6877.6348\n",
      "Epoch 11, Training Loss: 17.7382, Validation Loss: 15.8846, Test Loss: 16.3092\n",
      "Epoch 21, Training Loss: 1.3771, Validation Loss: 0.8823, Test Loss: 1.6366\n",
      "Epoch 31, Training Loss: 0.6635, Validation Loss: 0.3574, Test Loss: 0.8867\n",
      "Epoch 41, Training Loss: 0.5702, Validation Loss: 0.3497, Test Loss: 0.7352\n",
      "Epoch 51, Training Loss: 0.5372, Validation Loss: 0.3416, Test Loss: 0.6776\n",
      "Epoch 61, Training Loss: 0.5437, Validation Loss: 0.3769, Test Loss: 0.6598\n",
      "Epoch 71, Training Loss: 0.5332, Validation Loss: 0.3479, Test Loss: 0.6210\n",
      "Epoch 81, Training Loss: 0.4838, Validation Loss: 0.3200, Test Loss: 0.6379\n",
      "Epoch 91, Training Loss: 0.4185, Validation Loss: 0.2952, Test Loss: 0.5406\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 0.4185\n",
      "Validation loss: 0.2952\n",
      "Test loss: 0.5406\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 200/243 with parameters:\n",
      "Config indices: (2, 1, 1, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6953.6382, Validation Loss: 6349.5391, Test Loss: 6961.7612\n",
      "Epoch 11, Training Loss: 35.8959, Validation Loss: 36.2220, Test Loss: 33.3066\n",
      "Epoch 21, Training Loss: 1.8506, Validation Loss: 1.5786, Test Loss: 2.0942\n",
      "Epoch 31, Training Loss: 0.6631, Validation Loss: 0.4205, Test Loss: 0.9371\n",
      "Epoch 41, Training Loss: 0.4704, Validation Loss: 0.2764, Test Loss: 0.6851\n",
      "Epoch 51, Training Loss: 0.4675, Validation Loss: 0.3041, Test Loss: 0.6086\n",
      "Epoch 61, Training Loss: 0.4057, Validation Loss: 0.2640, Test Loss: 0.5326\n",
      "Epoch 71, Training Loss: 0.3957, Validation Loss: 0.2537, Test Loss: 0.4911\n",
      "Epoch 81, Training Loss: 0.3611, Validation Loss: 0.2459, Test Loss: 0.4559\n",
      "Epoch 91, Training Loss: 0.3754, Validation Loss: 0.2811, Test Loss: 0.5108\n",
      "Epoch 101, Training Loss: 0.3517, Validation Loss: 0.2551, Test Loss: 0.4261\n",
      "Epoch 111, Training Loss: 0.3139, Validation Loss: 0.2318, Test Loss: 0.4420\n",
      "Epoch 121, Training Loss: 0.3831, Validation Loss: 0.2924, Test Loss: 0.4461\n",
      "Epoch 131, Training Loss: 0.3116, Validation Loss: 0.2421, Test Loss: 0.3774\n",
      "Epoch 141, Training Loss: 0.4239, Validation Loss: 0.3389, Test Loss: 0.4740\n",
      "Epoch 151, Training Loss: 0.2982, Validation Loss: 0.2329, Test Loss: 0.4107\n",
      "Epoch 161, Training Loss: 0.2302, Validation Loss: 0.1649, Test Loss: 0.3066\n",
      "Epoch 171, Training Loss: 0.3166, Validation Loss: 0.2239, Test Loss: 0.3543\n",
      "Epoch 181, Training Loss: 0.2268, Validation Loss: 0.1620, Test Loss: 0.2942\n",
      "Epoch 191, Training Loss: 0.2595, Validation Loss: 0.2047, Test Loss: 0.3533\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.2595\n",
      "Validation loss: 0.2047\n",
      "Test loss: 0.3533\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 201/243 with parameters:\n",
      "Config indices: (2, 1, 1, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7001.9292, Validation Loss: 6395.7954, Test Loss: 7009.8086\n",
      "Epoch 11, Training Loss: 62.2494, Validation Loss: 62.9970, Test Loss: 59.9105\n",
      "Epoch 21, Training Loss: 2.3183, Validation Loss: 1.8047, Test Loss: 2.3863\n",
      "Epoch 31, Training Loss: 0.7024, Validation Loss: 0.3922, Test Loss: 0.8926\n",
      "Epoch 41, Training Loss: 0.6221, Validation Loss: 0.3824, Test Loss: 0.8263\n",
      "Epoch 51, Training Loss: 0.5544, Validation Loss: 0.3457, Test Loss: 0.7132\n",
      "Epoch 61, Training Loss: 0.5704, Validation Loss: 0.3803, Test Loss: 0.6727\n",
      "Epoch 71, Training Loss: 0.4902, Validation Loss: 0.3140, Test Loss: 0.6011\n",
      "Epoch 81, Training Loss: 0.4912, Validation Loss: 0.3084, Test Loss: 0.5718\n",
      "Epoch 91, Training Loss: 0.4262, Validation Loss: 0.2725, Test Loss: 0.5477\n",
      "Epoch 101, Training Loss: 0.3931, Validation Loss: 0.2519, Test Loss: 0.5085\n",
      "Epoch 111, Training Loss: 0.3812, Validation Loss: 0.2870, Test Loss: 0.5087\n",
      "Epoch 121, Training Loss: 0.4282, Validation Loss: 0.3156, Test Loss: 0.4893\n",
      "Epoch 131, Training Loss: 0.3032, Validation Loss: 0.2060, Test Loss: 0.4037\n",
      "Epoch 141, Training Loss: 0.2880, Validation Loss: 0.2064, Test Loss: 0.3851\n",
      "Epoch 151, Training Loss: 0.3714, Validation Loss: 0.2765, Test Loss: 0.4962\n",
      "Epoch 161, Training Loss: 0.2680, Validation Loss: 0.1991, Test Loss: 0.3474\n",
      "Epoch 171, Training Loss: 0.3950, Validation Loss: 0.2897, Test Loss: 0.4321\n",
      "Epoch 181, Training Loss: 0.2833, Validation Loss: 0.2342, Test Loss: 0.3674\n",
      "Epoch 191, Training Loss: 0.2674, Validation Loss: 0.2070, Test Loss: 0.3553\n",
      "Epoch 201, Training Loss: 0.2335, Validation Loss: 0.1733, Test Loss: 0.3131\n",
      "Epoch 211, Training Loss: 0.2474, Validation Loss: 0.1937, Test Loss: 0.3493\n",
      "Epoch 221, Training Loss: 0.2425, Validation Loss: 0.1974, Test Loss: 0.3271\n",
      "Epoch 231, Training Loss: 0.2136, Validation Loss: 0.1568, Test Loss: 0.2874\n",
      "Epoch 241, Training Loss: 0.2268, Validation Loss: 0.1629, Test Loss: 0.2859\n",
      "Epoch 251, Training Loss: 0.2146, Validation Loss: 0.1641, Test Loss: 0.2926\n",
      "Epoch 261, Training Loss: 0.2761, Validation Loss: 0.2381, Test Loss: 0.3395\n",
      "Epoch 271, Training Loss: 0.2379, Validation Loss: 0.1827, Test Loss: 0.3177\n",
      "Epoch 281, Training Loss: 0.2392, Validation Loss: 0.1844, Test Loss: 0.3045\n",
      "Epoch 291, Training Loss: 0.2660, Validation Loss: 0.2277, Test Loss: 0.3350\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.2660\n",
      "Validation loss: 0.2277\n",
      "Test loss: 0.3350\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 202/243 with parameters:\n",
      "Config indices: (2, 1, 1, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7119.8994, Validation Loss: 6512.3320, Test Loss: 7126.8901\n",
      "Epoch 11, Training Loss: 277.3413, Validation Loss: 278.4917, Test Loss: 284.3665\n",
      "Epoch 21, Training Loss: 22.4087, Validation Loss: 22.1099, Test Loss: 21.0384\n",
      "Epoch 31, Training Loss: 2.9495, Validation Loss: 2.4872, Test Loss: 3.0085\n",
      "Epoch 41, Training Loss: 0.8818, Validation Loss: 0.6127, Test Loss: 1.0623\n",
      "Epoch 51, Training Loss: 0.5425, Validation Loss: 0.3313, Test Loss: 0.7226\n",
      "Epoch 61, Training Loss: 0.4758, Validation Loss: 0.2950, Test Loss: 0.6317\n",
      "Epoch 71, Training Loss: 0.4408, Validation Loss: 0.2768, Test Loss: 0.5973\n",
      "Epoch 81, Training Loss: 0.4280, Validation Loss: 0.2718, Test Loss: 0.5823\n",
      "Epoch 91, Training Loss: 0.4236, Validation Loss: 0.2780, Test Loss: 0.5566\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 0.4236\n",
      "Validation loss: 0.2780\n",
      "Test loss: 0.5566\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 203/243 with parameters:\n",
      "Config indices: (2, 1, 1, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7132.2534, Validation Loss: 6523.2661, Test Loss: 7139.1729\n",
      "Epoch 11, Training Loss: 206.5599, Validation Loss: 203.2612, Test Loss: 218.9493\n",
      "Epoch 21, Training Loss: 29.0975, Validation Loss: 26.8623, Test Loss: 28.1119\n",
      "Epoch 31, Training Loss: 4.5376, Validation Loss: 3.4185, Test Loss: 4.7095\n",
      "Epoch 41, Training Loss: 1.1639, Validation Loss: 0.6866, Test Loss: 1.3157\n",
      "Epoch 51, Training Loss: 0.8149, Validation Loss: 0.5212, Test Loss: 0.9471\n",
      "Epoch 61, Training Loss: 0.7186, Validation Loss: 0.4434, Test Loss: 0.8709\n",
      "Epoch 71, Training Loss: 0.6903, Validation Loss: 0.4258, Test Loss: 0.8528\n",
      "Epoch 81, Training Loss: 0.6759, Validation Loss: 0.4283, Test Loss: 0.8351\n",
      "Epoch 91, Training Loss: 0.6159, Validation Loss: 0.3894, Test Loss: 0.7615\n",
      "Epoch 101, Training Loss: 0.5948, Validation Loss: 0.3742, Test Loss: 0.7418\n",
      "Epoch 111, Training Loss: 0.5746, Validation Loss: 0.3645, Test Loss: 0.7108\n",
      "Epoch 121, Training Loss: 0.6024, Validation Loss: 0.4210, Test Loss: 0.7462\n",
      "Epoch 131, Training Loss: 0.5365, Validation Loss: 0.3537, Test Loss: 0.6797\n",
      "Epoch 141, Training Loss: 0.5394, Validation Loss: 0.3726, Test Loss: 0.6435\n",
      "Epoch 151, Training Loss: 0.4736, Validation Loss: 0.3142, Test Loss: 0.6115\n",
      "Epoch 161, Training Loss: 0.4510, Validation Loss: 0.3078, Test Loss: 0.5688\n",
      "Epoch 171, Training Loss: 0.4315, Validation Loss: 0.2736, Test Loss: 0.5539\n",
      "Epoch 181, Training Loss: 0.4272, Validation Loss: 0.3062, Test Loss: 0.5251\n",
      "Epoch 191, Training Loss: 0.4067, Validation Loss: 0.2787, Test Loss: 0.5284\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.4067\n",
      "Validation loss: 0.2787\n",
      "Test loss: 0.5284\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 204/243 with parameters:\n",
      "Config indices: (2, 1, 1, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7129.9951, Validation Loss: 6519.5996, Test Loss: 7136.9448\n",
      "Epoch 11, Training Loss: 242.5312, Validation Loss: 243.5937, Test Loss: 252.1730\n",
      "Epoch 21, Training Loss: 60.1780, Validation Loss: 61.8489, Test Loss: 58.0549\n",
      "Epoch 31, Training Loss: 10.2964, Validation Loss: 9.6382, Test Loss: 9.9550\n",
      "Epoch 41, Training Loss: 1.4607, Validation Loss: 0.9984, Test Loss: 1.6682\n",
      "Epoch 51, Training Loss: 0.8042, Validation Loss: 0.4507, Test Loss: 1.0684\n",
      "Epoch 61, Training Loss: 0.6264, Validation Loss: 0.3452, Test Loss: 0.8504\n",
      "Epoch 71, Training Loss: 0.5594, Validation Loss: 0.3194, Test Loss: 0.7514\n",
      "Epoch 81, Training Loss: 0.5309, Validation Loss: 0.3152, Test Loss: 0.7039\n",
      "Epoch 91, Training Loss: 0.5123, Validation Loss: 0.3116, Test Loss: 0.6798\n",
      "Epoch 101, Training Loss: 0.4904, Validation Loss: 0.3026, Test Loss: 0.6335\n",
      "Epoch 111, Training Loss: 0.4747, Validation Loss: 0.3036, Test Loss: 0.6053\n",
      "Epoch 121, Training Loss: 0.4807, Validation Loss: 0.3133, Test Loss: 0.6288\n",
      "Epoch 131, Training Loss: 0.4577, Validation Loss: 0.2968, Test Loss: 0.5970\n",
      "Epoch 141, Training Loss: 0.4769, Validation Loss: 0.3365, Test Loss: 0.5647\n",
      "Epoch 151, Training Loss: 0.4179, Validation Loss: 0.2710, Test Loss: 0.5147\n",
      "Epoch 161, Training Loss: 0.3956, Validation Loss: 0.2643, Test Loss: 0.5054\n",
      "Epoch 171, Training Loss: 0.3861, Validation Loss: 0.2660, Test Loss: 0.4834\n",
      "Epoch 181, Training Loss: 0.3843, Validation Loss: 0.2803, Test Loss: 0.4797\n",
      "Epoch 191, Training Loss: 0.3654, Validation Loss: 0.2707, Test Loss: 0.4868\n",
      "Epoch 201, Training Loss: 0.3330, Validation Loss: 0.2220, Test Loss: 0.4449\n",
      "Epoch 211, Training Loss: 0.3200, Validation Loss: 0.2192, Test Loss: 0.4080\n",
      "Epoch 221, Training Loss: 0.3010, Validation Loss: 0.1989, Test Loss: 0.3881\n",
      "Epoch 231, Training Loss: 0.3379, Validation Loss: 0.2413, Test Loss: 0.4560\n",
      "Epoch 241, Training Loss: 0.3095, Validation Loss: 0.2132, Test Loss: 0.3794\n",
      "Epoch 251, Training Loss: 0.3058, Validation Loss: 0.2235, Test Loss: 0.3695\n",
      "Epoch 261, Training Loss: 0.2872, Validation Loss: 0.2073, Test Loss: 0.3537\n",
      "Epoch 271, Training Loss: 0.3306, Validation Loss: 0.2416, Test Loss: 0.3861\n",
      "Epoch 281, Training Loss: 0.2431, Validation Loss: 0.1737, Test Loss: 0.3333\n",
      "Epoch 291, Training Loss: 0.4132, Validation Loss: 0.3301, Test Loss: 0.5421\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.4132\n",
      "Validation loss: 0.3301\n",
      "Test loss: 0.5421\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 205/243 with parameters:\n",
      "Config indices: (2, 1, 1, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7156.3569, Validation Loss: 6545.9712, Test Loss: 7163.1875\n",
      "Epoch 11, Training Loss: 721.6182, Validation Loss: 691.1545, Test Loss: 771.5563\n",
      "Epoch 21, Training Loss: 285.7675, Validation Loss: 280.2855, Test Loss: 292.0529\n",
      "Epoch 31, Training Loss: 61.3182, Validation Loss: 60.7415, Test Loss: 58.4033\n",
      "Epoch 41, Training Loss: 15.9325, Validation Loss: 14.6707, Test Loss: 14.9962\n",
      "Epoch 51, Training Loss: 5.9537, Validation Loss: 5.1278, Test Loss: 5.9337\n",
      "Epoch 61, Training Loss: 2.8739, Validation Loss: 2.3405, Test Loss: 3.0877\n",
      "Epoch 71, Training Loss: 1.5405, Validation Loss: 1.0893, Test Loss: 1.9182\n",
      "Epoch 81, Training Loss: 0.9796, Validation Loss: 0.5809, Test Loss: 1.3908\n",
      "Epoch 91, Training Loss: 0.7308, Validation Loss: 0.3921, Test Loss: 1.1099\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 0.7308\n",
      "Validation loss: 0.3921\n",
      "Test loss: 1.1099\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 206/243 with parameters:\n",
      "Config indices: (2, 1, 1, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7153.8799, Validation Loss: 6543.4062, Test Loss: 7160.6772\n",
      "Epoch 11, Training Loss: 902.0783, Validation Loss: 801.4893, Test Loss: 905.0543\n",
      "Epoch 21, Training Loss: 219.2907, Validation Loss: 217.6475, Test Loss: 229.9236\n",
      "Epoch 31, Training Loss: 94.9959, Validation Loss: 95.0154, Test Loss: 94.2897\n",
      "Epoch 41, Training Loss: 34.1250, Validation Loss: 33.2703, Test Loss: 32.7875\n",
      "Epoch 51, Training Loss: 10.2586, Validation Loss: 9.4022, Test Loss: 10.1524\n",
      "Epoch 61, Training Loss: 3.8624, Validation Loss: 3.2727, Test Loss: 4.0297\n",
      "Epoch 71, Training Loss: 1.8764, Validation Loss: 1.4312, Test Loss: 2.1762\n",
      "Epoch 81, Training Loss: 1.1273, Validation Loss: 0.7086, Test Loss: 1.4008\n",
      "Epoch 91, Training Loss: 0.8456, Validation Loss: 0.4533, Test Loss: 1.0918\n",
      "Epoch 101, Training Loss: 0.7068, Validation Loss: 0.3717, Test Loss: 0.9482\n",
      "Epoch 111, Training Loss: 0.6257, Validation Loss: 0.3341, Test Loss: 0.8505\n",
      "Epoch 121, Training Loss: 0.5756, Validation Loss: 0.3110, Test Loss: 0.7946\n",
      "Epoch 131, Training Loss: 0.5418, Validation Loss: 0.3019, Test Loss: 0.7561\n",
      "Epoch 141, Training Loss: 0.5208, Validation Loss: 0.3060, Test Loss: 0.7042\n",
      "Epoch 151, Training Loss: 0.5016, Validation Loss: 0.2987, Test Loss: 0.7063\n",
      "Epoch 161, Training Loss: 0.4828, Validation Loss: 0.2928, Test Loss: 0.6602\n",
      "Epoch 171, Training Loss: 0.4732, Validation Loss: 0.2925, Test Loss: 0.6425\n",
      "Epoch 181, Training Loss: 0.4796, Validation Loss: 0.2978, Test Loss: 0.6683\n",
      "Epoch 191, Training Loss: 0.4534, Validation Loss: 0.2894, Test Loss: 0.6174\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.4534\n",
      "Validation loss: 0.2894\n",
      "Test loss: 0.6174\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 207/243 with parameters:\n",
      "Config indices: (2, 1, 1, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7158.1064, Validation Loss: 6547.4951, Test Loss: 7164.9141\n",
      "Epoch 11, Training Loss: 1003.1562, Validation Loss: 888.2732, Test Loss: 1022.6326\n",
      "Epoch 21, Training Loss: 256.3070, Validation Loss: 252.6976, Test Loss: 270.4619\n",
      "Epoch 31, Training Loss: 101.6115, Validation Loss: 101.8952, Test Loss: 99.7625\n",
      "Epoch 41, Training Loss: 25.3486, Validation Loss: 25.9997, Test Loss: 23.4846\n",
      "Epoch 51, Training Loss: 7.4315, Validation Loss: 7.0507, Test Loss: 7.0088\n",
      "Epoch 61, Training Loss: 2.7013, Validation Loss: 2.5481, Test Loss: 2.9099\n",
      "Epoch 71, Training Loss: 1.3911, Validation Loss: 1.1922, Test Loss: 1.7080\n",
      "Epoch 81, Training Loss: 0.9695, Validation Loss: 0.7121, Test Loss: 1.2550\n",
      "Epoch 91, Training Loss: 0.7984, Validation Loss: 0.5482, Test Loss: 1.0558\n",
      "Epoch 101, Training Loss: 0.7160, Validation Loss: 0.4898, Test Loss: 0.9236\n",
      "Epoch 111, Training Loss: 0.6602, Validation Loss: 0.4595, Test Loss: 0.8521\n",
      "Epoch 121, Training Loss: 0.6159, Validation Loss: 0.4298, Test Loss: 0.7932\n",
      "Epoch 131, Training Loss: 0.5843, Validation Loss: 0.4059, Test Loss: 0.7528\n",
      "Epoch 141, Training Loss: 0.5793, Validation Loss: 0.4191, Test Loss: 0.7230\n",
      "Epoch 151, Training Loss: 0.5756, Validation Loss: 0.4070, Test Loss: 0.6978\n",
      "Epoch 161, Training Loss: 0.5493, Validation Loss: 0.3840, Test Loss: 0.7126\n",
      "Epoch 171, Training Loss: 0.5076, Validation Loss: 0.3522, Test Loss: 0.6306\n",
      "Epoch 181, Training Loss: 0.4952, Validation Loss: 0.3410, Test Loss: 0.6389\n",
      "Epoch 191, Training Loss: 0.4726, Validation Loss: 0.3241, Test Loss: 0.5907\n",
      "Epoch 201, Training Loss: 0.4626, Validation Loss: 0.3192, Test Loss: 0.5950\n",
      "Epoch 211, Training Loss: 0.4519, Validation Loss: 0.3076, Test Loss: 0.5865\n",
      "Epoch 221, Training Loss: 0.4312, Validation Loss: 0.2896, Test Loss: 0.5454\n",
      "Epoch 231, Training Loss: 0.4352, Validation Loss: 0.2955, Test Loss: 0.5291\n",
      "Epoch 241, Training Loss: 0.4156, Validation Loss: 0.2797, Test Loss: 0.5145\n",
      "Epoch 251, Training Loss: 0.4208, Validation Loss: 0.2910, Test Loss: 0.5407\n",
      "Epoch 261, Training Loss: 0.3789, Validation Loss: 0.2494, Test Loss: 0.4853\n",
      "Epoch 271, Training Loss: 0.4059, Validation Loss: 0.2781, Test Loss: 0.5428\n",
      "Epoch 281, Training Loss: 0.3598, Validation Loss: 0.2411, Test Loss: 0.4642\n",
      "Epoch 291, Training Loss: 0.3649, Validation Loss: 0.2468, Test Loss: 0.4899\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 0.3649\n",
      "Validation loss: 0.2468\n",
      "Test loss: 0.4899\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 208/243 with parameters:\n",
      "Config indices: (2, 1, 2, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7162.4385, Validation Loss: 6551.7305, Test Loss: 7169.2563\n",
      "Epoch 11, Training Loss: 6508.8994, Validation Loss: 5929.8818, Test Loss: 6521.6777\n",
      "Epoch 21, Training Loss: 2875.8018, Validation Loss: 2539.8716, Test Loss: 2912.5049\n",
      "Epoch 31, Training Loss: 723.3772, Validation Loss: 665.9897, Test Loss: 752.3954\n",
      "Epoch 41, Training Loss: 511.6304, Validation Loss: 494.5040, Test Loss: 537.4061\n",
      "Epoch 51, Training Loss: 377.1697, Validation Loss: 369.8995, Test Loss: 391.6708\n",
      "Epoch 61, Training Loss: 241.1954, Validation Loss: 240.0663, Test Loss: 243.2760\n",
      "Epoch 71, Training Loss: 144.9341, Validation Loss: 145.7929, Test Loss: 139.2197\n",
      "Epoch 81, Training Loss: 80.3284, Validation Loss: 80.6465, Test Loss: 74.1753\n",
      "Epoch 91, Training Loss: 35.8912, Validation Loss: 34.0315, Test Loss: 32.4689\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 35.8912\n",
      "Validation loss: 34.0315\n",
      "Test loss: 32.4689\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 209/243 with parameters:\n",
      "Config indices: (2, 1, 2, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7162.4741, Validation Loss: 6551.8120, Test Loss: 7169.2925\n",
      "Epoch 11, Training Loss: 6458.4585, Validation Loss: 5912.0361, Test Loss: 6473.8179\n",
      "Epoch 21, Training Loss: 2806.2290, Validation Loss: 2623.6177, Test Loss: 2875.3555\n",
      "Epoch 31, Training Loss: 921.9986, Validation Loss: 924.3945, Test Loss: 1011.2758\n",
      "Epoch 41, Training Loss: 652.6199, Validation Loss: 643.9753, Test Loss: 713.9949\n",
      "Epoch 51, Training Loss: 463.8985, Validation Loss: 451.1014, Test Loss: 492.5100\n",
      "Epoch 61, Training Loss: 296.5419, Validation Loss: 287.4839, Test Loss: 301.8437\n",
      "Epoch 71, Training Loss: 153.8241, Validation Loss: 146.6895, Test Loss: 151.0778\n",
      "Epoch 81, Training Loss: 75.8215, Validation Loss: 70.0887, Test Loss: 72.1917\n",
      "Epoch 91, Training Loss: 37.6789, Validation Loss: 33.8080, Test Loss: 34.8881\n",
      "Epoch 101, Training Loss: 18.6983, Validation Loss: 16.1416, Test Loss: 17.1845\n",
      "Epoch 111, Training Loss: 9.0848, Validation Loss: 7.2359, Test Loss: 8.5871\n",
      "Epoch 121, Training Loss: 4.5376, Validation Loss: 3.2460, Test Loss: 4.6338\n",
      "Epoch 131, Training Loss: 2.3826, Validation Loss: 1.4138, Test Loss: 2.6971\n",
      "Epoch 141, Training Loss: 1.4042, Validation Loss: 0.6619, Test Loss: 1.7500\n",
      "Epoch 151, Training Loss: 0.9755, Validation Loss: 0.4229, Test Loss: 1.3066\n",
      "Epoch 161, Training Loss: 0.8006, Validation Loss: 0.3723, Test Loss: 1.1127\n",
      "Epoch 171, Training Loss: 0.6884, Validation Loss: 0.3408, Test Loss: 0.9325\n",
      "Epoch 181, Training Loss: 0.6371, Validation Loss: 0.3393, Test Loss: 0.8428\n",
      "Epoch 191, Training Loss: 0.5908, Validation Loss: 0.3309, Test Loss: 0.7685\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.5908\n",
      "Validation loss: 0.3309\n",
      "Test loss: 0.7685\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 210/243 with parameters:\n",
      "Config indices: (2, 1, 2, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7161.4312, Validation Loss: 6550.7876, Test Loss: 7168.2563\n",
      "Epoch 11, Training Loss: 6173.2637, Validation Loss: 5604.7632, Test Loss: 6188.5884\n",
      "Epoch 21, Training Loss: 1980.3391, Validation Loss: 1720.6497, Test Loss: 2004.7024\n",
      "Epoch 31, Training Loss: 543.6343, Validation Loss: 511.4887, Test Loss: 569.7464\n",
      "Epoch 41, Training Loss: 364.4754, Validation Loss: 355.9695, Test Loss: 384.8293\n",
      "Epoch 51, Training Loss: 231.1352, Validation Loss: 229.6949, Test Loss: 239.6479\n",
      "Epoch 61, Training Loss: 152.4440, Validation Loss: 152.8932, Test Loss: 151.6360\n",
      "Epoch 71, Training Loss: 88.6931, Validation Loss: 89.3276, Test Loss: 85.1925\n",
      "Epoch 81, Training Loss: 40.6236, Validation Loss: 39.2730, Test Loss: 37.9013\n",
      "Epoch 91, Training Loss: 17.0715, Validation Loss: 15.5956, Test Loss: 16.1981\n",
      "Epoch 101, Training Loss: 6.5380, Validation Loss: 5.5981, Test Loss: 6.5757\n",
      "Epoch 111, Training Loss: 2.5708, Validation Loss: 1.8459, Test Loss: 2.8717\n",
      "Epoch 121, Training Loss: 1.3534, Validation Loss: 0.8086, Test Loss: 1.6949\n",
      "Epoch 131, Training Loss: 0.9543, Validation Loss: 0.5542, Test Loss: 1.3250\n",
      "Epoch 141, Training Loss: 0.7480, Validation Loss: 0.4116, Test Loss: 1.0880\n",
      "Epoch 151, Training Loss: 0.6361, Validation Loss: 0.3580, Test Loss: 0.9334\n",
      "Epoch 161, Training Loss: 0.5888, Validation Loss: 0.3461, Test Loss: 0.8492\n",
      "Epoch 171, Training Loss: 0.5543, Validation Loss: 0.3293, Test Loss: 0.8136\n",
      "Epoch 181, Training Loss: 0.5692, Validation Loss: 0.3639, Test Loss: 0.7699\n",
      "Epoch 191, Training Loss: 0.5267, Validation Loss: 0.3291, Test Loss: 0.7293\n",
      "Epoch 201, Training Loss: 0.5118, Validation Loss: 0.3236, Test Loss: 0.7095\n",
      "Epoch 211, Training Loss: 0.5008, Validation Loss: 0.3177, Test Loss: 0.7027\n",
      "Epoch 221, Training Loss: 0.5106, Validation Loss: 0.3355, Test Loss: 0.6749\n",
      "Epoch 231, Training Loss: 0.4866, Validation Loss: 0.3056, Test Loss: 0.6800\n",
      "Epoch 241, Training Loss: 0.4720, Validation Loss: 0.2982, Test Loss: 0.6464\n",
      "Epoch 251, Training Loss: 0.4645, Validation Loss: 0.2922, Test Loss: 0.6364\n",
      "Epoch 261, Training Loss: 0.4602, Validation Loss: 0.2967, Test Loss: 0.6239\n",
      "Epoch 271, Training Loss: 0.4576, Validation Loss: 0.3001, Test Loss: 0.6103\n",
      "Epoch 281, Training Loss: 0.4485, Validation Loss: 0.2882, Test Loss: 0.5974\n",
      "Epoch 291, Training Loss: 0.4377, Validation Loss: 0.2861, Test Loss: 0.5936\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.4377\n",
      "Validation loss: 0.2861\n",
      "Test loss: 0.5936\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 211/243 with parameters:\n",
      "Config indices: (2, 1, 2, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7165.4316, Validation Loss: 6554.4956, Test Loss: 7172.2671\n",
      "Epoch 11, Training Loss: 7097.8115, Validation Loss: 6487.0552, Test Loss: 7104.9878\n",
      "Epoch 21, Training Loss: 6606.1899, Validation Loss: 5988.9131, Test Loss: 6616.6489\n",
      "Epoch 31, Training Loss: 5371.7520, Validation Loss: 4757.9844, Test Loss: 5379.0430\n",
      "Epoch 41, Training Loss: 3964.0991, Validation Loss: 3451.5103, Test Loss: 3940.8506\n",
      "Epoch 51, Training Loss: 2574.7671, Validation Loss: 2245.4775, Test Loss: 2541.8647\n",
      "Epoch 61, Training Loss: 1297.7709, Validation Loss: 1147.7552, Test Loss: 1278.0817\n",
      "Epoch 71, Training Loss: 517.2119, Validation Loss: 482.5052, Test Loss: 516.0982\n",
      "Epoch 81, Training Loss: 289.9222, Validation Loss: 288.0058, Test Loss: 296.6383\n",
      "Epoch 91, Training Loss: 236.8604, Validation Loss: 238.8978, Test Loss: 245.1554\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 236.8604\n",
      "Validation loss: 238.8978\n",
      "Test loss: 245.1554\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 212/243 with parameters:\n",
      "Config indices: (2, 1, 2, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7164.6836, Validation Loss: 6553.8799, Test Loss: 7171.5063\n",
      "Epoch 11, Training Loss: 7074.6865, Validation Loss: 6469.6055, Test Loss: 7081.8823\n",
      "Epoch 21, Training Loss: 6548.4263, Validation Loss: 5976.6851, Test Loss: 6560.4385\n",
      "Epoch 31, Training Loss: 5070.1851, Validation Loss: 4593.0415, Test Loss: 5096.2529\n",
      "Epoch 41, Training Loss: 2896.9097, Validation Loss: 2583.2458, Test Loss: 2933.9941\n",
      "Epoch 51, Training Loss: 1281.3646, Validation Loss: 1140.3309, Test Loss: 1310.5302\n",
      "Epoch 61, Training Loss: 702.8593, Validation Loss: 656.3781, Test Loss: 726.7705\n",
      "Epoch 71, Training Loss: 566.3025, Validation Loss: 547.3509, Test Loss: 589.3773\n",
      "Epoch 81, Training Loss: 489.0729, Validation Loss: 478.5845, Test Loss: 509.6227\n",
      "Epoch 91, Training Loss: 410.3317, Validation Loss: 405.2080, Test Loss: 425.9082\n",
      "Epoch 101, Training Loss: 328.7577, Validation Loss: 327.5016, Test Loss: 339.3083\n",
      "Epoch 111, Training Loss: 257.4063, Validation Loss: 257.9395, Test Loss: 262.0128\n",
      "Epoch 121, Training Loss: 202.0154, Validation Loss: 202.5608, Test Loss: 201.4949\n",
      "Epoch 131, Training Loss: 161.8211, Validation Loss: 162.2607, Test Loss: 157.6505\n",
      "Epoch 141, Training Loss: 126.7953, Validation Loss: 127.4994, Test Loss: 121.2153\n",
      "Epoch 151, Training Loss: 94.7822, Validation Loss: 94.9747, Test Loss: 89.1832\n",
      "Epoch 161, Training Loss: 66.8494, Validation Loss: 66.0331, Test Loss: 61.9852\n",
      "Epoch 171, Training Loss: 44.0508, Validation Loss: 42.4698, Test Loss: 40.3963\n",
      "Epoch 181, Training Loss: 27.2455, Validation Loss: 25.1600, Test Loss: 25.1510\n",
      "Epoch 191, Training Loss: 16.5516, Validation Loss: 14.9085, Test Loss: 15.7423\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 16.5516\n",
      "Validation loss: 14.9085\n",
      "Test loss: 15.7423\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 213/243 with parameters:\n",
      "Config indices: (2, 1, 2, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7163.6826, Validation Loss: 6552.7026, Test Loss: 7170.5264\n",
      "Epoch 11, Training Loss: 7092.8428, Validation Loss: 6482.4077, Test Loss: 7100.1567\n",
      "Epoch 21, Training Loss: 6661.1240, Validation Loss: 6052.9165, Test Loss: 6672.8115\n",
      "Epoch 31, Training Loss: 5501.3188, Validation Loss: 4913.1802, Test Loss: 5521.0122\n",
      "Epoch 41, Training Loss: 3872.0159, Validation Loss: 3373.9983, Test Loss: 3886.0352\n",
      "Epoch 51, Training Loss: 2386.9392, Validation Loss: 2062.8115, Test Loss: 2390.9868\n",
      "Epoch 61, Training Loss: 1230.9670, Validation Loss: 1073.0448, Test Loss: 1243.6760\n",
      "Epoch 71, Training Loss: 641.7228, Validation Loss: 581.8507, Test Loss: 665.9184\n",
      "Epoch 81, Training Loss: 440.9034, Validation Loss: 419.4870, Test Loss: 470.1720\n",
      "Epoch 91, Training Loss: 361.1547, Validation Loss: 351.2066, Test Loss: 389.3131\n",
      "Epoch 101, Training Loss: 309.6705, Validation Loss: 304.3512, Test Loss: 333.1246\n",
      "Epoch 111, Training Loss: 262.8294, Validation Loss: 260.5684, Test Loss: 281.2641\n",
      "Epoch 121, Training Loss: 221.8175, Validation Loss: 221.9306, Test Loss: 235.5058\n",
      "Epoch 131, Training Loss: 188.4657, Validation Loss: 189.7018, Test Loss: 197.7512\n",
      "Epoch 141, Training Loss: 158.2211, Validation Loss: 160.2737, Test Loss: 163.9541\n",
      "Epoch 151, Training Loss: 129.8146, Validation Loss: 132.2653, Test Loss: 132.7190\n",
      "Epoch 161, Training Loss: 104.0322, Validation Loss: 106.2668, Test Loss: 104.6311\n",
      "Epoch 171, Training Loss: 81.1636, Validation Loss: 82.7896, Test Loss: 80.4294\n",
      "Epoch 181, Training Loss: 61.3380, Validation Loss: 62.3134, Test Loss: 59.6094\n",
      "Epoch 191, Training Loss: 44.9743, Validation Loss: 45.3231, Test Loss: 43.1671\n",
      "Epoch 201, Training Loss: 31.8458, Validation Loss: 31.7260, Test Loss: 30.4533\n",
      "Epoch 211, Training Loss: 21.3852, Validation Loss: 21.0149, Test Loss: 20.5505\n",
      "Epoch 221, Training Loss: 13.3141, Validation Loss: 12.6625, Test Loss: 12.8928\n",
      "Epoch 231, Training Loss: 7.6877, Validation Loss: 6.9586, Test Loss: 7.6573\n",
      "Epoch 241, Training Loss: 4.3914, Validation Loss: 3.6580, Test Loss: 4.6154\n",
      "Epoch 251, Training Loss: 2.7534, Validation Loss: 2.1083, Test Loss: 3.0782\n",
      "Epoch 261, Training Loss: 1.9967, Validation Loss: 1.4506, Test Loss: 2.3442\n",
      "Epoch 271, Training Loss: 1.5358, Validation Loss: 1.0728, Test Loss: 1.8656\n",
      "Epoch 281, Training Loss: 1.2193, Validation Loss: 0.8199, Test Loss: 1.5162\n",
      "Epoch 291, Training Loss: 0.9954, Validation Loss: 0.6594, Test Loss: 1.2798\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.9954\n",
      "Validation loss: 0.6594\n",
      "Test loss: 1.2798\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 214/243 with parameters:\n",
      "Config indices: (2, 1, 2, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7163.7563, Validation Loss: 6553.1489, Test Loss: 7170.5962\n",
      "Epoch 11, Training Loss: 7133.9790, Validation Loss: 6525.7085, Test Loss: 7140.9043\n",
      "Epoch 21, Training Loss: 7011.5684, Validation Loss: 6413.7295, Test Loss: 7019.5063\n",
      "Epoch 31, Training Loss: 6657.2002, Validation Loss: 6090.1372, Test Loss: 6668.6729\n",
      "Epoch 41, Training Loss: 5915.4937, Validation Loss: 5412.5293, Test Loss: 5935.0610\n",
      "Epoch 51, Training Loss: 4707.4634, Validation Loss: 4309.6216, Test Loss: 4740.6885\n",
      "Epoch 61, Training Loss: 3203.4878, Validation Loss: 2938.4131, Test Loss: 3252.4150\n",
      "Epoch 71, Training Loss: 1846.6611, Validation Loss: 1705.3876, Test Loss: 1905.6638\n",
      "Epoch 81, Training Loss: 1039.4296, Validation Loss: 977.6158, Test Loss: 1097.3110\n",
      "Epoch 91, Training Loss: 747.7324, Validation Loss: 719.1016, Test Loss: 796.7368\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 747.7324\n",
      "Validation loss: 719.1016\n",
      "Test loss: 796.7368\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 215/243 with parameters:\n",
      "Config indices: (2, 1, 2, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7165.7891, Validation Loss: 6554.8530, Test Loss: 7172.6089\n",
      "Epoch 11, Training Loss: 7142.4458, Validation Loss: 6532.7578, Test Loss: 7149.3066\n",
      "Epoch 21, Training Loss: 7049.1865, Validation Loss: 6444.4907, Test Loss: 7056.6099\n",
      "Epoch 31, Training Loss: 6764.9146, Validation Loss: 6176.8940, Test Loss: 6774.9858\n",
      "Epoch 41, Training Loss: 6159.4570, Validation Loss: 5609.4434, Test Loss: 6176.0962\n",
      "Epoch 51, Training Loss: 5165.0083, Validation Loss: 4682.0483, Test Loss: 5192.4092\n",
      "Epoch 61, Training Loss: 3860.5820, Validation Loss: 3473.9043, Test Loss: 3900.2664\n",
      "Epoch 71, Training Loss: 2528.6262, Validation Loss: 2253.3333, Test Loss: 2576.5474\n",
      "Epoch 81, Training Loss: 1490.2570, Validation Loss: 1320.8479, Test Loss: 1538.9254\n",
      "Epoch 91, Training Loss: 918.0953, Validation Loss: 825.0768, Test Loss: 962.5357\n",
      "Epoch 101, Training Loss: 691.2344, Validation Loss: 639.7941, Test Loss: 732.3002\n",
      "Epoch 111, Training Loss: 607.7565, Validation Loss: 575.2870, Test Loss: 647.3088\n",
      "Epoch 121, Training Loss: 561.8491, Validation Loss: 538.1196, Test Loss: 598.7662\n",
      "Epoch 131, Training Loss: 521.7949, Validation Loss: 503.2194, Test Loss: 555.9312\n",
      "Epoch 141, Training Loss: 481.2145, Validation Loss: 466.6649, Test Loss: 511.2634\n",
      "Epoch 151, Training Loss: 439.8342, Validation Loss: 428.8622, Test Loss: 465.8269\n",
      "Epoch 161, Training Loss: 396.8121, Validation Loss: 389.0851, Test Loss: 418.0570\n",
      "Epoch 171, Training Loss: 352.1208, Validation Loss: 347.7930, Test Loss: 369.0867\n",
      "Epoch 181, Training Loss: 307.3092, Validation Loss: 305.9987, Test Loss: 319.9359\n",
      "Epoch 191, Training Loss: 265.0042, Validation Loss: 265.4515, Test Loss: 273.2847\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 265.0042\n",
      "Validation loss: 265.4515\n",
      "Test loss: 273.2847\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 216/243 with parameters:\n",
      "Config indices: (2, 1, 2, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adam.AdamOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.2920, Validation Loss: 6555.3799, Test Loss: 7173.1128\n",
      "Epoch 11, Training Loss: 7152.0537, Validation Loss: 6541.7183, Test Loss: 7158.8540\n",
      "Epoch 21, Training Loss: 7099.7822, Validation Loss: 6490.9355, Test Loss: 7106.7642\n",
      "Epoch 31, Training Loss: 6937.2671, Validation Loss: 6331.5195, Test Loss: 6945.2646\n",
      "Epoch 41, Training Loss: 6558.7817, Validation Loss: 5962.1040, Test Loss: 6570.1011\n",
      "Epoch 51, Training Loss: 5889.8745, Validation Loss: 5313.5405, Test Loss: 5907.1733\n",
      "Epoch 61, Training Loss: 4916.1528, Validation Loss: 4373.2437, Test Loss: 4939.6333\n",
      "Epoch 71, Training Loss: 3799.3125, Validation Loss: 3318.0481, Test Loss: 3823.4211\n",
      "Epoch 81, Training Loss: 2767.2808, Validation Loss: 2381.8196, Test Loss: 2785.7249\n",
      "Epoch 91, Training Loss: 1902.8428, Validation Loss: 1628.7319, Test Loss: 1919.8820\n",
      "Epoch 101, Training Loss: 1231.6498, Validation Loss: 1056.3387, Test Loss: 1254.0210\n",
      "Epoch 111, Training Loss: 759.6455, Validation Loss: 661.6162, Test Loss: 790.0492\n",
      "Epoch 121, Training Loss: 529.8157, Validation Loss: 475.4573, Test Loss: 565.1827\n",
      "Epoch 131, Training Loss: 442.6459, Validation Loss: 407.5784, Test Loss: 479.5249\n",
      "Epoch 141, Training Loss: 395.7777, Validation Loss: 369.4763, Test Loss: 431.5428\n",
      "Epoch 151, Training Loss: 355.9642, Validation Loss: 334.6817, Test Loss: 389.0282\n",
      "Epoch 161, Training Loss: 317.7233, Validation Loss: 300.5164, Test Loss: 347.1445\n",
      "Epoch 171, Training Loss: 281.5853, Validation Loss: 267.7202, Test Loss: 307.8067\n",
      "Epoch 181, Training Loss: 248.1383, Validation Loss: 237.2619, Test Loss: 271.0192\n",
      "Epoch 191, Training Loss: 217.8279, Validation Loss: 209.1505, Test Loss: 237.5020\n",
      "Epoch 201, Training Loss: 191.7368, Validation Loss: 184.7301, Test Loss: 208.0102\n",
      "Epoch 211, Training Loss: 169.4813, Validation Loss: 163.4822, Test Loss: 183.2592\n",
      "Epoch 221, Training Loss: 150.8356, Validation Loss: 145.5848, Test Loss: 162.0009\n",
      "Epoch 231, Training Loss: 134.6126, Validation Loss: 130.0918, Test Loss: 143.9412\n",
      "Epoch 241, Training Loss: 120.2896, Validation Loss: 116.2456, Test Loss: 127.7863\n",
      "Epoch 251, Training Loss: 107.3375, Validation Loss: 103.7638, Test Loss: 113.4323\n",
      "Epoch 261, Training Loss: 95.2682, Validation Loss: 92.1067, Test Loss: 100.2211\n",
      "Epoch 271, Training Loss: 83.9200, Validation Loss: 81.1756, Test Loss: 87.9907\n",
      "Epoch 281, Training Loss: 73.5228, Validation Loss: 71.0885, Test Loss: 76.6906\n",
      "Epoch 291, Training Loss: 63.8905, Validation Loss: 61.8621, Test Loss: 66.1737\n",
      "Optimizer: AdamOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 63.8905\n",
      "Validation loss: 61.8621\n",
      "Test loss: 66.1737\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 217/243 with parameters:\n",
      "Config indices: (2, 2, 0, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 3582.8923, Validation Loss: 3185.2861, Test Loss: 3622.5532\n",
      "Epoch 11, Training Loss: 161.1684, Validation Loss: 163.1207, Test Loss: 162.8166\n",
      "Epoch 21, Training Loss: 45.0550, Validation Loss: 44.3246, Test Loss: 42.3630\n",
      "Epoch 31, Training Loss: 12.3681, Validation Loss: 10.8242, Test Loss: 11.8842\n",
      "Epoch 41, Training Loss: 6.0402, Validation Loss: 4.9320, Test Loss: 6.1430\n",
      "Epoch 51, Training Loss: 2.9386, Validation Loss: 2.1068, Test Loss: 3.3586\n",
      "Epoch 61, Training Loss: 1.7852, Validation Loss: 1.1886, Test Loss: 2.2603\n",
      "Epoch 71, Training Loss: 1.3956, Validation Loss: 0.9681, Test Loss: 1.8527\n",
      "Epoch 81, Training Loss: 1.2225, Validation Loss: 0.8572, Test Loss: 1.6596\n",
      "Epoch 91, Training Loss: 1.1210, Validation Loss: 0.8117, Test Loss: 1.5383\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 1.1210\n",
      "Validation loss: 0.8117\n",
      "Test loss: 1.5383\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 218/243 with parameters:\n",
      "Config indices: (2, 2, 0, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 2302.2290, Validation Loss: 2068.3193, Test Loss: 2348.7395\n",
      "Epoch 11, Training Loss: 80.4566, Validation Loss: 79.6959, Test Loss: 74.8817\n",
      "Epoch 21, Training Loss: 15.1525, Validation Loss: 13.6858, Test Loss: 14.6609\n",
      "Epoch 31, Training Loss: 6.8391, Validation Loss: 5.8896, Test Loss: 7.0166\n",
      "Epoch 41, Training Loss: 4.0735, Validation Loss: 3.2845, Test Loss: 4.3353\n",
      "Epoch 51, Training Loss: 2.7117, Validation Loss: 2.1705, Test Loss: 3.0335\n",
      "Epoch 61, Training Loss: 1.9930, Validation Loss: 1.5359, Test Loss: 2.3049\n",
      "Epoch 71, Training Loss: 1.6098, Validation Loss: 1.2090, Test Loss: 1.8802\n",
      "Epoch 81, Training Loss: 1.3725, Validation Loss: 1.0063, Test Loss: 1.6147\n",
      "Epoch 91, Training Loss: 1.2071, Validation Loss: 0.8622, Test Loss: 1.4289\n",
      "Epoch 101, Training Loss: 1.0819, Validation Loss: 0.7502, Test Loss: 1.2864\n",
      "Epoch 111, Training Loss: 0.9829, Validation Loss: 0.6694, Test Loss: 1.1830\n",
      "Epoch 121, Training Loss: 0.9055, Validation Loss: 0.6069, Test Loss: 1.1062\n",
      "Epoch 131, Training Loss: 0.8443, Validation Loss: 0.5600, Test Loss: 1.0394\n",
      "Epoch 141, Training Loss: 0.7982, Validation Loss: 0.5205, Test Loss: 0.9793\n",
      "Epoch 151, Training Loss: 0.7606, Validation Loss: 0.4878, Test Loss: 0.9317\n",
      "Epoch 161, Training Loss: 0.7271, Validation Loss: 0.4564, Test Loss: 0.8863\n",
      "Epoch 171, Training Loss: 0.6978, Validation Loss: 0.4314, Test Loss: 0.8528\n",
      "Epoch 181, Training Loss: 0.6729, Validation Loss: 0.4130, Test Loss: 0.8189\n",
      "Epoch 191, Training Loss: 0.6510, Validation Loss: 0.3983, Test Loss: 0.7911\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 0.6510\n",
      "Validation loss: 0.3983\n",
      "Test loss: 0.7911\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 219/243 with parameters:\n",
      "Config indices: (2, 2, 0, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 4432.0845, Validation Loss: 3970.6418, Test Loss: 4462.6230\n",
      "Epoch 11, Training Loss: 162.8785, Validation Loss: 160.3284, Test Loss: 166.5373\n",
      "Epoch 21, Training Loss: 59.7637, Validation Loss: 59.5307, Test Loss: 56.8836\n",
      "Epoch 31, Training Loss: 23.9159, Validation Loss: 23.0408, Test Loss: 21.9074\n",
      "Epoch 41, Training Loss: 9.5179, Validation Loss: 8.1552, Test Loss: 8.8211\n",
      "Epoch 51, Training Loss: 4.3372, Validation Loss: 3.3467, Test Loss: 4.4062\n",
      "Epoch 61, Training Loss: 2.5880, Validation Loss: 1.7868, Test Loss: 2.7708\n",
      "Epoch 71, Training Loss: 1.8558, Validation Loss: 1.2577, Test Loss: 2.0815\n",
      "Epoch 81, Training Loss: 1.4894, Validation Loss: 0.9884, Test Loss: 1.7095\n",
      "Epoch 91, Training Loss: 1.2690, Validation Loss: 0.8301, Test Loss: 1.4731\n",
      "Epoch 101, Training Loss: 1.0927, Validation Loss: 0.7099, Test Loss: 1.2978\n",
      "Epoch 111, Training Loss: 0.9572, Validation Loss: 0.6161, Test Loss: 1.1494\n",
      "Epoch 121, Training Loss: 0.8618, Validation Loss: 0.5583, Test Loss: 1.0412\n",
      "Epoch 131, Training Loss: 0.7920, Validation Loss: 0.5205, Test Loss: 0.9620\n",
      "Epoch 141, Training Loss: 0.7394, Validation Loss: 0.4841, Test Loss: 0.8983\n",
      "Epoch 151, Training Loss: 0.7006, Validation Loss: 0.4638, Test Loss: 0.8540\n",
      "Epoch 161, Training Loss: 0.6676, Validation Loss: 0.4387, Test Loss: 0.8085\n",
      "Epoch 171, Training Loss: 0.6414, Validation Loss: 0.4141, Test Loss: 0.7764\n",
      "Epoch 181, Training Loss: 0.6194, Validation Loss: 0.3947, Test Loss: 0.7545\n",
      "Epoch 191, Training Loss: 0.5993, Validation Loss: 0.3772, Test Loss: 0.7341\n",
      "Epoch 201, Training Loss: 0.5801, Validation Loss: 0.3623, Test Loss: 0.7172\n",
      "Epoch 211, Training Loss: 0.5643, Validation Loss: 0.3505, Test Loss: 0.6996\n",
      "Epoch 221, Training Loss: 0.5507, Validation Loss: 0.3382, Test Loss: 0.6780\n",
      "Epoch 231, Training Loss: 0.5388, Validation Loss: 0.3297, Test Loss: 0.6658\n",
      "Epoch 241, Training Loss: 0.5284, Validation Loss: 0.3232, Test Loss: 0.6577\n",
      "Epoch 251, Training Loss: 0.5196, Validation Loss: 0.3170, Test Loss: 0.6472\n",
      "Epoch 261, Training Loss: 0.5116, Validation Loss: 0.3113, Test Loss: 0.6344\n",
      "Epoch 271, Training Loss: 0.5051, Validation Loss: 0.3075, Test Loss: 0.6247\n",
      "Epoch 281, Training Loss: 0.4996, Validation Loss: 0.3055, Test Loss: 0.6216\n",
      "Epoch 291, Training Loss: 0.4944, Validation Loss: 0.3027, Test Loss: 0.6121\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 0.4944\n",
      "Validation loss: 0.3027\n",
      "Test loss: 0.6121\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 220/243 with parameters:\n",
      "Config indices: (2, 2, 0, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 5906.1401, Validation Loss: 5414.0991, Test Loss: 5925.2524\n",
      "Epoch 11, Training Loss: 375.5607, Validation Loss: 372.3749, Test Loss: 379.3327\n",
      "Epoch 21, Training Loss: 125.2525, Validation Loss: 125.5924, Test Loss: 118.3716\n",
      "Epoch 31, Training Loss: 41.4992, Validation Loss: 40.6581, Test Loss: 38.0682\n",
      "Epoch 41, Training Loss: 20.4440, Validation Loss: 19.3976, Test Loss: 18.9868\n",
      "Epoch 51, Training Loss: 11.7658, Validation Loss: 10.9163, Test Loss: 11.3579\n",
      "Epoch 61, Training Loss: 7.4125, Validation Loss: 6.5166, Test Loss: 7.3256\n",
      "Epoch 71, Training Loss: 4.0282, Validation Loss: 3.2544, Test Loss: 4.1489\n",
      "Epoch 81, Training Loss: 2.3477, Validation Loss: 1.7407, Test Loss: 2.5835\n",
      "Epoch 91, Training Loss: 1.5544, Validation Loss: 1.0780, Test Loss: 1.8451\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 1.5544\n",
      "Validation loss: 1.0780\n",
      "Test loss: 1.8451\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 221/243 with parameters:\n",
      "Config indices: (2, 2, 0, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6169.8882, Validation Loss: 5592.4141, Test Loss: 6184.9526\n",
      "Epoch 11, Training Loss: 306.3061, Validation Loss: 309.8891, Test Loss: 321.6051\n",
      "Epoch 21, Training Loss: 166.0383, Validation Loss: 171.3638, Test Loss: 165.6621\n",
      "Epoch 31, Training Loss: 100.9525, Validation Loss: 104.1846, Test Loss: 95.9302\n",
      "Epoch 41, Training Loss: 59.8968, Validation Loss: 62.3997, Test Loss: 55.1645\n",
      "Epoch 51, Training Loss: 34.3460, Validation Loss: 35.9286, Test Loss: 30.9800\n",
      "Epoch 61, Training Loss: 20.1758, Validation Loss: 20.6185, Test Loss: 18.0248\n",
      "Epoch 71, Training Loss: 13.4648, Validation Loss: 13.5470, Test Loss: 12.0518\n",
      "Epoch 81, Training Loss: 9.5267, Validation Loss: 9.3470, Test Loss: 8.6462\n",
      "Epoch 91, Training Loss: 6.9215, Validation Loss: 6.7478, Test Loss: 6.4068\n",
      "Epoch 101, Training Loss: 5.1785, Validation Loss: 5.0277, Test Loss: 4.9234\n",
      "Epoch 111, Training Loss: 3.9966, Validation Loss: 3.7812, Test Loss: 3.8891\n",
      "Epoch 121, Training Loss: 3.1374, Validation Loss: 2.8842, Test Loss: 3.1067\n",
      "Epoch 131, Training Loss: 2.4711, Validation Loss: 2.2060, Test Loss: 2.5146\n",
      "Epoch 141, Training Loss: 1.9747, Validation Loss: 1.7064, Test Loss: 2.0689\n",
      "Epoch 151, Training Loss: 1.6047, Validation Loss: 1.3481, Test Loss: 1.7338\n",
      "Epoch 161, Training Loss: 1.3332, Validation Loss: 1.0685, Test Loss: 1.4855\n",
      "Epoch 171, Training Loss: 1.1337, Validation Loss: 0.8596, Test Loss: 1.3097\n",
      "Epoch 181, Training Loss: 0.9840, Validation Loss: 0.7037, Test Loss: 1.1726\n",
      "Epoch 191, Training Loss: 0.8746, Validation Loss: 0.5901, Test Loss: 1.0715\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 0.8746\n",
      "Validation loss: 0.5901\n",
      "Test loss: 1.0715\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 222/243 with parameters:\n",
      "Config indices: (2, 2, 0, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 5816.9819, Validation Loss: 5298.5581, Test Loss: 5836.2168\n",
      "Epoch 11, Training Loss: 331.6242, Validation Loss: 319.1541, Test Loss: 341.5542\n",
      "Epoch 21, Training Loss: 91.3723, Validation Loss: 88.7183, Test Loss: 87.5453\n",
      "Epoch 31, Training Loss: 36.8656, Validation Loss: 33.7575, Test Loss: 34.4877\n",
      "Epoch 41, Training Loss: 19.4481, Validation Loss: 17.3486, Test Loss: 18.6324\n",
      "Epoch 51, Training Loss: 11.6304, Validation Loss: 9.9139, Test Loss: 11.4281\n",
      "Epoch 61, Training Loss: 6.6741, Validation Loss: 5.4373, Test Loss: 6.7863\n",
      "Epoch 71, Training Loss: 3.9801, Validation Loss: 3.1025, Test Loss: 4.0948\n",
      "Epoch 81, Training Loss: 2.9133, Validation Loss: 2.2023, Test Loss: 2.9977\n",
      "Epoch 91, Training Loss: 2.4274, Validation Loss: 1.8069, Test Loss: 2.4894\n",
      "Epoch 101, Training Loss: 2.1369, Validation Loss: 1.5786, Test Loss: 2.2072\n",
      "Epoch 111, Training Loss: 1.9217, Validation Loss: 1.4095, Test Loss: 1.9920\n",
      "Epoch 121, Training Loss: 1.7540, Validation Loss: 1.2594, Test Loss: 1.8276\n",
      "Epoch 131, Training Loss: 1.6128, Validation Loss: 1.1341, Test Loss: 1.7026\n",
      "Epoch 141, Training Loss: 1.4943, Validation Loss: 1.0232, Test Loss: 1.5888\n",
      "Epoch 151, Training Loss: 1.3880, Validation Loss: 0.9243, Test Loss: 1.4925\n",
      "Epoch 161, Training Loss: 1.3007, Validation Loss: 0.8465, Test Loss: 1.4188\n",
      "Epoch 171, Training Loss: 1.2317, Validation Loss: 0.7858, Test Loss: 1.3474\n",
      "Epoch 181, Training Loss: 1.1661, Validation Loss: 0.7255, Test Loss: 1.2873\n",
      "Epoch 191, Training Loss: 1.1077, Validation Loss: 0.6800, Test Loss: 1.2402\n",
      "Epoch 201, Training Loss: 1.0578, Validation Loss: 0.6406, Test Loss: 1.1979\n",
      "Epoch 211, Training Loss: 1.0164, Validation Loss: 0.6097, Test Loss: 1.1611\n",
      "Epoch 221, Training Loss: 0.9806, Validation Loss: 0.5803, Test Loss: 1.1320\n",
      "Epoch 231, Training Loss: 0.9500, Validation Loss: 0.5550, Test Loss: 1.1014\n",
      "Epoch 241, Training Loss: 0.9216, Validation Loss: 0.5326, Test Loss: 1.0796\n",
      "Epoch 251, Training Loss: 0.8950, Validation Loss: 0.5088, Test Loss: 1.0597\n",
      "Epoch 261, Training Loss: 0.8727, Validation Loss: 0.4910, Test Loss: 1.0368\n",
      "Epoch 271, Training Loss: 0.8521, Validation Loss: 0.4727, Test Loss: 1.0246\n",
      "Epoch 281, Training Loss: 0.8337, Validation Loss: 0.4579, Test Loss: 1.0033\n",
      "Epoch 291, Training Loss: 0.8171, Validation Loss: 0.4440, Test Loss: 0.9925\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 0.8171\n",
      "Validation loss: 0.4440\n",
      "Test loss: 0.9925\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 223/243 with parameters:\n",
      "Config indices: (2, 2, 0, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 6578.4702, Validation Loss: 6005.0996, Test Loss: 6590.7056\n",
      "Epoch 11, Training Loss: 444.2043, Validation Loss: 434.1894, Test Loss: 473.6274\n",
      "Epoch 21, Training Loss: 220.5417, Validation Loss: 223.4754, Test Loss: 227.6825\n",
      "Epoch 31, Training Loss: 144.5617, Validation Loss: 146.7174, Test Loss: 144.2606\n",
      "Epoch 41, Training Loss: 94.5214, Validation Loss: 96.9322, Test Loss: 91.1930\n",
      "Epoch 51, Training Loss: 61.3387, Validation Loss: 62.9216, Test Loss: 57.4599\n",
      "Epoch 61, Training Loss: 39.3362, Validation Loss: 40.0614, Test Loss: 36.3245\n",
      "Epoch 71, Training Loss: 25.4794, Validation Loss: 25.7189, Test Loss: 23.5757\n",
      "Epoch 81, Training Loss: 17.1934, Validation Loss: 17.0231, Test Loss: 16.0798\n",
      "Epoch 91, Training Loss: 11.8106, Validation Loss: 11.4441, Test Loss: 11.2989\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 11.8106\n",
      "Validation loss: 11.4441\n",
      "Test loss: 11.2989\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 224/243 with parameters:\n",
      "Config indices: (2, 2, 0, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 6982.7207, Validation Loss: 6376.1313, Test Loss: 6990.5049\n",
      "Epoch 11, Training Loss: 437.3639, Validation Loss: 408.9181, Test Loss: 448.8925\n",
      "Epoch 21, Training Loss: 211.7235, Validation Loss: 212.0742, Test Loss: 224.4657\n",
      "Epoch 31, Training Loss: 147.4788, Validation Loss: 149.1994, Test Loss: 155.3239\n",
      "Epoch 41, Training Loss: 101.7178, Validation Loss: 104.0127, Test Loss: 106.5116\n",
      "Epoch 51, Training Loss: 69.6296, Validation Loss: 71.5643, Test Loss: 72.3192\n",
      "Epoch 61, Training Loss: 47.5249, Validation Loss: 48.9104, Test Loss: 49.1758\n",
      "Epoch 71, Training Loss: 32.5745, Validation Loss: 33.3892, Test Loss: 33.2108\n",
      "Epoch 81, Training Loss: 22.7997, Validation Loss: 23.0950, Test Loss: 22.7145\n",
      "Epoch 91, Training Loss: 15.3269, Validation Loss: 15.2909, Test Loss: 15.0130\n",
      "Epoch 101, Training Loss: 9.4483, Validation Loss: 9.2238, Test Loss: 9.2028\n",
      "Epoch 111, Training Loss: 5.7638, Validation Loss: 5.4400, Test Loss: 5.6468\n",
      "Epoch 121, Training Loss: 3.5477, Validation Loss: 3.1879, Test Loss: 3.5644\n",
      "Epoch 131, Training Loss: 2.2893, Validation Loss: 1.9240, Test Loss: 2.4015\n",
      "Epoch 141, Training Loss: 1.5929, Validation Loss: 1.2447, Test Loss: 1.7554\n",
      "Epoch 151, Training Loss: 1.2111, Validation Loss: 0.8888, Test Loss: 1.4079\n",
      "Epoch 161, Training Loss: 0.9914, Validation Loss: 0.6946, Test Loss: 1.1920\n",
      "Epoch 171, Training Loss: 0.8592, Validation Loss: 0.5888, Test Loss: 1.0539\n",
      "Epoch 181, Training Loss: 0.7784, Validation Loss: 0.5272, Test Loss: 0.9632\n",
      "Epoch 191, Training Loss: 0.7234, Validation Loss: 0.4897, Test Loss: 0.9067\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 0.7234\n",
      "Validation loss: 0.4897\n",
      "Test loss: 0.9067\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 225/243 with parameters:\n",
      "Config indices: (2, 2, 0, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.01, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 6973.3896, Validation Loss: 6373.9150, Test Loss: 6981.5073\n",
      "Epoch 11, Training Loss: 589.4371, Validation Loss: 562.4462, Test Loss: 618.2695\n",
      "Epoch 21, Training Loss: 327.7461, Validation Loss: 331.2306, Test Loss: 344.3570\n",
      "Epoch 31, Training Loss: 220.3479, Validation Loss: 225.8055, Test Loss: 225.5037\n",
      "Epoch 41, Training Loss: 159.5451, Validation Loss: 164.0760, Test Loss: 158.2170\n",
      "Epoch 51, Training Loss: 118.5721, Validation Loss: 121.9172, Test Loss: 114.1516\n",
      "Epoch 61, Training Loss: 87.2689, Validation Loss: 89.8912, Test Loss: 81.8651\n",
      "Epoch 71, Training Loss: 63.5100, Validation Loss: 65.6680, Test Loss: 58.4690\n",
      "Epoch 81, Training Loss: 45.9809, Validation Loss: 47.4240, Test Loss: 41.7039\n",
      "Epoch 91, Training Loss: 33.0287, Validation Loss: 34.1036, Test Loss: 29.7228\n",
      "Epoch 101, Training Loss: 23.3577, Validation Loss: 23.9301, Test Loss: 20.8884\n",
      "Epoch 111, Training Loss: 16.2227, Validation Loss: 16.3487, Test Loss: 14.4432\n",
      "Epoch 121, Training Loss: 11.2610, Validation Loss: 11.1638, Test Loss: 10.0377\n",
      "Epoch 131, Training Loss: 7.9762, Validation Loss: 7.7082, Test Loss: 7.1003\n",
      "Epoch 141, Training Loss: 5.7668, Validation Loss: 5.4222, Test Loss: 5.1458\n",
      "Epoch 151, Training Loss: 4.3891, Validation Loss: 4.0270, Test Loss: 3.9289\n",
      "Epoch 161, Training Loss: 3.5117, Validation Loss: 3.1656, Test Loss: 3.1898\n",
      "Epoch 171, Training Loss: 2.8938, Validation Loss: 2.5364, Test Loss: 2.6647\n",
      "Epoch 181, Training Loss: 2.4368, Validation Loss: 2.0745, Test Loss: 2.2872\n",
      "Epoch 191, Training Loss: 2.1005, Validation Loss: 1.7501, Test Loss: 2.0026\n",
      "Epoch 201, Training Loss: 1.8493, Validation Loss: 1.5207, Test Loss: 1.7948\n",
      "Epoch 211, Training Loss: 1.6688, Validation Loss: 1.3534, Test Loss: 1.6426\n",
      "Epoch 221, Training Loss: 1.5258, Validation Loss: 1.2210, Test Loss: 1.5363\n",
      "Epoch 231, Training Loss: 1.4162, Validation Loss: 1.1120, Test Loss: 1.4411\n",
      "Epoch 241, Training Loss: 1.3238, Validation Loss: 1.0235, Test Loss: 1.3573\n",
      "Epoch 251, Training Loss: 1.2470, Validation Loss: 0.9512, Test Loss: 1.2966\n",
      "Epoch 261, Training Loss: 1.1839, Validation Loss: 0.8910, Test Loss: 1.2454\n",
      "Epoch 271, Training Loss: 1.1306, Validation Loss: 0.8411, Test Loss: 1.1995\n",
      "Epoch 281, Training Loss: 1.0857, Validation Loss: 0.8003, Test Loss: 1.1657\n",
      "Epoch 291, Training Loss: 1.0445, Validation Loss: 0.7638, Test Loss: 1.1277\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.01\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 1.0445\n",
      "Validation loss: 0.7638\n",
      "Test loss: 1.1277\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 226/243 with parameters:\n",
      "Config indices: (2, 2, 1, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7151.3618, Validation Loss: 6540.8818, Test Loss: 7158.1870\n",
      "Epoch 11, Training Loss: 7010.5181, Validation Loss: 6402.2417, Test Loss: 7018.1772\n",
      "Epoch 21, Training Loss: 6739.8296, Validation Loss: 6131.4033, Test Loss: 6749.8208\n",
      "Epoch 31, Training Loss: 6351.1753, Validation Loss: 5741.2671, Test Loss: 6364.0391\n",
      "Epoch 41, Training Loss: 5888.5659, Validation Loss: 5279.1196, Test Loss: 5903.5459\n",
      "Epoch 51, Training Loss: 5395.9819, Validation Loss: 4792.2031, Test Loss: 5411.0107\n",
      "Epoch 61, Training Loss: 4902.7437, Validation Loss: 4313.1836, Test Loss: 4915.0396\n",
      "Epoch 71, Training Loss: 4438.1309, Validation Loss: 3872.9241, Test Loss: 4445.1860\n",
      "Epoch 81, Training Loss: 4019.1689, Validation Loss: 3487.4966, Test Loss: 4019.9031\n",
      "Epoch 91, Training Loss: 3637.8044, Validation Loss: 3146.0002, Test Loss: 3632.8250\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 3637.8044\n",
      "Validation loss: 3146.0002\n",
      "Test loss: 3632.8250\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 227/243 with parameters:\n",
      "Config indices: (2, 2, 1, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7159.1646, Validation Loss: 6548.4937, Test Loss: 7165.9697\n",
      "Epoch 11, Training Loss: 7047.7466, Validation Loss: 6439.5444, Test Loss: 7055.0933\n",
      "Epoch 21, Training Loss: 6824.0625, Validation Loss: 6220.4575, Test Loss: 6833.3037\n",
      "Epoch 31, Training Loss: 6503.6719, Validation Loss: 5907.3315, Test Loss: 6515.8228\n",
      "Epoch 41, Training Loss: 6105.0972, Validation Loss: 5519.2231, Test Loss: 6120.6514\n",
      "Epoch 51, Training Loss: 5651.0366, Validation Loss: 5079.5068, Test Loss: 5669.8066\n",
      "Epoch 61, Training Loss: 5164.6641, Validation Loss: 4612.0957, Test Loss: 5185.9204\n",
      "Epoch 71, Training Loss: 4668.9644, Validation Loss: 4140.3882, Test Loss: 4691.5566\n",
      "Epoch 81, Training Loss: 4184.1235, Validation Loss: 3684.5635, Test Loss: 4206.6655\n",
      "Epoch 91, Training Loss: 3720.1052, Validation Loss: 3254.4556, Test Loss: 3741.2429\n",
      "Epoch 101, Training Loss: 3283.4465, Validation Loss: 2856.4011, Test Loss: 3302.1707\n",
      "Epoch 111, Training Loss: 2887.5945, Validation Loss: 2501.6370, Test Loss: 2903.6895\n",
      "Epoch 121, Training Loss: 2531.3611, Validation Loss: 2187.2146, Test Loss: 2545.2625\n",
      "Epoch 131, Training Loss: 2211.7878, Validation Loss: 1908.6672, Test Loss: 2224.2891\n",
      "Epoch 141, Training Loss: 1926.8055, Validation Loss: 1662.4403, Test Loss: 1938.8079\n",
      "Epoch 151, Training Loss: 1675.1630, Validation Loss: 1446.5583, Test Loss: 1687.4844\n",
      "Epoch 161, Training Loss: 1455.8403, Validation Loss: 1259.5258, Test Loss: 1469.0519\n",
      "Epoch 171, Training Loss: 1267.4529, Validation Loss: 1099.6284, Test Loss: 1281.8433\n",
      "Epoch 181, Training Loss: 1108.1770, Validation Loss: 965.1918, Test Loss: 1123.8527\n",
      "Epoch 191, Training Loss: 975.4327, Validation Loss: 853.8934, Test Loss: 992.5728\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 975.4327\n",
      "Validation loss: 853.8934\n",
      "Test loss: 992.5728\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 228/243 with parameters:\n",
      "Config indices: (2, 2, 1, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7155.4917, Validation Loss: 6545.0889, Test Loss: 7162.3066\n",
      "Epoch 11, Training Loss: 7003.8184, Validation Loss: 6399.1523, Test Loss: 7011.5269\n",
      "Epoch 21, Training Loss: 6713.2349, Validation Loss: 6119.7012, Test Loss: 6723.5923\n",
      "Epoch 31, Training Loss: 6299.6553, Validation Loss: 5722.4805, Test Loss: 6314.2925\n",
      "Epoch 41, Training Loss: 5793.0454, Validation Loss: 5237.3691, Test Loss: 5812.9077\n",
      "Epoch 51, Training Loss: 5211.2153, Validation Loss: 4682.7280, Test Loss: 5236.6284\n",
      "Epoch 61, Training Loss: 4592.6685, Validation Loss: 4097.3198, Test Loss: 4623.1509\n",
      "Epoch 71, Training Loss: 3978.1702, Validation Loss: 3520.9067, Test Loss: 4012.5857\n",
      "Epoch 81, Training Loss: 3398.0298, Validation Loss: 2982.8118, Test Loss: 3434.9646\n",
      "Epoch 91, Training Loss: 2873.3738, Validation Loss: 2502.7207, Test Loss: 2911.4883\n",
      "Epoch 101, Training Loss: 2415.5857, Validation Loss: 2090.1750, Test Loss: 2453.8618\n",
      "Epoch 111, Training Loss: 2026.6838, Validation Loss: 1745.3666, Test Loss: 2064.6685\n",
      "Epoch 121, Training Loss: 1702.7776, Validation Loss: 1462.7416, Test Loss: 1740.6935\n",
      "Epoch 131, Training Loss: 1437.7488, Validation Loss: 1234.8405, Test Loss: 1475.9280\n",
      "Epoch 141, Training Loss: 1224.7499, Validation Loss: 1054.1781, Test Loss: 1263.4956\n",
      "Epoch 151, Training Loss: 1056.8324, Validation Loss: 913.5051, Test Loss: 1096.4440\n",
      "Epoch 161, Training Loss: 926.7849, Validation Loss: 805.8922, Test Loss: 967.5605\n",
      "Epoch 171, Training Loss: 827.3902, Validation Loss: 724.7256, Test Loss: 869.5795\n",
      "Epoch 181, Training Loss: 752.4739, Validation Loss: 664.2271, Test Loss: 795.9730\n",
      "Epoch 191, Training Loss: 696.3583, Validation Loss: 619.5162, Test Loss: 740.9604\n",
      "Epoch 201, Training Loss: 654.2535, Validation Loss: 586.3083, Test Loss: 699.8406\n",
      "Epoch 211, Training Loss: 622.2405, Validation Loss: 561.2469, Test Loss: 668.7048\n",
      "Epoch 221, Training Loss: 597.3651, Validation Loss: 541.9844, Test Loss: 644.5655\n",
      "Epoch 231, Training Loss: 577.5096, Validation Loss: 526.7073, Test Loss: 625.2924\n",
      "Epoch 241, Training Loss: 561.1611, Validation Loss: 514.0378, Test Loss: 609.3045\n",
      "Epoch 251, Training Loss: 547.2130, Validation Loss: 503.0919, Test Loss: 595.5423\n",
      "Epoch 261, Training Loss: 535.0070, Validation Loss: 493.2822, Test Loss: 583.3750\n",
      "Epoch 271, Training Loss: 524.0306, Validation Loss: 484.3541, Test Loss: 572.2505\n",
      "Epoch 281, Training Loss: 513.9418, Validation Loss: 476.0766, Test Loss: 561.8876\n",
      "Epoch 291, Training Loss: 504.4663, Validation Loss: 468.2622, Test Loss: 552.0792\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 504.4663\n",
      "Validation loss: 468.2622\n",
      "Test loss: 552.0792\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 229/243 with parameters:\n",
      "Config indices: (2, 2, 1, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7161.2402, Validation Loss: 6550.4839, Test Loss: 7168.0298\n",
      "Epoch 11, Training Loss: 7119.9336, Validation Loss: 6510.3857, Test Loss: 7126.8208\n",
      "Epoch 21, Training Loss: 7053.8789, Validation Loss: 6446.1704, Test Loss: 7061.1763\n",
      "Epoch 31, Training Loss: 6960.2754, Validation Loss: 6355.4458, Test Loss: 6968.3022\n",
      "Epoch 41, Training Loss: 6840.4497, Validation Loss: 6239.4917, Test Loss: 6849.5312\n",
      "Epoch 51, Training Loss: 6696.0034, Validation Loss: 6099.8687, Test Loss: 6706.4058\n",
      "Epoch 61, Training Loss: 6528.9097, Validation Loss: 5938.6235, Test Loss: 6540.8584\n",
      "Epoch 71, Training Loss: 6341.4219, Validation Loss: 5758.0576, Test Loss: 6355.0801\n",
      "Epoch 81, Training Loss: 6135.8306, Validation Loss: 5560.5386, Test Loss: 6151.2939\n",
      "Epoch 91, Training Loss: 5914.6309, Validation Loss: 5348.6230, Test Loss: 5931.9507\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 5914.6309\n",
      "Validation loss: 5348.6230\n",
      "Test loss: 5931.9507\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 230/243 with parameters:\n",
      "Config indices: (2, 2, 1, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7160.8687, Validation Loss: 6550.2378, Test Loss: 7167.6763\n",
      "Epoch 11, Training Loss: 7106.4336, Validation Loss: 6498.8223, Test Loss: 7113.5083\n",
      "Epoch 21, Training Loss: 7012.1250, Validation Loss: 6410.0903, Test Loss: 7020.0503\n",
      "Epoch 31, Training Loss: 6868.7710, Validation Loss: 6275.4951, Test Loss: 6878.1743\n",
      "Epoch 41, Training Loss: 6679.3569, Validation Loss: 6097.7568, Test Loss: 6690.7974\n",
      "Epoch 51, Training Loss: 6445.5820, Validation Loss: 5878.8496, Test Loss: 6459.6772\n",
      "Epoch 61, Training Loss: 6171.7754, Validation Loss: 5622.9038, Test Loss: 6188.9810\n",
      "Epoch 71, Training Loss: 5865.1909, Validation Loss: 5336.5352, Test Loss: 5885.7441\n",
      "Epoch 81, Training Loss: 5531.2856, Validation Loss: 5024.7910, Test Loss: 5555.4917\n",
      "Epoch 91, Training Loss: 5177.7754, Validation Loss: 4695.2861, Test Loss: 5205.7754\n",
      "Epoch 101, Training Loss: 4811.2109, Validation Loss: 4354.2046, Test Loss: 4842.9673\n",
      "Epoch 111, Training Loss: 4438.1587, Validation Loss: 4007.8301, Test Loss: 4473.5200\n",
      "Epoch 121, Training Loss: 4066.8579, Validation Loss: 3663.8418, Test Loss: 4105.5107\n",
      "Epoch 131, Training Loss: 3702.2109, Validation Loss: 3326.9717, Test Loss: 3743.7852\n",
      "Epoch 141, Training Loss: 3350.0027, Validation Loss: 3002.6675, Test Loss: 3394.0566\n",
      "Epoch 151, Training Loss: 3016.3738, Validation Loss: 2696.5659, Test Loss: 3062.3931\n",
      "Epoch 161, Training Loss: 2703.5591, Validation Loss: 2410.7854, Test Loss: 2751.0354\n",
      "Epoch 171, Training Loss: 2415.7065, Validation Loss: 2149.0498, Test Loss: 2464.1316\n",
      "Epoch 181, Training Loss: 2154.5078, Validation Loss: 1912.8173, Test Loss: 2203.4009\n",
      "Epoch 191, Training Loss: 1920.2672, Validation Loss: 1702.2769, Test Loss: 1969.2166\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 1920.2672\n",
      "Validation loss: 1702.2769\n",
      "Test loss: 1969.2166\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 231/243 with parameters:\n",
      "Config indices: (2, 2, 1, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7161.3066, Validation Loss: 6550.6870, Test Loss: 7168.1167\n",
      "Epoch 11, Training Loss: 7124.2451, Validation Loss: 6515.4429, Test Loss: 7131.1191\n",
      "Epoch 21, Training Loss: 7071.2319, Validation Loss: 6464.5352, Test Loss: 7078.4312\n",
      "Epoch 31, Training Loss: 6999.6572, Validation Loss: 6395.1460, Test Loss: 7007.4355\n",
      "Epoch 41, Training Loss: 6909.1313, Validation Loss: 6306.6475, Test Loss: 6917.7109\n",
      "Epoch 51, Training Loss: 6800.3320, Validation Loss: 6199.7520, Test Loss: 6809.8857\n",
      "Epoch 61, Training Loss: 6674.7295, Validation Loss: 6076.0083, Test Loss: 6685.4077\n",
      "Epoch 71, Training Loss: 6533.6226, Validation Loss: 5936.8213, Test Loss: 6545.5264\n",
      "Epoch 81, Training Loss: 6378.9780, Validation Loss: 5784.1899, Test Loss: 6392.1338\n",
      "Epoch 91, Training Loss: 6212.3711, Validation Loss: 5619.8408, Test Loss: 6226.7485\n",
      "Epoch 101, Training Loss: 6035.9531, Validation Loss: 5446.0693, Test Loss: 6051.4712\n",
      "Epoch 111, Training Loss: 5851.8521, Validation Loss: 5265.0591, Test Loss: 5868.3584\n",
      "Epoch 121, Training Loss: 5661.8804, Validation Loss: 5078.8218, Test Loss: 5679.1729\n",
      "Epoch 131, Training Loss: 5468.1582, Validation Loss: 4889.5684, Test Loss: 5485.9634\n",
      "Epoch 141, Training Loss: 5272.8735, Validation Loss: 4699.5874, Test Loss: 5290.8784\n",
      "Epoch 151, Training Loss: 5077.7158, Validation Loss: 4510.6631, Test Loss: 5095.5752\n",
      "Epoch 161, Training Loss: 4884.4951, Validation Loss: 4324.6548, Test Loss: 4901.8252\n",
      "Epoch 171, Training Loss: 4694.3384, Validation Loss: 4142.8306, Test Loss: 4710.7773\n",
      "Epoch 181, Training Loss: 4508.7090, Validation Loss: 3966.6074, Test Loss: 4523.8936\n",
      "Epoch 191, Training Loss: 4328.4204, Validation Loss: 3796.7942, Test Loss: 4342.0327\n",
      "Epoch 201, Training Loss: 4154.3931, Validation Loss: 3634.2842, Test Loss: 4166.1821\n",
      "Epoch 211, Training Loss: 3986.1794, Validation Loss: 3478.6094, Test Loss: 3995.9561\n",
      "Epoch 221, Training Loss: 3824.6360, Validation Loss: 3330.4739, Test Loss: 3832.3113\n",
      "Epoch 231, Training Loss: 3669.0105, Validation Loss: 3189.0466, Test Loss: 3674.5679\n",
      "Epoch 241, Training Loss: 3519.3540, Validation Loss: 3054.1882, Test Loss: 3522.8335\n",
      "Epoch 251, Training Loss: 3374.6377, Validation Loss: 2924.8186, Test Loss: 3376.1697\n",
      "Epoch 261, Training Loss: 3234.5742, Validation Loss: 2800.4812, Test Loss: 3234.3633\n",
      "Epoch 271, Training Loss: 3098.7771, Validation Loss: 2680.6311, Test Loss: 3097.0435\n",
      "Epoch 281, Training Loss: 2966.3430, Validation Loss: 2564.3657, Test Loss: 2963.3806\n",
      "Epoch 291, Training Loss: 2837.7185, Validation Loss: 2451.8804, Test Loss: 2833.8430\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 2837.7185\n",
      "Validation loss: 2451.8804\n",
      "Test loss: 2833.8430\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 232/243 with parameters:\n",
      "Config indices: (2, 2, 1, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7162.7246, Validation Loss: 6551.9795, Test Loss: 7169.5161\n",
      "Epoch 11, Training Loss: 7136.3726, Validation Loss: 6527.4292, Test Loss: 7143.1714\n",
      "Epoch 21, Training Loss: 7102.6187, Validation Loss: 6496.5381, Test Loss: 7109.5713\n",
      "Epoch 31, Training Loss: 7056.5811, Validation Loss: 6454.7964, Test Loss: 7063.8677\n",
      "Epoch 41, Training Loss: 6997.6982, Validation Loss: 6401.6855, Test Loss: 7005.5098\n",
      "Epoch 51, Training Loss: 6925.7881, Validation Loss: 6337.0381, Test Loss: 6934.3252\n",
      "Epoch 61, Training Loss: 6841.8750, Validation Loss: 6261.7954, Test Loss: 6851.3291\n",
      "Epoch 71, Training Loss: 6747.5420, Validation Loss: 6177.3701, Test Loss: 6758.0923\n",
      "Epoch 81, Training Loss: 6642.0132, Validation Loss: 6083.0747, Test Loss: 6653.8501\n",
      "Epoch 91, Training Loss: 6526.4092, Validation Loss: 5979.9150, Test Loss: 6539.6938\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 6526.4092\n",
      "Validation loss: 5979.9150\n",
      "Test loss: 6539.6938\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 233/243 with parameters:\n",
      "Config indices: (2, 2, 1, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7161.1689, Validation Loss: 6550.5464, Test Loss: 7168.0044\n",
      "Epoch 11, Training Loss: 7130.1689, Validation Loss: 6520.9805, Test Loss: 7137.1558\n",
      "Epoch 21, Training Loss: 7089.8369, Validation Loss: 6482.3638, Test Loss: 7097.1274\n",
      "Epoch 31, Training Loss: 7038.0566, Validation Loss: 6432.6392, Test Loss: 7045.7549\n",
      "Epoch 41, Training Loss: 6974.9741, Validation Loss: 6371.8452, Test Loss: 6983.1919\n",
      "Epoch 51, Training Loss: 6901.1479, Validation Loss: 6300.5298, Test Loss: 6909.9746\n",
      "Epoch 61, Training Loss: 6817.4902, Validation Loss: 6219.6299, Test Loss: 6827.0161\n",
      "Epoch 71, Training Loss: 6724.0239, Validation Loss: 6129.2656, Test Loss: 6734.3521\n",
      "Epoch 81, Training Loss: 6620.2036, Validation Loss: 6028.9648, Test Loss: 6631.4009\n",
      "Epoch 91, Training Loss: 6507.7510, Validation Loss: 5920.3452, Test Loss: 6519.8945\n",
      "Epoch 101, Training Loss: 6387.4131, Validation Loss: 5804.1558, Test Loss: 6400.5410\n",
      "Epoch 111, Training Loss: 6260.1392, Validation Loss: 5681.3560, Test Loss: 6274.2739\n",
      "Epoch 121, Training Loss: 6126.1436, Validation Loss: 5552.2666, Test Loss: 6141.2817\n",
      "Epoch 131, Training Loss: 5986.9097, Validation Loss: 5418.2964, Test Loss: 6003.0146\n",
      "Epoch 141, Training Loss: 5842.6758, Validation Loss: 5279.7080, Test Loss: 5859.7031\n",
      "Epoch 151, Training Loss: 5694.1675, Validation Loss: 5137.2456, Test Loss: 5712.0420\n",
      "Epoch 161, Training Loss: 5542.6963, Validation Loss: 4992.2041, Test Loss: 5561.3281\n",
      "Epoch 171, Training Loss: 5388.4121, Validation Loss: 4844.8105, Test Loss: 5407.6904\n",
      "Epoch 181, Training Loss: 5231.8384, Validation Loss: 4695.5986, Test Loss: 5251.6597\n",
      "Epoch 191, Training Loss: 5073.7744, Validation Loss: 4545.4082, Test Loss: 5094.0176\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 5073.7744\n",
      "Validation loss: 4545.4082\n",
      "Test loss: 5094.0176\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 234/243 with parameters:\n",
      "Config indices: (2, 2, 1, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7163.7012, Validation Loss: 6552.9409, Test Loss: 7170.5107\n",
      "Epoch 11, Training Loss: 7142.2920, Validation Loss: 6532.9062, Test Loss: 7149.0884\n",
      "Epoch 21, Training Loss: 7110.2686, Validation Loss: 6503.3462, Test Loss: 7117.2593\n",
      "Epoch 31, Training Loss: 7064.0518, Validation Loss: 6460.7798, Test Loss: 7071.3511\n",
      "Epoch 41, Training Loss: 7001.5493, Validation Loss: 6403.4268, Test Loss: 7009.3252\n",
      "Epoch 51, Training Loss: 6925.2646, Validation Loss: 6333.5967, Test Loss: 6933.6685\n",
      "Epoch 61, Training Loss: 6835.3701, Validation Loss: 6251.3530, Test Loss: 6844.5649\n",
      "Epoch 71, Training Loss: 6731.5347, Validation Loss: 6156.3989, Test Loss: 6741.6841\n",
      "Epoch 81, Training Loss: 6614.8110, Validation Loss: 6049.7388, Test Loss: 6626.0640\n",
      "Epoch 91, Training Loss: 6485.3530, Validation Loss: 5931.4673, Test Loss: 6497.8613\n",
      "Epoch 101, Training Loss: 6345.0176, Validation Loss: 5803.2876, Test Loss: 6358.9307\n",
      "Epoch 111, Training Loss: 6194.3076, Validation Loss: 5665.6504, Test Loss: 6209.7734\n",
      "Epoch 121, Training Loss: 6034.6768, Validation Loss: 5519.8604, Test Loss: 6051.8066\n",
      "Epoch 131, Training Loss: 5865.5845, Validation Loss: 5365.4839, Test Loss: 5884.4702\n",
      "Epoch 141, Training Loss: 5689.1836, Validation Loss: 5204.4229, Test Loss: 5709.8994\n",
      "Epoch 151, Training Loss: 5505.9707, Validation Loss: 5037.1626, Test Loss: 5528.6113\n",
      "Epoch 161, Training Loss: 5317.3843, Validation Loss: 4865.0420, Test Loss: 5342.0088\n",
      "Epoch 171, Training Loss: 5124.4585, Validation Loss: 4688.9922, Test Loss: 5151.0806\n",
      "Epoch 181, Training Loss: 4928.1396, Validation Loss: 4509.8555, Test Loss: 4956.7803\n",
      "Epoch 191, Training Loss: 4729.1455, Validation Loss: 4328.3262, Test Loss: 4759.8154\n",
      "Epoch 201, Training Loss: 4528.5527, Validation Loss: 4145.3506, Test Loss: 4561.2441\n",
      "Epoch 211, Training Loss: 4327.7866, Validation Loss: 3962.2727, Test Loss: 4362.4746\n",
      "Epoch 221, Training Loss: 4127.5732, Validation Loss: 3779.7427, Test Loss: 4164.2144\n",
      "Epoch 231, Training Loss: 3929.1226, Validation Loss: 3598.8772, Test Loss: 3967.6626\n",
      "Epoch 241, Training Loss: 3733.4099, Validation Loss: 3420.5508, Test Loss: 3773.7751\n",
      "Epoch 251, Training Loss: 3540.5742, Validation Loss: 3244.9468, Test Loss: 3582.6975\n",
      "Epoch 261, Training Loss: 3351.5784, Validation Loss: 3072.8862, Test Loss: 3395.3713\n",
      "Epoch 271, Training Loss: 3167.0906, Validation Loss: 2905.0054, Test Loss: 3212.4536\n",
      "Epoch 281, Training Loss: 2988.4480, Validation Loss: 2742.4951, Test Loss: 3035.2612\n",
      "Epoch 291, Training Loss: 2816.3645, Validation Loss: 2585.9988, Test Loss: 2864.5093\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 2816.3645\n",
      "Validation loss: 2585.9988\n",
      "Test loss: 2864.5093\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 235/243 with parameters:\n",
      "Config indices: (2, 2, 2, 0, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7165.6372, Validation Loss: 6554.7832, Test Loss: 7172.4639\n",
      "Epoch 11, Training Loss: 7163.4424, Validation Loss: 6552.7061, Test Loss: 7170.2686\n",
      "Epoch 21, Training Loss: 7162.0229, Validation Loss: 6551.3564, Test Loss: 7168.8525\n",
      "Epoch 31, Training Loss: 7160.7632, Validation Loss: 6550.1519, Test Loss: 7167.5942\n",
      "Epoch 41, Training Loss: 7159.6016, Validation Loss: 6549.0415, Test Loss: 7166.4341\n",
      "Epoch 51, Training Loss: 7158.4941, Validation Loss: 6547.9844, Test Loss: 7165.3315\n",
      "Epoch 61, Training Loss: 7157.4204, Validation Loss: 6546.9604, Test Loss: 7164.2637\n",
      "Epoch 71, Training Loss: 7156.3735, Validation Loss: 6545.9575, Test Loss: 7163.2183\n",
      "Epoch 81, Training Loss: 7155.3418, Validation Loss: 6544.9712, Test Loss: 7162.1885\n",
      "Epoch 91, Training Loss: 7154.3184, Validation Loss: 6543.9932, Test Loss: 7161.1694\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 100\n",
      "Training loss: 7154.3184\n",
      "Validation loss: 6543.9932\n",
      "Test loss: 7161.1694\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 236/243 with parameters:\n",
      "Config indices: (2, 2, 2, 0, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.9316, Validation Loss: 6555.9077, Test Loss: 7173.7358\n",
      "Epoch 11, Training Loss: 7164.9253, Validation Loss: 6554.0112, Test Loss: 7171.7246\n",
      "Epoch 21, Training Loss: 7163.6333, Validation Loss: 6552.7891, Test Loss: 7170.4258\n",
      "Epoch 31, Training Loss: 7162.5757, Validation Loss: 6551.7896, Test Loss: 7169.3706\n",
      "Epoch 41, Training Loss: 7161.6509, Validation Loss: 6550.9126, Test Loss: 7168.4429\n",
      "Epoch 51, Training Loss: 7160.8188, Validation Loss: 6550.1206, Test Loss: 7167.6118\n",
      "Epoch 61, Training Loss: 7160.0601, Validation Loss: 6549.3975, Test Loss: 7166.8525\n",
      "Epoch 71, Training Loss: 7159.3491, Validation Loss: 6548.7188, Test Loss: 7166.1436\n",
      "Epoch 81, Training Loss: 7158.6777, Validation Loss: 6548.0776, Test Loss: 7165.4697\n",
      "Epoch 91, Training Loss: 7158.0342, Validation Loss: 6547.4604, Test Loss: 7164.8257\n",
      "Epoch 101, Training Loss: 7157.4106, Validation Loss: 6546.8628, Test Loss: 7164.2041\n",
      "Epoch 111, Training Loss: 7156.8027, Validation Loss: 6546.2803, Test Loss: 7163.5957\n",
      "Epoch 121, Training Loss: 7156.2061, Validation Loss: 6545.7085, Test Loss: 7163.0015\n",
      "Epoch 131, Training Loss: 7155.6216, Validation Loss: 6545.1445, Test Loss: 7162.4136\n",
      "Epoch 141, Training Loss: 7155.0420, Validation Loss: 6544.5869, Test Loss: 7161.8345\n",
      "Epoch 151, Training Loss: 7154.4707, Validation Loss: 6544.0337, Test Loss: 7161.2622\n",
      "Epoch 161, Training Loss: 7153.8970, Validation Loss: 6543.4839, Test Loss: 7160.6914\n",
      "Epoch 171, Training Loss: 7153.3296, Validation Loss: 6542.9365, Test Loss: 7160.1240\n",
      "Epoch 181, Training Loss: 7152.7632, Validation Loss: 6542.3906, Test Loss: 7159.5581\n",
      "Epoch 191, Training Loss: 7152.1997, Validation Loss: 6541.8447, Test Loss: 7158.9927\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 200\n",
      "Training loss: 7152.1997\n",
      "Validation loss: 6541.8447\n",
      "Test loss: 7158.9927\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 237/243 with parameters:\n",
      "Config indices: (2, 2, 2, 0, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 16, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.1719, Validation Loss: 6555.2959, Test Loss: 7173.0044\n",
      "Epoch 11, Training Loss: 7164.7241, Validation Loss: 6553.9194, Test Loss: 7171.5522\n",
      "Epoch 21, Training Loss: 7163.7612, Validation Loss: 6553.0044, Test Loss: 7170.5830\n",
      "Epoch 31, Training Loss: 7162.8945, Validation Loss: 6552.1865, Test Loss: 7169.7173\n",
      "Epoch 41, Training Loss: 7162.0747, Validation Loss: 6551.4141, Test Loss: 7168.8960\n",
      "Epoch 51, Training Loss: 7161.3037, Validation Loss: 6550.6865, Test Loss: 7168.1211\n",
      "Epoch 61, Training Loss: 7160.5527, Validation Loss: 6549.9824, Test Loss: 7167.3726\n",
      "Epoch 71, Training Loss: 7159.8115, Validation Loss: 6549.2896, Test Loss: 7166.6294\n",
      "Epoch 81, Training Loss: 7159.0649, Validation Loss: 6548.5889, Test Loss: 7165.8838\n",
      "Epoch 91, Training Loss: 7158.2969, Validation Loss: 6547.8691, Test Loss: 7165.1128\n",
      "Epoch 101, Training Loss: 7157.5195, Validation Loss: 6547.1421, Test Loss: 7164.3359\n",
      "Epoch 111, Training Loss: 7156.7466, Validation Loss: 6546.4209, Test Loss: 7163.5625\n",
      "Epoch 121, Training Loss: 7155.9756, Validation Loss: 6545.7021, Test Loss: 7162.7891\n",
      "Epoch 131, Training Loss: 7155.1958, Validation Loss: 6544.9746, Test Loss: 7162.0063\n",
      "Epoch 141, Training Loss: 7154.4043, Validation Loss: 6544.2373, Test Loss: 7161.2173\n",
      "Epoch 151, Training Loss: 7153.6089, Validation Loss: 6543.4956, Test Loss: 7160.4209\n",
      "Epoch 161, Training Loss: 7152.8018, Validation Loss: 6542.7466, Test Loss: 7159.6152\n",
      "Epoch 171, Training Loss: 7151.9888, Validation Loss: 6541.9878, Test Loss: 7158.8008\n",
      "Epoch 181, Training Loss: 7151.1631, Validation Loss: 6541.2202, Test Loss: 7157.9775\n",
      "Epoch 191, Training Loss: 7150.3262, Validation Loss: 6540.4434, Test Loss: 7157.1416\n",
      "Epoch 201, Training Loss: 7149.4790, Validation Loss: 6539.6567, Test Loss: 7156.2954\n",
      "Epoch 211, Training Loss: 7148.6187, Validation Loss: 6538.8594, Test Loss: 7155.4351\n",
      "Epoch 221, Training Loss: 7147.7451, Validation Loss: 6538.0508, Test Loss: 7154.5645\n",
      "Epoch 231, Training Loss: 7146.8604, Validation Loss: 6537.2329, Test Loss: 7153.6812\n",
      "Epoch 241, Training Loss: 7145.9629, Validation Loss: 6536.4033, Test Loss: 7152.7847\n",
      "Epoch 251, Training Loss: 7145.0503, Validation Loss: 6535.5630, Test Loss: 7151.8760\n",
      "Epoch 261, Training Loss: 7144.1265, Validation Loss: 6534.7100, Test Loss: 7150.9546\n",
      "Epoch 271, Training Loss: 7143.1890, Validation Loss: 6533.8452, Test Loss: 7150.0205\n",
      "Epoch 281, Training Loss: 7142.2344, Validation Loss: 6532.9653, Test Loss: 7149.0688\n",
      "Epoch 291, Training Loss: 7141.2612, Validation Loss: 6532.0723, Test Loss: 7148.1011\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 16, Epochs: 300\n",
      "Training loss: 7141.2612\n",
      "Validation loss: 6532.0723\n",
      "Test loss: 7148.1011\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 238/243 with parameters:\n",
      "Config indices: (2, 2, 2, 1, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7166.9106, Validation Loss: 6556.0112, Test Loss: 7173.7354\n",
      "Epoch 11, Training Loss: 7165.4302, Validation Loss: 6554.5898, Test Loss: 7172.2495\n",
      "Epoch 21, Training Loss: 7164.5762, Validation Loss: 6553.7734, Test Loss: 7171.3916\n",
      "Epoch 31, Training Loss: 7163.8667, Validation Loss: 6553.0986, Test Loss: 7170.6821\n",
      "Epoch 41, Training Loss: 7163.2437, Validation Loss: 6552.5068, Test Loss: 7170.0547\n",
      "Epoch 51, Training Loss: 7162.6738, Validation Loss: 6551.9668, Test Loss: 7169.4878\n",
      "Epoch 61, Training Loss: 7162.1431, Validation Loss: 6551.4634, Test Loss: 7168.9531\n",
      "Epoch 71, Training Loss: 7161.6387, Validation Loss: 6550.9844, Test Loss: 7168.4482\n",
      "Epoch 81, Training Loss: 7161.1553, Validation Loss: 6550.5249, Test Loss: 7167.9639\n",
      "Epoch 91, Training Loss: 7160.6865, Validation Loss: 6550.0811, Test Loss: 7167.4937\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 100\n",
      "Training loss: 7160.6865\n",
      "Validation loss: 6550.0811\n",
      "Test loss: 7167.4937\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 239/243 with parameters:\n",
      "Config indices: (2, 2, 2, 1, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7166.6201, Validation Loss: 6555.6382, Test Loss: 7173.4453\n",
      "Epoch 11, Training Loss: 7165.4141, Validation Loss: 6554.4731, Test Loss: 7172.2358\n",
      "Epoch 21, Training Loss: 7164.7305, Validation Loss: 6553.8115, Test Loss: 7171.5474\n",
      "Epoch 31, Training Loss: 7164.1543, Validation Loss: 6553.2549, Test Loss: 7170.9736\n",
      "Epoch 41, Training Loss: 7163.6382, Validation Loss: 6552.7563, Test Loss: 7170.4561\n",
      "Epoch 51, Training Loss: 7163.1592, Validation Loss: 6552.2964, Test Loss: 7169.9795\n",
      "Epoch 61, Training Loss: 7162.7144, Validation Loss: 6551.8682, Test Loss: 7169.5342\n",
      "Epoch 71, Training Loss: 7162.2822, Validation Loss: 6551.4521, Test Loss: 7169.1011\n",
      "Epoch 81, Training Loss: 7161.8618, Validation Loss: 6551.0444, Test Loss: 7168.6797\n",
      "Epoch 91, Training Loss: 7161.4482, Validation Loss: 6550.6470, Test Loss: 7168.2646\n",
      "Epoch 101, Training Loss: 7161.0454, Validation Loss: 6550.2607, Test Loss: 7167.8647\n",
      "Epoch 111, Training Loss: 7160.6528, Validation Loss: 6549.8833, Test Loss: 7167.4717\n",
      "Epoch 121, Training Loss: 7160.2681, Validation Loss: 6549.5132, Test Loss: 7167.0869\n",
      "Epoch 131, Training Loss: 7159.8887, Validation Loss: 6549.1484, Test Loss: 7166.7080\n",
      "Epoch 141, Training Loss: 7159.5132, Validation Loss: 6548.7876, Test Loss: 7166.3320\n",
      "Epoch 151, Training Loss: 7159.1396, Validation Loss: 6548.4292, Test Loss: 7165.9609\n",
      "Epoch 161, Training Loss: 7158.7705, Validation Loss: 6548.0742, Test Loss: 7165.5923\n",
      "Epoch 171, Training Loss: 7158.4033, Validation Loss: 6547.7197, Test Loss: 7165.2246\n",
      "Epoch 181, Training Loss: 7158.0371, Validation Loss: 6547.3667, Test Loss: 7164.8574\n",
      "Epoch 191, Training Loss: 7157.6729, Validation Loss: 6547.0156, Test Loss: 7164.4937\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 200\n",
      "Training loss: 7157.6729\n",
      "Validation loss: 6547.0156\n",
      "Test loss: 7164.4937\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 240/243 with parameters:\n",
      "Config indices: (2, 2, 2, 1, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 32, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.1753, Validation Loss: 6555.3369, Test Loss: 7173.0190\n",
      "Epoch 11, Training Loss: 7164.4722, Validation Loss: 6553.7168, Test Loss: 7171.3149\n",
      "Epoch 21, Training Loss: 7163.4336, Validation Loss: 6552.7324, Test Loss: 7170.2749\n",
      "Epoch 31, Training Loss: 7162.5796, Validation Loss: 6551.9189, Test Loss: 7169.4199\n",
      "Epoch 41, Training Loss: 7161.8076, Validation Loss: 6551.1870, Test Loss: 7168.6465\n",
      "Epoch 51, Training Loss: 7161.0854, Validation Loss: 6550.5020, Test Loss: 7167.9229\n",
      "Epoch 61, Training Loss: 7160.4097, Validation Loss: 6549.8618, Test Loss: 7167.2480\n",
      "Epoch 71, Training Loss: 7159.7710, Validation Loss: 6549.2559, Test Loss: 7166.6074\n",
      "Epoch 81, Training Loss: 7159.1572, Validation Loss: 6548.6729, Test Loss: 7165.9941\n",
      "Epoch 91, Training Loss: 7158.5664, Validation Loss: 6548.1118, Test Loss: 7165.4023\n",
      "Epoch 101, Training Loss: 7157.9888, Validation Loss: 6547.5635, Test Loss: 7164.8237\n",
      "Epoch 111, Training Loss: 7157.4243, Validation Loss: 6547.0283, Test Loss: 7164.2563\n",
      "Epoch 121, Training Loss: 7156.8652, Validation Loss: 6546.4985, Test Loss: 7163.6992\n",
      "Epoch 131, Training Loss: 7156.3140, Validation Loss: 6545.9766, Test Loss: 7163.1479\n",
      "Epoch 141, Training Loss: 7155.7710, Validation Loss: 6545.4590, Test Loss: 7162.6021\n",
      "Epoch 151, Training Loss: 7155.2305, Validation Loss: 6544.9478, Test Loss: 7162.0615\n",
      "Epoch 161, Training Loss: 7154.6938, Validation Loss: 6544.4385, Test Loss: 7161.5249\n",
      "Epoch 171, Training Loss: 7154.1592, Validation Loss: 6543.9307, Test Loss: 7160.9902\n",
      "Epoch 181, Training Loss: 7153.6265, Validation Loss: 6543.4253, Test Loss: 7160.4556\n",
      "Epoch 191, Training Loss: 7153.0928, Validation Loss: 6542.9204, Test Loss: 7159.9229\n",
      "Epoch 201, Training Loss: 7152.5635, Validation Loss: 6542.4160, Test Loss: 7159.3916\n",
      "Epoch 211, Training Loss: 7152.0293, Validation Loss: 6541.9111, Test Loss: 7158.8589\n",
      "Epoch 221, Training Loss: 7151.4985, Validation Loss: 6541.4053, Test Loss: 7158.3257\n",
      "Epoch 231, Training Loss: 7150.9644, Validation Loss: 6540.8989, Test Loss: 7157.7920\n",
      "Epoch 241, Training Loss: 7150.4287, Validation Loss: 6540.3921, Test Loss: 7157.2563\n",
      "Epoch 251, Training Loss: 7149.8931, Validation Loss: 6539.8828, Test Loss: 7156.7183\n",
      "Epoch 261, Training Loss: 7149.3540, Validation Loss: 6539.3721, Test Loss: 7156.1797\n",
      "Epoch 271, Training Loss: 7148.8140, Validation Loss: 6538.8594, Test Loss: 7155.6387\n",
      "Epoch 281, Training Loss: 7148.2729, Validation Loss: 6538.3447, Test Loss: 7155.0962\n",
      "Epoch 291, Training Loss: 7147.7256, Validation Loss: 6537.8291, Test Loss: 7154.5508\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 32, Epochs: 300\n",
      "Training loss: 7147.7256\n",
      "Validation loss: 6537.8291\n",
      "Test loss: 7154.5508\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 241/243 with parameters:\n",
      "Config indices: (2, 2, 2, 2, 0)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 100}\n",
      "Epoch 1, Training Loss: 7167.0869, Validation Loss: 6556.1489, Test Loss: 7173.9102\n",
      "Epoch 11, Training Loss: 7166.1948, Validation Loss: 6555.2915, Test Loss: 7173.0137\n",
      "Epoch 21, Training Loss: 7165.6792, Validation Loss: 6554.8071, Test Loss: 7172.4980\n",
      "Epoch 31, Training Loss: 7165.2388, Validation Loss: 6554.3906, Test Loss: 7172.0562\n",
      "Epoch 41, Training Loss: 7164.8618, Validation Loss: 6554.0317, Test Loss: 7171.6772\n",
      "Epoch 51, Training Loss: 7164.5356, Validation Loss: 6553.7202, Test Loss: 7171.3462\n",
      "Epoch 61, Training Loss: 7164.2363, Validation Loss: 6553.4380, Test Loss: 7171.0474\n",
      "Epoch 71, Training Loss: 7163.9595, Validation Loss: 6553.1768, Test Loss: 7170.7705\n",
      "Epoch 81, Training Loss: 7163.6997, Validation Loss: 6552.9272, Test Loss: 7170.5073\n",
      "Epoch 91, Training Loss: 7163.4473, Validation Loss: 6552.6885, Test Loss: 7170.2549\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 100\n",
      "Training loss: 7163.4473\n",
      "Validation loss: 6552.6885\n",
      "Test loss: 7170.2549\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 242/243 with parameters:\n",
      "Config indices: (2, 2, 2, 2, 1)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 200}\n",
      "Epoch 1, Training Loss: 7167.7192, Validation Loss: 6556.7881, Test Loss: 7174.5171\n",
      "Epoch 11, Training Loss: 7166.7412, Validation Loss: 6555.8433, Test Loss: 7173.5356\n",
      "Epoch 21, Training Loss: 7166.2012, Validation Loss: 6555.3257, Test Loss: 7172.9951\n",
      "Epoch 31, Training Loss: 7165.7856, Validation Loss: 6554.9277, Test Loss: 7172.5830\n",
      "Epoch 41, Training Loss: 7165.4351, Validation Loss: 6554.5923, Test Loss: 7172.2324\n",
      "Epoch 51, Training Loss: 7165.1274, Validation Loss: 6554.2993, Test Loss: 7171.9258\n",
      "Epoch 61, Training Loss: 7164.8496, Validation Loss: 6554.0352, Test Loss: 7171.6494\n",
      "Epoch 71, Training Loss: 7164.5972, Validation Loss: 6553.7915, Test Loss: 7171.3936\n",
      "Epoch 81, Training Loss: 7164.3589, Validation Loss: 6553.5664, Test Loss: 7171.1567\n",
      "Epoch 91, Training Loss: 7164.1348, Validation Loss: 6553.3525, Test Loss: 7170.9321\n",
      "Epoch 101, Training Loss: 7163.9204, Validation Loss: 6553.1504, Test Loss: 7170.7197\n",
      "Epoch 111, Training Loss: 7163.7144, Validation Loss: 6552.9561, Test Loss: 7170.5161\n",
      "Epoch 121, Training Loss: 7163.5195, Validation Loss: 6552.7710, Test Loss: 7170.3203\n",
      "Epoch 131, Training Loss: 7163.3296, Validation Loss: 6552.5918, Test Loss: 7170.1304\n",
      "Epoch 141, Training Loss: 7163.1460, Validation Loss: 6552.4180, Test Loss: 7169.9463\n",
      "Epoch 151, Training Loss: 7162.9692, Validation Loss: 6552.2500, Test Loss: 7169.7686\n",
      "Epoch 161, Training Loss: 7162.7939, Validation Loss: 6552.0850, Test Loss: 7169.5947\n",
      "Epoch 171, Training Loss: 7162.6201, Validation Loss: 6551.9224, Test Loss: 7169.4233\n",
      "Epoch 181, Training Loss: 7162.4531, Validation Loss: 6551.7632, Test Loss: 7169.2559\n",
      "Epoch 191, Training Loss: 7162.2871, Validation Loss: 6551.6074, Test Loss: 7169.0894\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 200\n",
      "Training loss: 7162.2871\n",
      "Validation loss: 6551.6074\n",
      "Test loss: 7169.0894\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Training model 243/243 with parameters:\n",
      "Config indices: (2, 2, 2, 2, 2)\n",
      "{'hidden_sizes': [8, 16, 32], 'optimizer': <class 'tensorflow.python.training.adagrad.AdagradOptimizer'>, 'learning_rate': 0.0001, 'batch_size': 64, 'epoch': 300}\n",
      "Epoch 1, Training Loss: 7166.4414, Validation Loss: 6555.5186, Test Loss: 7173.2607\n",
      "Epoch 11, Training Loss: 7165.4868, Validation Loss: 6554.6016, Test Loss: 7172.3071\n",
      "Epoch 21, Training Loss: 7164.9497, Validation Loss: 6554.0835, Test Loss: 7171.7671\n",
      "Epoch 31, Training Loss: 7164.5161, Validation Loss: 6553.6709, Test Loss: 7171.3345\n",
      "Epoch 41, Training Loss: 7164.1411, Validation Loss: 6553.3115, Test Loss: 7170.9575\n",
      "Epoch 51, Training Loss: 7163.7964, Validation Loss: 6552.9844, Test Loss: 7170.6113\n",
      "Epoch 61, Training Loss: 7163.4766, Validation Loss: 6552.6792, Test Loss: 7170.2905\n",
      "Epoch 71, Training Loss: 7163.1729, Validation Loss: 6552.3882, Test Loss: 7169.9839\n",
      "Epoch 81, Training Loss: 7162.8750, Validation Loss: 6552.1074, Test Loss: 7169.6870\n",
      "Epoch 91, Training Loss: 7162.5762, Validation Loss: 6551.8286, Test Loss: 7169.3916\n",
      "Epoch 101, Training Loss: 7162.2856, Validation Loss: 6551.5518, Test Loss: 7169.0977\n",
      "Epoch 111, Training Loss: 7162.0000, Validation Loss: 6551.2817, Test Loss: 7168.8145\n",
      "Epoch 121, Training Loss: 7161.7217, Validation Loss: 6551.0190, Test Loss: 7168.5356\n",
      "Epoch 131, Training Loss: 7161.4482, Validation Loss: 6550.7598, Test Loss: 7168.2627\n",
      "Epoch 141, Training Loss: 7161.1792, Validation Loss: 6550.5063, Test Loss: 7167.9937\n",
      "Epoch 151, Training Loss: 7160.9141, Validation Loss: 6550.2568, Test Loss: 7167.7295\n",
      "Epoch 161, Training Loss: 7160.6509, Validation Loss: 6550.0093, Test Loss: 7167.4673\n",
      "Epoch 171, Training Loss: 7160.3911, Validation Loss: 6549.7646, Test Loss: 7167.2075\n",
      "Epoch 181, Training Loss: 7160.1348, Validation Loss: 6549.5225, Test Loss: 7166.9517\n",
      "Epoch 191, Training Loss: 7159.8809, Validation Loss: 6549.2812, Test Loss: 7166.6963\n",
      "Epoch 201, Training Loss: 7159.6265, Validation Loss: 6549.0425, Test Loss: 7166.4438\n",
      "Epoch 211, Training Loss: 7159.3750, Validation Loss: 6548.8057, Test Loss: 7166.1934\n",
      "Epoch 221, Training Loss: 7159.1265, Validation Loss: 6548.5708, Test Loss: 7165.9434\n",
      "Epoch 231, Training Loss: 7158.8765, Validation Loss: 6548.3369, Test Loss: 7165.6948\n",
      "Epoch 241, Training Loss: 7158.6289, Validation Loss: 6548.1045, Test Loss: 7165.4478\n",
      "Epoch 251, Training Loss: 7158.3838, Validation Loss: 6547.8735, Test Loss: 7165.2041\n",
      "Epoch 261, Training Loss: 7158.1411, Validation Loss: 6547.6440, Test Loss: 7164.9595\n",
      "Epoch 271, Training Loss: 7157.8979, Validation Loss: 6547.4160, Test Loss: 7164.7183\n",
      "Epoch 281, Training Loss: 7157.6567, Validation Loss: 6547.1880, Test Loss: 7164.4766\n",
      "Epoch 291, Training Loss: 7157.4141, Validation Loss: 6546.9600, Test Loss: 7164.2354\n",
      "Optimizer: AdagradOptimizer, Learning rate: 0.0001\n",
      "Hidden sizes: [8, 16, 32], Batch size: 64, Epochs: 300\n",
      "Training loss: 7157.4141\n",
      "Validation loss: 6546.9600\n",
      "Test loss: 7164.2354\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1800 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Conclusion\n",
      "[Best Parameters]\n",
      "Optimizer: GradientDescentOptimizer\n",
      "Learning rate: 0.01\n",
      "Hidden sizes: [32, 16, 8]\n",
      "Batch size: 16\n",
      "Epochs: 100\n",
      "Training loss: nan\n",
      "Validation loss: nan\n",
      "Test loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of Actual Values vs Predicted Values:\n",
      "Actual: 74.4171, Predicted: nan\n",
      "Actual: 45.4671, Predicted: nan\n",
      "Actual: 54.7971, Predicted: nan\n",
      "Actual: 94.6771, Predicted: nan\n",
      "Actual: 111.7900, Predicted: nan\n",
      "Actual: 72.2986, Predicted: nan\n",
      "Actual: 90.2800, Predicted: nan\n",
      "Actual: 49.6571, Predicted: nan\n",
      "Actual: 75.7771, Predicted: nan\n",
      "Actual: 98.6300, Predicted: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########################\n",
    "# Data\n",
    "\n",
    "def load_csv(file_path):\n",
    "    # 첫 번째 행(헤더) 읽기\n",
    "    with open(file_path, 'r') as f:\n",
    "        header = f.readline().strip().split(',')\n",
    "\n",
    "    # 데이터 로드 (첫 번째 행 제외, 'open', 'close', 'low', 'high', 'volume' 열만 사용)\n",
    "    # 열 인덱스: 0=date, 1=symbol, 2=open, 3=close, 4=low, 5=high, 6=volume\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=1, usecols=(2, 3, 4, 5, 6))\n",
    "\n",
    "    # 입력 변수: open, low, high, volume\n",
    "    x_data = data[:, [0, 2, 3, 4]]  # 열 인덱스 0=open, 2=low, 3=high, 4=volume\n",
    "    # 타겟 변수: close\n",
    "    y_data = data[:, 1].reshape(-1, 1)  # 열 인덱스 1=close\n",
    "\n",
    "    # 특성 이름\n",
    "    feature_names = ['open', 'low', 'high', 'volume']\n",
    "\n",
    "    return x_data, y_data, feature_names\n",
    "\n",
    "def split_data(x_data, y_data):\n",
    "    # 데이터셋을 학습, 검증, 테스트로 나누기\n",
    "    # 전체 데이터의 20%를 테스트 데이터로 사용\n",
    "    x_temp, x_test, y_temp, y_test = train_test_split(\n",
    "        x_data, y_data, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 테스트 데이터를 제외한 나머지 데이터의 12.5%를 검증 데이터로 사용\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_temp, y_temp, test_size=0.125, random_state=42)\n",
    "    \n",
    "    # 표준화 (스케일링)\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_val = sc.transform(x_val)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "########################\n",
    "# Data Analysis\n",
    "\n",
    "def draw_data_distributions(data, feature_names, title, num_print_columns=3):\n",
    "    print(f'1. Data Analysis')  # 대제목 출력\n",
    "    num_columns = data.shape[1]\n",
    "    num_rows = (num_columns + num_print_columns - 1) // num_print_columns\n",
    "\n",
    "    fig, figures = plt.subplots(num_rows + 1, num_print_columns, figsize=(15, 5 * (num_rows + 1)), \n",
    "                                gridspec_kw={'height_ratios': [0.3] + [2] * num_rows})\n",
    "\n",
    "    # 소제목을 첫 번째 행에 왼쪽 정렬로 추가\n",
    "    figures[0, 0].text(0, 0.5, f'{title}', fontsize=16, ha='left', va='center')\n",
    "    for j in range(num_print_columns):\n",
    "        figures[0, j].axis('off')\n",
    "\n",
    "    # 각 행별로 최대 y값을 계산하여 동일한 y축 범위 설정\n",
    "    for row in range(1, num_rows + 1):\n",
    "        # 해당 행의 모든 그래프에 대한 최대 y값 찾기\n",
    "        max_y = 0\n",
    "        for col in range(num_print_columns):\n",
    "            i = (row - 1) * num_print_columns + col\n",
    "            if i < num_columns:\n",
    "                counts, _ = np.histogram(data[:, i], bins=30)\n",
    "                current_max = counts.max()\n",
    "                if current_max > max_y:\n",
    "                    max_y = current_max\n",
    "        \n",
    "        # 각 그래프에 히스토그램 그리기 및 y축 범위 설정\n",
    "        for col in range(num_print_columns):\n",
    "            i = (row - 1) * num_print_columns + col\n",
    "            if i < num_columns:\n",
    "                figure = figures[row, col]\n",
    "                figure.hist(data[:, i], bins=30, color='skyblue', edgecolor='black')\n",
    "                figure.set_title(f'{feature_names[i]}')\n",
    "                figure.set_xlabel(f'{feature_names[i]}')\n",
    "                figure.set_ylabel('Frequency')\n",
    "                figure.set_ylim(0, max_y + max_y * 0.1)  # 최대 y값의 110%로 설정\n",
    "\n",
    "    # 남은 빈 플롯 비활성화\n",
    "    for i in range(num_columns, num_rows * num_print_columns):\n",
    "        row = (i // num_print_columns) + 1\n",
    "        col = i % num_print_columns\n",
    "        figures[row, col].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def draw_correlation_matrix(x, feature_names, title=None):\n",
    "    # 상관계수 행렬 계산\n",
    "    corr_matrix = np.corrcoef(x, rowvar=False)\n",
    "    # 특성 수\n",
    "    num_features = x.shape[1]\n",
    "    # 레이블 설정\n",
    "    feature_labels = feature_names\n",
    "    \n",
    "    # 히트맵 시각화\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    im = plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest', vmin=-1, vmax=1)\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # 축 레이블 설정\n",
    "    plt.xticks(range(num_features), feature_labels, rotation=90)\n",
    "    plt.yticks(range(num_features), feature_labels)\n",
    "    \n",
    "    # 각 셀에 상관계수 표시\n",
    "    for i in range(num_features):\n",
    "        for j in range(num_features):\n",
    "            text = f\"{corr_matrix[i, j]:.2f}\"\n",
    "            plt.text(j, i, text,\n",
    "                     ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    # 제목 설정\n",
    "    if title:\n",
    "        plt.title(title, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "########################\n",
    "# Learning\n",
    "\n",
    "def create_graph(optimizer, input_num, hidden_sizes):\n",
    "    tf.reset_default_graph()  # 그래프 초기화\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, input_num], name='x')  # 입력 데이터 플레이스홀더\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='y')  # 레이블 데이터 플레이스홀더 (회귀 문제)\n",
    "\n",
    "    # 은닉층 생성\n",
    "    layer_input = x\n",
    "    layer_input_size = input_num\n",
    "\n",
    "    for idx, hidden_size in enumerate(hidden_sizes):\n",
    "        W_hidden = tf.Variable(tf.random_normal([layer_input_size, hidden_size], stddev=0.1),\n",
    "                               name=f'W_hidden_{idx+1}')\n",
    "        b_hidden = tf.Variable(tf.zeros([hidden_size]), name=f'b_hidden_{idx+1}')\n",
    "        layer_output = tf.nn.relu(tf.matmul(layer_input, W_hidden) + b_hidden, name=f'relu_{idx+1}')\n",
    "\n",
    "        # 다음 층을 위한 입력 업데이트\n",
    "        layer_input = layer_output\n",
    "        layer_input_size = hidden_size\n",
    "\n",
    "    # 출력층\n",
    "    W_output = tf.Variable(tf.random_normal([layer_input_size, 1], stddev=0.1), name='W_output')\n",
    "    b_output = tf.Variable(tf.zeros([1]), name='b_output')\n",
    "    prediction = tf.matmul(layer_input, W_output) + b_output  # 예측값 계산\n",
    "    loss = tf.reduce_mean(tf.square(y - prediction))  # MSE 손실 계산\n",
    "    train_step = optimizer.minimize(loss)  # 최적화 수행\n",
    "\n",
    "    return x, y, prediction, loss, train_step\n",
    "\n",
    "def execute_graph(x, y, prediction, loss, train_step, data, epoch, batch_size):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = data\n",
    "   \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())  # 변수 초기화\n",
    "        for e in range(1, epoch + 1):\n",
    "            # 학습 데이터 셔플\n",
    "            permutation = np.random.permutation(len(x_train))\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            num_batches = (len(x_train) + batch_size - 1) // batch_size\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i+1) * batch_size, len(x_train))\n",
    "                batch_x = x_train_shuffled[start_idx:end_idx]\n",
    "                batch_y = y_train_shuffled[start_idx:end_idx]\n",
    "                sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            if e % 10 == 1:\n",
    "                # 손실 계산\n",
    "                train_loss = sess.run(loss, feed_dict={x: x_train, y: y_train})\n",
    "                val_loss = sess.run(loss, feed_dict={x: x_val, y: y_val})\n",
    "                test_loss = sess.run(loss, feed_dict={x: x_test, y: y_test})\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                print(f'Epoch {e}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "       \n",
    "        # 최종 예측 수행\n",
    "        predictions = sess.run(prediction, feed_dict={x: x_test})\n",
    "\n",
    "    return train_losses, val_losses, test_losses, predictions\n",
    "\n",
    "########################\n",
    "# Visualization Functions\n",
    "\n",
    "def draw_loss(figure, train_losses, val_losses, test_losses, title=None, y_lim=None):\n",
    "    if len(train_losses) > 0:\n",
    "        epochs = range(1, len(train_losses) + 1)\n",
    "        figure.plot(epochs, train_losses, label='Training Loss', color='blue')\n",
    "        figure.plot(epochs, val_losses, label='Validation Loss', color='orange')\n",
    "        figure.plot(epochs, test_losses, label='Test Loss', color='green', linestyle='--')\n",
    "        figure.set_xlabel('Epochs')\n",
    "        figure.set_ylabel('Mean Squared Error (MSE)')\n",
    "        if title is not None:\n",
    "            figure.set_title(f'{title}')\n",
    "        figure.legend()\n",
    "        figure.grid(True)\n",
    "        if y_lim:\n",
    "            figure.set_ylim(y_lim)\n",
    "\n",
    "def draw_predictions_scatter(y_true, y_pred, title=None):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6, color='purple', edgecolor='k')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    if title is not None:\n",
    "        plt.title(f'{title}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "########################\n",
    "# Helper Function for Formatting Titles\n",
    "\n",
    "def format_param_title(param_name, param_value):\n",
    "    if param_name == 'optimizer':\n",
    "        return f\"{param_name.replace('_', ' ').title()}: {param_value.__name__}\"\n",
    "    elif param_name == 'hidden_sizes':\n",
    "        return f\"{param_name.replace('_', ' ').title()}: {param_value}\"\n",
    "    else:\n",
    "        return f\"{param_name.replace('_', ' ').title()}: {param_value}\"\n",
    "\n",
    "########################\n",
    "# Main Learning Function\n",
    "\n",
    "# 모델 학습 및 결과 도출\n",
    "def learn(data, param_grid):\n",
    "    x_train = data[0]\n",
    "    best_report = None\n",
    "\n",
    "    # 모든 가능한 하이퍼파라미터 조합 생성\n",
    "    param_keys = list(param_grid.keys())\n",
    "    param_values = [param_grid[key] for key in param_keys]\n",
    "    all_configurations = list(itertools.product(*param_values))\n",
    "    print('2. Training Models')\n",
    "    print(f\"Total configurations: {len(all_configurations)}\")\n",
    "\n",
    "    # 하이퍼파라미터 중 가장 많은 옵션을 가진 것을 기준으로 컬럼 수 결정\n",
    "    num_columns = max(len(options) for options in param_grid.values())\n",
    "    num_rows = len(param_keys)\n",
    "    \n",
    "    # 시각화를 위한 구성 인덱스 수집\n",
    "    plot_config_indices = {}\n",
    "    for param_idx, key in enumerate(param_keys):\n",
    "        options_length = len(param_grid[key])\n",
    "        configs = []\n",
    "        for idx in range(options_length):\n",
    "            config_indices = [0] * len(param_keys)\n",
    "            config_indices[param_idx] = idx\n",
    "            configs.append(tuple(config_indices))\n",
    "        plot_config_indices[param_idx] = configs\n",
    "\n",
    "    # 모든 구성의 결과 저장\n",
    "    results = {}\n",
    "    for idx, config_values in enumerate(all_configurations):\n",
    "        params = dict(zip(param_keys, config_values))\n",
    "        config_indices = tuple(param_grid[key].index(value) for key, value in params.items())\n",
    "        print(f\"Training model {idx+1}/{len(all_configurations)} with parameters:\")\n",
    "        print(f\"Config indices: {config_indices}\")\n",
    "        print(params)\n",
    "\n",
    "        optimizer_class = params['optimizer']\n",
    "        learning_rate = params['learning_rate']\n",
    "        hidden_sizes = params['hidden_sizes']\n",
    "        epoch = params['epoch']\n",
    "        batch_size = params['batch_size']\n",
    "\n",
    "        optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "        \n",
    "        # 그래프 생성\n",
    "        x, y, prediction, loss, train_step = create_graph(\n",
    "            optimizer, x_train.shape[1], hidden_sizes)\n",
    "                \n",
    "        # 그래프 실행\n",
    "        train_losses, val_losses, test_losses, predictions = execute_graph(\n",
    "            x, y, prediction, loss, train_step, data, epoch, batch_size)\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"Optimizer: {optimizer_class.__name__}, Learning rate: {learning_rate}\")\n",
    "        print(f\"Hidden sizes: {hidden_sizes}, Batch size: {batch_size}, Epochs: {params['epoch']}\")\n",
    "        if len(train_losses) > 0:\n",
    "            print(f\"Training loss: {train_losses[-1]:.4f}\")\n",
    "            print(f\"Validation loss: {val_losses[-1]:.4f}\")\n",
    "            print(f\"Test loss: {test_losses[-1]:.4f}\\n\")\n",
    "        else:\n",
    "            print(f\"No loss recorded. Check the epoch increments.\\n\")\n",
    "\n",
    "        # 최고 성능 모델 업데이트\n",
    "        if best_report is None or (len(test_losses) > 0 and test_losses[-1] < best_report['test_losses'][-1]):\n",
    "            best_report = {\n",
    "                \"params\": params,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"val_losses\": val_losses,\n",
    "                \"test_losses\": test_losses,\n",
    "                \"predictions\": predictions,\n",
    "                \"y_test\": data[-1]\n",
    "            }\n",
    "            \n",
    "        # 모든 구성의 결과 저장\n",
    "        results[config_indices] = {\n",
    "            \"params\": params,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"test_losses\": test_losses,\n",
    "        }\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "    # 지정된 구성을 시각화\n",
    "    # 서브플롯 준비\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(5 * num_columns, 5 * num_rows))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # 서브플롯이 2D 배열이 되도록 조정 (단일 행 또는 열인 경우)\n",
    "    if num_rows == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    if num_columns == 1:\n",
    "        axes = np.expand_dims(axes, axis=1)\n",
    "\n",
    "    for param_idx, configs in plot_config_indices.items():\n",
    "        # 각 행의 최대 손실 값 찾기\n",
    "        max_loss = 0\n",
    "        for config_indices in configs:\n",
    "            result = results.get(config_indices)\n",
    "            if result:\n",
    "                current_max = max(result['train_losses'] + result['val_losses'] + result['test_losses'])\n",
    "                if current_max > max_loss:\n",
    "                    max_loss = current_max\n",
    "        # y_lim 설정 (최대 손실 값의 110%)\n",
    "        y_lim = (0, max_loss * 1.1) if max_loss > 0 else None\n",
    "\n",
    "        for col_index, config_indices in enumerate(configs):\n",
    "            row_index = param_idx\n",
    "            ax = axes[row_index, col_index]\n",
    "\n",
    "            result = results.get(config_indices)\n",
    "            if result:\n",
    "                param_name = param_keys[param_idx]\n",
    "                param_value = param_grid[param_name][config_indices[param_idx]]\n",
    "                formatted_title = format_param_title(param_name, param_value)\n",
    "                draw_loss(\n",
    "                    ax,\n",
    "                    result['train_losses'],\n",
    "                    result['val_losses'],\n",
    "                    result['test_losses'],\n",
    "                    title=formatted_title,\n",
    "                    y_lim=y_lim\n",
    "                )\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return best_report\n",
    "\n",
    "########################\n",
    "# Report\n",
    "\n",
    "def generate_report(report):\n",
    "    if report is None:\n",
    "        print(\"No report to generate.\")\n",
    "        return\n",
    "\n",
    "    params = report['params']\n",
    "    \n",
    "    print('3. Conclusion')\n",
    "    print(f\"[Best Parameters]\")\n",
    "    print(f\"Optimizer: {params['optimizer'].__name__}\")\n",
    "    print(f\"Learning rate: {params['learning_rate']}\")\n",
    "    print(f\"Hidden sizes: {params['hidden_sizes']}\")\n",
    "    print(f\"Batch size: {params['batch_size']}\")\n",
    "    print(f\"Epochs: {params['epoch']}\")\n",
    "    if len(report['train_losses']) > 0:\n",
    "        print(f\"Training loss: {report['train_losses'][-1]:.4f}\")\n",
    "        print(f\"Validation loss: {report['val_losses'][-1]:.4f}\")\n",
    "        print(f\"Test loss: {report['test_losses'][-1]:.4f}\")\n",
    "\n",
    "    # 손실 시각화\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    draw_loss(ax, report['train_losses'], report['val_losses'], report['test_losses'],\n",
    "              title=f\"Best Model Loss: {params['optimizer'].__name__} (LR={params['learning_rate']})\")\n",
    "    plt.show()\n",
    "\n",
    "    # 예측 vs 실제 값 시각화\n",
    "    y_test = report['y_test']\n",
    "    predictions = report['predictions']\n",
    "    draw_predictions_scatter(y_test.flatten(), predictions.flatten(),\n",
    "                             title=f\"Best Model Predictions vs Actual: {params['optimizer'].__name__} (LR={params['learning_rate']})\")\n",
    "\n",
    "    # 실제 값과 예측 값 출력\n",
    "    print(\"\\nSample of Actual Values vs Predicted Values:\")\n",
    "    for actual, predicted in zip(y_test.flatten()[:10], predictions.flatten()[:10]):\n",
    "        print(f\"Actual: {actual:.4f}, Predicted: {predicted:.4f}\")\n",
    "\n",
    "########################\n",
    "# 파라미터 그리드 설정\n",
    "\n",
    "# 하이퍼파라미터 그리드 설정\n",
    "param_grid = {    \n",
    "    'hidden_sizes': [[32, 16, 8], [16, 16, 16], [8, 16, 32]],\n",
    "    'optimizer': [tf.train.GradientDescentOptimizer, tf.train.AdamOptimizer, tf.train.AdagradOptimizer],\n",
    "    'learning_rate': [0.0001, 0.0001, 0.00001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epoch': [100, 300, 500]\n",
    "}\n",
    "\n",
    "########################\n",
    "# 메인 실행 흐름\n",
    "\n",
    "def main():\n",
    "    # 데이터 로드 및 전처리\n",
    "    x_data, y_data, feature_names = load_csv('aapl_prices.csv')\n",
    "    print(f\"x_data shape: {x_data.shape}\")\n",
    "    print(f\"y_data shape: {y_data.shape}\")\n",
    "    print(f\"Feature names: {feature_names}\")\n",
    "    \n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = split_data(x_data, y_data)\n",
    "    data = (x_train, x_val, x_test, y_train, y_val, y_test)\n",
    "\n",
    "    # 1. Data Analysis: 데이터셋의 열 분포도 시각화\n",
    "    draw_data_distributions(x_train, feature_names, title='Training Data Distributions', num_print_columns=3)\n",
    "    \n",
    "    # 2. Data Correlation Analysis: 데이터 상관관계 시각화 (타겟 변수 제외)\n",
    "    draw_correlation_matrix(x_train, feature_names, title='Training Data Correlation Matrix')\n",
    "    \n",
    "    # Learning: 모델 학습 및 하이퍼파라미터 그리드 서치\n",
    "    best_report = learn(data, param_grid)\n",
    "\n",
    "    # Report: 최적 모델의 성능 보고\n",
    "    generate_report(best_report)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
