{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6148ce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Training Model\n",
      "Total parameter combinations: 243\n",
      "Training model 1/243 with parameters:\n",
      "Hidden sizes: [32, 16, 8], Optimizer: \tGradientDescentOptimizer, Learning rate: \t0.01, Batch size: 16, \tEpochs: 100\n",
      "Epoch 1, Training Loss: 0.03020770661532879, Validation Loss: 0.02284235693514347, Test Loss: 0.02800462394952774\n",
      "Epoch 11, Training Loss: 0.007498438935726881, Validation Loss: 0.007506952155381441, Test Loss: 0.011081351898610592\n",
      "Epoch 21, Training Loss: 0.0033962393645197153, Validation Loss: 0.0036478086840361357, Test Loss: 0.004600758198648691\n",
      "Epoch 31, Training Loss: 0.005837236996740103, Validation Loss: 0.005494697950780392, Test Loss: 0.006373272277414799\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-02ff75f15b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-02ff75f15b25>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mbest_report\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# Report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-02ff75f15b25>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(data, param_grid)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# 그래프 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         train_losses, val_losses, test_losses, predictions = execute_graph(\n\u001b[0;32m--> 148\u001b[0;31m             x, y, prediction, loss, train_step, data, epoch, batch_size)\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# 결과 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-02ff75f15b25>\u001b[0m in \u001b[0;36mexecute_graph\u001b[0;34m(x, y, prediction, loss, train_step, data, epoch, batch_size)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mbatch_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/ml/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "\n",
    "########################\n",
    "\n",
    "\n",
    "# Data\n",
    "\n",
    "def load_csv(file_path):\n",
    "    # CSV 파일 로드 (첫 번째 행을 건너뜀, 앞에서부터 8개의 열만 불러옴)\n",
    "    data = np.genfromtxt(file_path, delimiter=',', skip_header=1, usecols=range(8))\n",
    "\n",
    "    # 'HAEMOGLOBINS'를 목표 변수로 설정 (두 번째 열)\n",
    "    x_data = np.delete(data, 1, axis=1)  # 두 번째 열 제거 (입력 변수)\n",
    "    y_data = data[:, 1].reshape(-1, 1)  # 두 번째 열이 목표 변수\n",
    "    \n",
    "    return x_data, y_data\n",
    "\n",
    "def split_data(x_data, y_data):\n",
    "    # 데이터셋을 학습, 검증, 테스트로 나누기\n",
    "    # random_state는 Seed값으로 동일한 분할 결과를 얻기 위해 사용\n",
    "    # 전체 데이터의 20%를 테스트 데이터로 사용\n",
    "    x_temp, x_test, y_temp, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)\n",
    "    # 테스트 데이터를 제외한 나머지 데이터의 12.5%를 검증 데이터로 사용, \n",
    "    # 나머지를 학습 데이터로 사용\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.125, random_state=42) # stratify=y_data가 없음\n",
    "\n",
    "    # 표준화 (스케일링)\n",
    "    sc = StandardScaler()\n",
    "    # 학습 데이터에 fit_transform을 사용하여 평균과 표준편차를 계산하고 데이터를 표준화\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    # 검증 및 테스트 데이터에는 학습 데이터의 평균과 표준편차를 사용하여 동일하게 표준화\n",
    "    x_val = sc.transform(x_val)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "\n",
    "########################\n",
    "# Learning\n",
    "\n",
    "def create_graph(optimizer, input_num, hidden_sizes):\n",
    "    tf.reset_default_graph()  # 그래프 초기화\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [None, input_num], name='x')  # 입력 데이터 플레이스홀더\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='y')  # 레이블 데이터 플레이스홀더 (회귀 문제)\n",
    "\n",
    "    # 히든 레이어 생성\n",
    "    layer_input = x\n",
    "    layer_input_size = input_num\n",
    "    for idx, hidden_size in enumerate(hidden_sizes):\n",
    "        # 가중치 및 바이어스 초기화\n",
    "        # tf.random.normal - 정규 분포를 따르는 랜덤 값 생성, 가중치 초기화용\n",
    "        W_hidden = tf.Variable(tf.random.normal([layer_input_size, hidden_size], stddev=0.1), name=f'W_hidden_{idx+1}')\n",
    "        b_hidden = tf.Variable(tf.zeros([hidden_size]), name=f'b_hidden_{idx+1}')\n",
    "        # 레이어 출력 계산\n",
    "        layer_output = tf.nn.relu(tf.matmul(layer_input, W_hidden) + b_hidden, name=f'relu_{idx+1}')\n",
    "\n",
    "       \n",
    "        # 다음 레이어를 위해 업데이트\n",
    "        layer_input = layer_output\n",
    "        layer_input_size = hidden_size\n",
    "\n",
    "    # 출력 레이어 (1개의 출력값)\n",
    "    W_output = tf.Variable(tf.random.normal([layer_input_size, 1], stddev=0.1), name='W_output')\n",
    "    b_output = tf.Variable(tf.zeros([1]), name='b_output')\n",
    "\n",
    "    prediction = tf.matmul(layer_input, W_output) + b_output  # 예측값 계산\n",
    "    loss = tf.reduce_mean(tf.square(y - prediction))  # MSE 손실 계산\n",
    "    train_step = optimizer.minimize(loss)  # 최적화 수행\n",
    "\n",
    "    return x, y, prediction, loss, train_step\n",
    "\n",
    "def execute_graph(x, y, prediction, loss, train_step, data, epoch, batch_size):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = data\n",
    "   \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())  # 변수 초기화\n",
    "        for e in range(epoch):\n",
    "            # 학습 데이터 셔플\n",
    "            permutation = np.random.permutation(len(x_train))\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            num_batches = (len(x_train) + batch_size - 1) // batch_size\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = min((i+1) * batch_size, len(x_train))\n",
    "                batch_x = x_train_shuffled[start_idx:end_idx]\n",
    "                batch_y = y_train_shuffled[start_idx:end_idx]\n",
    "                sess.run(train_step, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "            if e % 10 == 1:\n",
    "                # 손실 계산\n",
    "                train_loss = sess.run(loss, feed_dict={x: x_train, y: y_train})\n",
    "                val_loss = sess.run(loss, feed_dict={x: x_val, y: y_val})\n",
    "                test_loss = sess.run(loss, feed_dict={x: x_test, y: y_test})\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                test_losses.append(test_loss)\n",
    "                print(f'Epoch {e}, Training Loss: {train_loss}, Validation Loss: {val_loss}, Test Loss: {test_loss}')\n",
    "       \n",
    "        # 최종 예측 수행\n",
    "        predictions = sess.run(prediction, feed_dict={x: x_test})\n",
    "\n",
    "    return train_losses, val_losses, test_losses, predictions\n",
    "\n",
    "def learn(data, param_grid):\n",
    "\n",
    "    x_train = data[0]\n",
    "    best_report = None\n",
    "\n",
    "    # 하이퍼파라미터 조합 생성\n",
    "    keys = param_grid.keys()\n",
    "    combinations = list(itertools.product(*(param_grid[key] for key in keys)))\n",
    "\n",
    "    print('1. Training Model')\n",
    "    print(f\"Total parameter combinations: {len(combinations)}\")\n",
    "\n",
    "    for idx, combo in enumerate(combinations, 1):\n",
    "        params = dict(zip(keys, combo))\n",
    "        print(f'Training model {idx}/{len(combinations)} with parameters:')\n",
    "        print(f\"Hidden sizes: {params['hidden_sizes']}, Optimizer: \t{params['optimizer'].__name__}, Learning rate: \t{params['learning_rate']}, Batch size: {params['batch_size']}, \tEpochs: {params['epoch']}\")\n",
    "\n",
    "        optimizer_class = params['optimizer']\n",
    "        learning_rate = params['learning_rate']\n",
    "        hidden_sizes = params['hidden_sizes']\n",
    "        epoch = params['epoch']\n",
    "        batch_size = params['batch_size']\n",
    "\n",
    "                            \n",
    "        optimizer = optimizer_class(learning_rate=learning_rate)\n",
    "\n",
    "        # 그래프 생성\n",
    "        x, y, prediction, loss, train_step = create_graph(\n",
    "            optimizer, x_train.shape[1], hidden_sizes)\n",
    "                \n",
    "        # 그래프 실행\n",
    "        train_losses, val_losses, test_losses, predictions = execute_graph(\n",
    "            x, y, prediction, loss, train_step, data, epoch, batch_size)\n",
    "\n",
    "        # 결과 출력\n",
    "        print(f\"Optimizer: {optimizer_class.__name__}, Learning rate: {learning_rate}\")\n",
    "        print(f\"Hidden sizes: {hidden_sizes}, Batch size: {batch_size}, Epochs: {params['epoch']}\")\n",
    "        print(f\"Training loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"Validation loss: {val_losses[-1]:.4f}\")\n",
    "        print(f\"Test loss: {test_losses[-1]:.4f}\\n\")\n",
    "\n",
    "        # 최적의 모델 업데이트 (여기서는 테스트 손실이 가장 작은 모델 선택)\n",
    "        if best_report is None or test_losses[-1] < best_report['test_losses'][-1]:\n",
    "            best_report = {\n",
    "                \"params\": params,\n",
    "                \"train_losses\": train_losses,\n",
    "                \"val_losses\": val_losses,\n",
    "                \"test_losses\": test_losses,\n",
    "                \"predictions\": predictions,\n",
    "                \"y_test\": data[-1]\n",
    "            }\n",
    "\n",
    "    return best_report\n",
    "\n",
    "########################\n",
    "# Report\n",
    "\n",
    "def generate_report(report):\n",
    "    if report is None:\n",
    "        print(\"No report to generate.\")\n",
    "        return\n",
    "\n",
    "    params = report['params']\n",
    "    print(f'2. Conclusion')  # 대제목 출력\n",
    "    print(f\"Best Parameters:\")\n",
    "    print(f\"Optimizer: {params['optimizer'].__name__}\")\n",
    "    print(f\"Learning rate: {params['learning_rate']}\")\n",
    "    print(f\"Hidden sizes: {params['hidden_sizes']}\")\n",
    "    print(f\"Batch size: {params['batch_size']}\")\n",
    "    print(f\"Epochs: {params['epoch']}\")\n",
    "    print(f\"Training loss: {report['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Validation loss: {report['val_losses'][-1]:.4f}\")\n",
    "    print(f\"Test loss: {report['test_losses'][-1]:.4f}\")\n",
    "\n",
    "    # 실제 값과 예측 값 출력\n",
    "    y_test = report['y_test']\n",
    "    predictions = report['predictions']\n",
    "    print(\"\\nActual Values vs Predicted Values:\")\n",
    "    for actual, predicted in zip(y_test.flatten(), predictions.flatten()):\n",
    "        print(f\"Actual: {actual:.4f}, Predicted: {predicted:.4f}\")\n",
    "\n",
    "########################\n",
    "# 파라미터 그리드 설정\n",
    "                            \n",
    "# 3(hidden_sizes) * 3(optimizer) * 3(learning_rate) * 3(batch_size) * 3(epochs) = 243 개의 모델을 생성\n",
    "# 뉴런 수 (Ex. [32, 16, 8]로 설정하면 3개의 히든 레이어가 생성 되고 뉴런 수는 각각 32, 16, 8로 할당)\n",
    "param_grid = {\n",
    "    'hidden_sizes': [[32, 16, 8], [16, 16, 16], [8, 16, 32]],\n",
    "    'optimizer': [tf.train.GradientDescentOptimizer, tf.train.AdamOptimizer, tf.train.AdagradOptimizer],\n",
    "    'learning_rate': [0.01, 0.001, 0.0001],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epoch': [100, 200, 300]\n",
    "}\n",
    "\n",
    "########################\n",
    "# 메인 실행 흐름\n",
    "\n",
    "def main():\n",
    "    # Data\n",
    "    x_data, y_data = load_csv('data-ori.csv')\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test = split_data(x_data, y_data)\n",
    "    data = (x_train, x_val, x_test, y_train, y_val, y_test)\n",
    "\n",
    "    # Learning\n",
    "    best_report = learn(data, param_grid)\n",
    "\n",
    "    # Report\n",
    "    generate_report(best_report)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85d7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f47420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
